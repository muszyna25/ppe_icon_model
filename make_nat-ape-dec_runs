#!/bin/bash

#==============================================================================
# Creates the ICON run scripts
# Leonidas Linardakis, MPI-M, 2011-25-1
#==============================================================================
#==============================================================================
# The basic command for creating an ICON experiment run script is
#   
#  $make_runscript in_script=exp.<name> in_script=exec.iconrun EXPNAME=<name>
#
# By default the folder in use is ./run, and the run script is named exp.<name>.run.
# 
# Basic optional parameters for the $make_runscript command are:
#
#    out_script=<output run script name>. By default is <in_script>.run
#
#    in_folder=<input folder>. By default is run
#
#    out_folder=<output folder>. By default is =<in_folder>
#
#    mpi_procs=<number of mpi processes>. In the case of MPI configuration,
#       defines how many processes per node will be used.
# 
#    no_of_nodes=<Number of nodes>. In the case of MPI configuration,
#       defines how many nodes will be used.
# 
#    openmp_threads=<Number of openmp threads>. In the case of OPENMP
#       configuration, defines how many OPENMP threads will be used.
#
#    cpu_time=<wall time>. Defines the expected run wall time.
#  
#    <free_variable>=<value> Free variables can be passed to the run script
#       using this syntax. For example: EXPNAME=test, will result the
#       definition of the variable EXPNAME=test in the run script. 
   

#
# For more details see the parameters in the ./config/make_target_runscript
#==============================================================================
set -x
echo "-----------------------------------------------------------"
base_folder=$(pwd)
input_folder=run
#==============================================================================
. $base_folder/config/set-up.info
use_shell=${use_shell:="/bin/ksh"}
# The $make_runscript command directs to the ./config/make_target_runscript
make_runscript="$use_shell ./config/make_target_runscript"

#==============================================================================
create_dec()
{

  if [[ x$atmo_dyn_gridname == x ]] ; then  
    grid_base_level=$(eval "echo \$$(echo grid_base_level_dec_${dec_size})")
    grid_level=`expr ${grid_base_level} + ${add_grid_level}`  
    atmo_dyn_gridname="iconR2B0${grid_level}_dec-${dec_size}"
  fi

  
  mod=`expr ${dec_size} % ${mpi_procs_pernode}`
  if [ $mod -eq 0 ] ; then
    nodes=`expr ${dec_size} / ${mpi_procs_pernode}`
  else
    nodes=`expr ${dec_size} / ${mpi_procs_pernode} + 1`
  fi

  icon_comm_method=${icon_comm_method:=1}
  comm_method=$icon_comm_method 
  if [ "x$use_icon_comm" != "x.true." ]; then
    comm_method=0
  fi
  exp_name=${outname}_${comm_method}-comm_${iorder_sendrecv}sendrecv.${openmp_threads}threads.${nodes}nodes.${dec_size}procs.${nproma}nproma.${atmo_dyn_gridname}.${nlevels}levels
  runname=exp.${exp_name}.run

  $make_runscript in_folder=$input_folder in_script=exp.$exp_descriptor in_script=exec.iconrun EXPNAME=${exp_name} out_script=${runname} \
    atmo_dyn_gridname="$atmo_dyn_gridname" atmo_rad_gridname="$atmo_rad_gridname" nproma=$nproma no_of_nodes=$nodes dec_size=$dec_size dt_rad=$dt_rad \
    mpi_procs=$mpi_procs_pernode openmp_threads=$openmp_threads cpu_time=$cpu_time use_barrier=$use_barrier testbed_mode=$testbed_mode division_method=$division_method \
    ndays=$ndays use_icon_comm=$use_icon_comm parallel_radiation_mode=$parallel_radiation_mode icon_comm_method=$icon_comm_method grids_folder=$grids_folder \
    max_mpi_message_size=$max_mpi_message_size dtime=$dtime nlevels=$nlevels iorder_sendrecv=$iorder_sendrecv itime_scheme=${itime_scheme} \
    advection_substepping=$advection_substepping  vwind_implicitness=${vwind_implicitness} max_send_recv_buffer_size=${max_send_recv_buffer_size}
  # queue="express"

  if [ $run = "true" ]; then
    cd $input_folder
    $use_submit ${runname}
    cd $base_folder
  fi

  atmo_dyn_gridname=""
  
}
#==============================================================================

#==============================================================================
create_seq()
{

  if [[ x$atmo_dyn_gridname == x ]] ; then  
    grid_base_level=$(eval "echo \$$(echo grid_base_level_dec_${dec_size})")
    grid_level=`expr ${grid_base_level} + ${add_grid_level}`  
    atmo_dyn_gridname="iconR2B0${grid_level}_dec-${dec_size}"
  fi
  
  exp_name=${outname}_seq.${nproma}nproma.${atmo_dyn_gridname}.${nlevels}levels
  runname=exp.${exp_name}.run

  $make_runscript in_folder=$input_folder in_script=exp.$exp_descriptor in_script=exec.iconrun EXPNAME=${exp_name} out_script=${runname} \
    atmo_dyn_gridname="$atmo_dyn_gridname" atmo_rad_gridname="$atmo_rad_gridname" nproma=$nproma dt_rad=$dt_rad \
    openmp_threads=$openmp_threads cpu_time=$cpu_time testbed_mode=$testbed_mode ndays=$ndays  \
    grids_folder=$grids_folder dtime=$dtime nlevels=$nlevels advection_substepping=$advection_substepping \
    vwind_implicitness=${vwind_implicitness} itime_scheme=${itime_scheme}
 
  # queue="express"

  if [ $run = "true" ]; then
    cd $input_folder
    $use_submit ${runname}
    cd $base_folder
  fi

  atmo_dyn_gridname=""
  
}
#==============================================================================

#==============================================================================
set_default_parameters()
{
  exp_descriptor=nat_ape-dec
  create_method=dec
  mpi_procs_pernode=32
  openmp_threads=2
  nproma=4
  max_mpi_message_size=8192
#  max_send_recv_buffer_size=262144
  max_send_recv_buffer_size=131072
  icon_comm_method=1
  rrad="false"
  division_method=1
  nlevels=96
  parallel_radiation_mode=0
  dtime=60
  dt_rad=1800
  itime_scheme=4
  vwind_implicitness=0.15
  advection_substepping=4
  use_barrier=".false."
  testbed_mode=0
  timers_level=6
  use_icon_comm=".true."
  icon_comm_method=103
  iorder_sendrecv=1
  ndays=10
  #---------------------------------
  grids_folder=/work/mh0287/leonidas.public/grids/grids.new
  run="false"
}
#==============================================================================


#==============================================================================
create_std_exp_on_blizzard()
{
  set_default_parameters
  mpi_procs_pernode=32
  dec_size=32  # create a total of 32 mpi procs, ie one node 
  openmp_threads=2
  atmo_dyn_gridname="iconR2B04-grid"
  outname="nat-ape_std"
  ndays=10
  cpu_time="01:00:00"
  create_dec
}
#==============================================================================

#create_std_exp_on_blizzard
#exit
#==============================================================================
create_strong_scaling()
{
  grid_name=$1
  outname=$2
  nodes_list="$3"
  division_method=1

  for iter_nodes in $nodes_list
  do
    dec_size=`expr ${mpi_procs_pernode} \* ${iter_nodes}`
    atmo_dyn_gridname=$grid_name
    create_dec
  done
}


#==============================================================================
create_weak_scaling()
{
  weak_scaling_setup_name=$1
  cells_per_domain=$2
  dec_size_list="$3"
  dec_size_list_all="32 92 162 362 642 1082 1442 1922 2432"
  if [ x$dec_size_list == x ] ;
  then
     dec_size_list="$dec_size_list_all"
  fi

  grid_base_level_dec_32=1
  grid_base_level_dec_92=2
  grid_base_level_dec_162=2
  grid_base_level_dec_362=3
  grid_base_level_dec_642=3
  grid_base_level_dec_1082=4
  grid_base_level_dec_1442=4
  grid_base_level_dec_1922=4
  grid_base_level_dec_2432=5

  case $cells_per_domain in
    "6" )  add_grid_level=0
  ;;
    "24" ) add_grid_level=1
  ;;
    "96" ) add_grid_level=2
  ;;
    "384" ) add_grid_level=3
  ;;
  *)  
    echo "Wrong cells_per_domain: $cells_per_domain"
    exit
  ;;
  esac

  if [ x$division_method == x0 ]; then
    outname="${weak_scaling_setup_name}_hexdec"
  else
    outname="${weak_scaling_setup_name}_geodec"
  fi
  outname="${outname}_${cells_per_domain}cellspd"
  atmo_dyn_gridname=""
  atmo_rad_gridname=""

  for dec_size in $dec_size_list
  do
    create_${create_method}
  done

}

#==============================================================================
create_seq_on_blizzard()
{
  set_default_parameters
  create_method=seq
  cpu_time="08:00:00"
  dt_rad=3600
  nlevels=64
  max_send_recv_buffer_size=131072
  max_mpi_message_size=8192
  mpi_procs_pernode=32
  openmp_threads=2
  use_icon_comm=".true."
  icon_comm_method=103
  iorder_sendrecv=1

  grids_folder=/work/mh0287/leonidas.public/grids/grids.new
  division_method=0
  parallel_radiation_mode=0
  setup_name='nat_ape_24cells'
  #---------------------------------
  run="true"
  #==============================================================================

  # run the 1082 cores
  ndays=18
  dtime=240
  create_weak_scaling "${setup_name}" 24 "1082"

  dtime=204
  create_weak_scaling "${setup_name}" 24 "1442"

  ndays=12
  dtime=175
  create_weak_scaling "${setup_name}" 24 "1922"

  dtime=154
  create_weak_scaling "${setup_name}" 24 "2432"

  exit
}



#==============================================================================
create_weak_scaling_on_blizzard()
{
  set_default_parameters
  ndays=120
  cpu_time="00:20:00"
  dt_rad=3600
  nlevels=64
  max_send_recv_buffer_size=131072
  max_mpi_message_size=8192
  mpi_procs_pernode=32
  openmp_threads=2
  use_icon_comm=".true."
  icon_comm_method=103
  iorder_sendrecv=1

  grids_folder=/work/mh0287/leonidas.public/grids/grids.new
  division_method=0
  parallel_radiation_mode=0
  setup_name='nat_ape_24cells'
  #---------------------------------
   run="true"
  #==============================================================================

  # run the 1082 cores
  ndays=180
  dtime=240
  create_weak_scaling "${setup_name}" 24 "1082"

  dtime=204
  create_weak_scaling "${setup_name}" 24 "1442"

  ndays=120
  dtime=175
  create_weak_scaling "${setup_name}" 24 "1922"

  dtime=154
  create_weak_scaling "${setup_name}" 24 "2432"

  exit

  create_weak_scaling "${setup_name}_60days_128Kmessage" 24 "1442"
  max_mpi_message_size=8192
  create_weak_scaling "${setup_name}_60days_64Kmessage" 24 "1442"
  max_mpi_message_size=4096
  create_weak_scaling "${setup_name}_60days_32Kmessage" 24 "1442"

  exit

  parallel_radiation_mode=0
  create_weak_scaling "nat-ape_norerad_20days"  24 "1442"

  use_icon_comm=".true."
  icon_comm_method=1
  create_weak_scaling "nat-ape_norerad_20days"  24 "1442"

  exit

  mpi_procs_pernode=16
  openmp_threads=4
  ndays=30
  create_weak_scaling "$setup_name" 96 "32 92 162 362 642 1082"
 
  mpi_procs_pernode=32
  openmp_threads=2
 
  ndays=60
  # create_weak_scaling "$setup_name" 24
  
  ndays=15
  # create_weak_scaling "$setup_name" 96
  #---------------------------------
}


#==============================================================================
create_nat_test-setups_on_blizzard()
{
  set_default_parameters
  ndays=120
  cpu_time="04:00:00"

  setup_name='nat-ape_setup-tests'
  #---------------------------------
   run="true"
  #==============================================================================
#  exp_descriptor=nat_ape-dec_notop
  itime_scheme=4
  setup_name='nat-ape_setup-tests_85km'
 
  vwind_implicitness=0.5
  advection_substepping=4
  dtime=165
  dt_rad=3960
  # create_strong_scaling "iconR2B05_dec-1922" "$setup_name" "4"

  max_send_recv_buffer_size=262144
  vwind_implicitness=0.4
  advection_substepping=4
  dtime=132
  dt_rad=3168
  create_strong_scaling "iconR2B05-grid" "$setup_name" "6"

  max_send_recv_buffer_size=393216
  vwind_implicitness=0.15
  advection_substepping=4
  dtime=60
  dt_rad=1800
  # create_strong_scaling "iconR2B06-grid" "$setup_name" "4"
  
  
  exit
}
   


#==============================================================================
create_comm_test_on_blizzard()
{
  set_default_parameters
  exp_descriptor="icon-testbed_communication"
  cpu_time="00:20:00"
  dt_rad=1800
  division_method=0
  parallel_radiation_mode=1
  use_icon_comm=".true."
  icon_comm_method=1
  mpi_procs_pernode=32
  openmp_threads=2
  ndays=500

  #---------------------------------
  # run="true"
  #==============================================================================
  grids_folder=/work/mh0287/leonidas.public/grids/grids.radiation_4-16-16-cells
  setup_name="rad-comm-test_16cells"
  max_mpi_message_size=16384
  create_weak_scaling "${setup_name}_128Kmessage" 96 "1082"
  max_mpi_message_size=8192
  create_weak_scaling "${setup_name}_64Kmessage" 96 "1082"
  max_mpi_message_size=4096
  create_weak_scaling "${setup_name}_32Kmessage" 96 "1082"

  exit

  grids_folder=/work/mh0287/users/leonidas/icon/grids.radiation_4-4-16-cells
  create_weak_scaling "rad-comm-test_4cells_2" 96 "1082"

  exit
  #==============================================================================
  ndays=60
  division_method=0
  create_weak_scaling "comm-weaktest_hexdec" 24
  create_weak_scaling "comm-weaktest_hexdec" 96
  create_weak_scaling "comm-weaktest_hexdec" 384 
  #==============================================================================
  division_method=1
  create_weak_scaling "comm-weaktest_hexdec" 96
  create_weak_scaling "comm-weaktest_geomdec" 384

}

create_nat_test-setups_on_blizzard

#create_seq_on_blizzard
# create_weak_scaling_on_blizzard
# create_comm_test_on_blizzard
exit


#==============================================================================
# ignore these

# name=icon-testbed_communication-dec
# run="true"
# #---------------------------------
# use_barrier=".false."
# testbed_mode=0
# 
# division_method=0
# outname=icon-testbed_communication-hexdec
# create_scaling_dec_lowres
# 
# division_method=1
# outname=icon-testbed_communication-geomdec
# create_scaling_dec_lowres
# 
# exit
# 
# 
# #==============================================================================
# 
# #==============================================================================
# # special treatment for exp.icon-testbed_communication
# # this includes the restart in the test_hat_jww-moist_cld-cnv-vdf_restart
# #
# 
# run_script="true"
# 
# nodes_list=(  10 20 40 60 80 )
# nproma_list=( 37 37 33 29 33 )
# nodes_list_size=${#nodes_list[*]}
# 
# name=icon-testbed_communication
# 
# j=0
# while [ $j -lt ${nodes_list_size} ]
# do
#   nodes=${nodes_list[$j]}
#   nproma=${nproma_list[$j]}
#  
#   exp_name=${name}.${nodes}nodes
#   runname=exp.${exp_name}.run
#   $make_runscript in_folder=$input_folder in_script=exp.$name in_script=exec.iconrun EXPNAME=$exp_name out_script=$runname\
#   grid="iconR2B06-grid.nc"  nproma=$nproma no_of_nodes=$nodes mpi_procs=16 openmp_threads=4 cpu_time="00:10:00"
# 
#   if [[ $run_script == "true" ]] ; then
#      cd $input_folder
#      $use_submit ./$runname
#      cd ..
#   fi
# 
#   let j=j+1
# done
# #==============================================================================
# 
