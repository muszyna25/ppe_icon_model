#!/bin/bash

set -e

# set required environment variables
export MODULESHOME="/usr/share/Modules"

# this wrapper needs to build two ICON binaries
MY_DIR=$(cd "$(dirname "$0")"; pwd)
ICON_DIR=$(cd "${MY_DIR}/../.."; pwd)

# =============================================================================
# create vector engine binary

mkdir -p build/vector
cd build/vector

# We are going to run 'make check' for the vector engine build. We also want to
# enable the checks of the bundled libraries that require a valid MPI_LAUNCH
# command. Currently, there are two libraries with such tests: YAXT and YAC.
# Both of them check whether the provided MPI_LAUNCH command is valid at the
# configure time and disable (skips) the respective tests if that is not the
# case. The problem is that we cannot run the cross-compiled vector binaries
# with 'mpirun' at the configure time. We are also not aware of an alternative
# to 'srun' command (an element of the SLURM scheduler) on this system, which
# can allocate a compute node and synchronously launch MPI jobs there. An
# alternative would additionally have to allow for running MPI jobs inside an
# existing allocation without additional modification of the command line
# arguments because we run 'make check' asynchronously via the scheduler (like
# we do for other Buildbot tests). A workaround is to make sure that the bundled
# libraries accept MPI_LAUNCH in the form that can be used on the existing
# allocation of compute nodes. First, we set MPI_LAUNCH to a valid value. This
# is enough for YAXT since its configure script does not check and does not
# discard MPI_LAUNCH in the cross-compilation mode. The configure script of YAC
# checks the command unconditionally because it expects a command that can work
# as 'srun'. To skip the check, we additionally set the 'acx_cv_prog_mpirun'
# cache variable to the same value as for the MPI_LAUNCH. Also, the configure
# script of YAC runs an additional MPI Fortran/C library compatibility check
# which fails if 'acx_cv_prog_mpirun' is set but does not work. To skip this
# additional check, we set another cache variable 'acx_cv_fc_c_compatible_mpi'
# to 'yes'.

# We need two additional patches to skip/adjust the tests (see
# config/buildbot/dwd_nec_patches/README.md). Also, the program 'patch' exits
# with exit code '1' if the patch has already been applied. We consider this a
# normal scenario:
PATCH='patch --forward --no-backup-if-mismatch --strip=1 --reject-file=-'
$PATCH -d "${ICON_DIR}/externals/yaxt" -i "${MY_DIR}/dwd_nec_patches/yaxt_mpi_abort_test.patch" || test $? -eq 1 || exit 2


${ICON_DIR}/config/dwd/rcl.VE.nfort-3.2.0_oper --disable-openmp --prefix=${ICON_DIR}/vector MPI_LAUNCH='/opt/nec/ve/bin/mpirun' acx_cv_prog_mpirun='/opt/nec/ve/bin/mpirun' acx_cv_fc_c_compatible_mpi=yes


if test 0 -ne "$?" && test ! -z "${BB_SLAVE}"; then
  echo " ***** Configuration logs from '$(pwd)' ***** "
  for f in $(find . -name 'config.log' -print); do
    echo " ||||| '$f' ||||| "
    cat "$f"
  done
  echo " ***** End of configuration logs from '$(pwd)' ***** "
fi

MAKE_PROCS=14
make -j ${MAKE_PROCS}
make install


cd ../../

# copy a valid info file to where runexp expects it. buildbot is doing
# in-source-builds, only at the moment
cp -v $OLDPWD/run/set-up.info run/

# =============================================================================
# create vector host binary
mkdir -p build/host
cd build/host
${ICON_DIR}/config/dwd/rcl.VH.gcc --prefix=${ICON_DIR}/host

if test 0 -ne "$?" && test ! -z "${BB_SLAVE}"; then
  echo " ***** Configuration logs from '$(pwd)' ***** "
  for f in $(find . -name 'config.log' -print); do
    echo " ||||| '$f' ||||| "
    cat "$f"
  done
  echo " ***** End of configuration logs from '$(pwd)' ***** "
fi

MAKE_PROCS=14
make -j ${MAKE_PROCS}
make install

cd ../..

# =============================================================================
VECTOR_BINARY=${ICON_DIR}/vector/bin/icon
HOST_BINARY=${ICON_DIR}/host/bin/icon

set +e
find "${PWD}/vector" "${PWD}/host"
file "$VECTOR_BINARY" "$HOST_BINARY"
