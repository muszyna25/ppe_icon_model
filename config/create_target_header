#!/usr/bin/env ksh
#==============================================================================
#
# Sets the parameters for various machine use_compiler configurations
# and creates the run script headers for these configurations
#
# Leonidas Linardakis, MPI-M, 2010-11-24
#
# Based on previous scripts by Marco Giorgetta and Hermann Asensio
#
#==============================================================================
# Basic parameters in this script
# -----------------------------
# use_nodes  =  no of nodes to run
# use_mpi_procs_pernode = number of mpi procs per node
#
#==============================================================================

add_free_var()
{
  varName=$1
  varValue=$2
  
  let "no_of_free_variables=$no_of_free_variables+1"
  free_variable[$no_of_free_variables]=$varName
  free_variable_value[$no_of_free_variables]=$varValue
}

set_default()
{
  if [[ "x$(eval echo \$$1)" == "x" ]]; then
    eval "$1=$2"
  fi 
}


start_header()
{
if [[ "$use_mpi" == "no" ]]; then
  start=${start:=""}
else
  start=${start:="$use_mpi_startrun"}
fi

set_default use_nodes 1
set_default use_omp_stacksize 32M
mpi_total_procs='$((no_of_nodes * mpi_procs_pernode))'

cat >> $output_script << EOF
#=============================================================================
#
# ICON run script. Created by $scriptname
# target machine is $use_target
# target use_compiler is $use_compiler
# with mpi=$use_mpi
# with openmp=$use_openmp
# memory_model=$use_memory_model
# submit with $use_submit
# 
#=============================================================================
set -x
. ./add_run_routines
#-----------------------------------------------------------------------------
# target parameters
# ----------------------------
site="$use_site"
target="$use_target"
compiler="$use_compiler"
loadmodule="$use_load_modules"
with_mpi="$use_mpi"
with_openmp="$use_openmp"
job_name="$job_name"
submit="$use_submit"
#-----------------------------------------------------------------------------
# openmp environment variables
# ----------------------------
export OMP_NUM_THREADS=${use_openmp_threads}
export ICON_THREADS=${use_openmp_threads}
export OMP_SCHEDULE=${use_OMP_SCHEDULE}
export OMP_DYNAMIC="false"
export OMP_STACKSIZE=${use_omp_stacksize}
#-----------------------------------------------------------------------------
# MPI variables
# ----------------------------
mpi_root=$use_mpi_root
no_of_nodes=${use_nodes}
mpi_procs_pernode=$use_mpi_procs_pernode
((mpi_total_procs=no_of_nodes * mpi_procs_pernode))
START="$start"
#-----------------------------------------------------------------------------
# load ../setting if exists  
if [ -a ../setting ]
then
  echo "Load Setting"
  . ../setting
fi
#-----------------------------------------------------------------------------
rundir="$run_folder"
bindir="\${basedir}/$use_builddir/bin"   # binaries
BUILDDIR=$use_builddir
#-----------------------------------------------------------------------------
EOF


if [[ "x$use_load_profile" != "x" ]]; then
  if [[ "x$use_load_modules" != "x" ]]; then
    profile_filename=`echo $use_load_profile | cut -d ' ' -f2`
cat >> $output_script << EOF
#=============================================================================
# load profile
if [ -a  $profile_filename ] ; then
$use_load_profile
#=============================================================================
#=============================================================================
# load modules
module purge
module load "\$loadmodule"
module list
#=============================================================================
fi
EOF
fi
fi

if [[ "x$use_netcdflibpath" != "x" ]]; then
cat >> $output_script << EOF
#=============================================================================
export LD_LIBRARY_PATH=$use_netcdflibpath:\$LD_LIBRARY_PATH
#=============================================================================
EOF
fi

cat >> $output_script << EOF
nproma=$use_nproma
cdo="${cdo}"
cdo_diff="${cdo} ${cdo_diff}"
icon_data_rootFolder="${icon_data_rootFolder}"
icon_data_poolFolder="${icon_data_poolFolder}"
icon_data_buildbotFolder="${icon_data_rootFolder}/buildbot_data"
icon_data_buildbotFolder_aes="${icon_data_rootFolder}/buildbot_data/aes"
icon_data_buildbotFolder_oes="${icon_data_rootFolder}/buildbot_data/oes"
EOF

#for i in {1..$no_of_free_variables}
i=1;while [ $i -le ${no_of_free_variables} ]
do
  echo "export ${free_variable[$i]}=\"${free_variable_value[$i]}\"" >> $output_script
  i=$((i+1));
done

}

#=============================================================================
set_run_target_bullx_cpu()
{
  # mistral.dkrz.de  
  icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=/pool/data/ICON}

  set_default use_nproma 16
  set_default use_nodes 1
  set_default use_account_no "$(id -gn)"
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    use_account_no="mh0156"
  fi
#   use_submit="$use_submit -N \${SLURM_JOB_NUM_NODES:-1}"
  use_OMP_SCHEDULE="dynamic,1"
  use_omp_stacksize=200M

  # set to 1 to use smt feature
  smt_adjust=2
  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / 6 / '"$smt_adjust"'))"'
      set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '$smt_adjust' / OMP_NUM_THREADS))"'
    else
      set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
    fi
  else
    # on mistral, srun is still needed for running processes at
    # full frequency
    start="srun --cpu-freq=HighM1 -n 1"
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
      #      set_default use_mpi_procs_pernode 1
      start+=" --cpus-per-task=\$(($smt_adjust * OMP_NUM_THREADS))"
    else
      is_serial_run="yes"
    fi
  fi

  [[ "x$use_queue" != 'x' ]] && queue=$use_queue

  use_mpi_startrun="srun --cpu-freq=HighM1 --kill-on-bad-exit=1 --nodes=\${SLURM_JOB_NUM_NODES:-1} --cpu_bind=verbose,cores --distribution=block:block --ntasks=\$((no_of_nodes * mpi_procs_pernode)) --ntasks-per-node=\${mpi_procs_pernode} --cpus-per-task=\$((${smt_adjust} * OMP_NUM_THREADS)) --propagate=STACK,CORE"
# TODO: disabled because other core files are generated
# case $use_flags_group in
#   debug)
#     use_mpi_startrun="${use_mpi_startrun},CORE"
#     ;;
# esac

  case _$use_compiler in
    "_intel" )
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac
  case $use_mpi_root in
    *bullxmpi*)
      # this does currently not work on mistral but is most probably needed
      # for other SLURM installations
      #use_mpi_startrun+=" --mpi=openmpi"
      add_free_var OMPI_MCA_pml cm
      add_free_var OMPI_MCA_mtl mxm
      add_free_var OMPI_MCA_coll '^ghc'
      add_free_var MXM_RDMA_PORTS 'mlx5_0:1'
      add_free_var OMPI_MCA_coll_fca_enable 1
      add_free_var OMPI_MCA_coll_fca_priority 95
      ;;
    *openmpi*)
      add_free_var OMPI_MCA_pml cm
      add_free_var OMPI_MCA_mtl mxm
      add_free_var OMPI_MCA_coll ^fca
      add_free_var MXM_RDMA_PORTS 'mlx5_0:1'
      add_free_var HCOLL_MAIN_IB 'mlx5_0:1'
      add_free_var HCOLL_ML_DISABLE_BARRIER 1
      add_free_var HCOLL_ML_DISABLE_IBARRIER 1
      add_free_var HCOLL_ML_DISABLE_BCAST 1
      add_free_var HCOLL_ENABLE_MCAST_ALL 1
      add_free_var HCOLL_ENABLE_MCAST 1
      add_free_var OMPI_MCA_coll_sync_barrier_after_alltoallv 1
      add_free_var OMPI_MCA_coll_sync_barrier_after_alltoallw 1
      add_free_var MXM_HANDLE_ERRORS bt
      add_free_var UCX_HANDLE_ERRORS bt
      ;;
    *intel*mpi*)
      add_free_var I_MPI_FABRICS shm:dapl
      add_free_var I_MPI_DAPL_UD enable
      add_free_var I_MPI_DAPL_UD_PROVIDER ofa-v2-mlx5_0-1u
      add_free_var DAPL_UCM_REP_TIME 8000
      add_free_var DAPL_UCM_RTU_TIME 4000
      add_free_var DAPL_UCM_CQ_SIZE 1000
      add_free_var DAPL_UCM_QP_SIZE 1000
      add_free_var DAPL_UCM_RETRY 10
      add_free_var DAPL_ACK_RETRY 10
      add_free_var DAPL_ACK_TIMER 20
      add_free_var DAPL_UCM_TX_BURST 100
      add_free_var DAPL_WR_MAX 500
      ;;
  esac

  add_free_var MALLOC_TRIM_THRESHOLD_ -1
  if [[ "$is_serial_run" == "yes"  ]] ; then
    queue=${queue:="compute2,compute"}
    use_nodes=1
    tasks_per_node=1
    SBATCH_ntasks_per_node=""
 else
    queue=${queue:="compute2,compute"}
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi

  # ------------------------------------
  s_cpu_time="00:30:00"
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time=$use_cpu_time
  fi
  # ------------------------------------

  cat >> $output_script << EOF
# =====================================
# mistral batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --account=$use_account_no
$(if [[ "x$BB_SYSTEM" != 'x' ]]; then
  printf '#SBATCH --qos=buildbot
';fi)
#SBATCH --job-name=$job_name
#SBATCH --partition=$queue
#SBATCH --workdir=$output_folder
#SBATCH --nodes=$use_nodes
#SBATCH --threads-per-core=2
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --exclusive
#SBATCH --time=$s_cpu_time
$(if [[ "$use_mpi_root" == *intel*mpi* ]]; then
  printf '#========================================
# the following line is only needed for srun to work with Intel MPI
# but should be commented when using BullX MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
#========================================' ; fi)
EOF

# for the rest use the SLURM_JOB_NUM_NODES by default 
use_nodes="\${SLURM_JOB_NUM_NODES:=${use_nodes}}"

  # create header
  start_header
  cat >>$output_script <<EOF
ulimit -s 2097152
ulimit -c 0
$(case $use_flags_group in
    (debug)
      printf 'ulimit -c unlimited'
      ;;
  esac)
# this can not be done in use_mpi_startrun since it depends on the
# environment at time of script execution
#case " \$loadmodule " in
#  *\ mxm\ *)
#    START+=" --export=\$(env | sed '/()=/d;/=/{;s/=.*//;b;};d' | tr '\n' ',')LD_PRELOAD=\${LD_PRELOAD+\$LD_PRELOAD:}\${MXM_HOME}/lib/libmxm.so"
#    ;;
#esac
EOF
}

#=============================================================================
set_run_target_bullx_gpu()
{

  # mistral.dkrz.de  with GPU
  icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=/pool/data/ICON}

  set_default use_nproma 16
  set_default use_nodes 1
  set_default use_account_no "$(id -gn)"
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    use_account_no="mh0156"
  fi
  use_submit="$use_submit -N \${SLURM_JOB_NUM_NODES:-1}"
  use_OMP_SCHEDULE="dynamic,1"
  use_omp_stacksize=200M

  # set to 1 to use smt feature
  smt_adjust=2
  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
  else
    # on mistral, srun is still needed for running processes at
    # full frequency
    start="srun --cpu-freq=HighM1 -n 1"
    is_serial_run="yes"
  fi

  [[ "x$use_queue" != 'x' ]] && queue=$use_queue

  use_mpi_startrun="srun --cpu-freq=HighM1"

  add_free_var MALLOC_TRIM_THRESHOLD_ -1
  if [[ "$is_serial_run" == "yes"  ]] ; then
    queue=${queue:="gpu"}
    use_nodes=1
    tasks_per_node=1
    SBATCH_ntasks_per_node=""
 else
    queue=${queue:="gpu"}
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi

  # ------------------------------------
  s_cpu_time="00:30:00"
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time=$use_cpu_time
  fi
  # ------------------------------------

  cat >> $output_script << EOF
# =====================================
# mistral batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --account=$use_account_no
$(if [[ "x$BB_SYSTEM" != 'x' ]]; then
  printf '#SBATCH --qos=buildbot
';fi)
#SBATCH --job-name=$job_name
#SBATCH --partition=$queue
#SBATCH --workdir=$output_folder
#SBATCH --nodes=$use_nodes
#SBATCH --threads-per-core=1
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --exclusive
#SBATCH --time=$s_cpu_time
#SBATCH --constraint=k80
$(if [[ "$use_mpi_root" == *intel*mpi* ]]; then
  printf '#========================================
# the following line is only needed for srun to work with Intel MPI
# but should be commented when using BullX MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
#========================================' ; fi)
EOF

# for the rest use the SLURM_JOB_NUM_NODES by default 
use_nodes="\${SLURM_JOB_NUM_NODES:=${use_nodes}}"

  # create header
  start_header
  cat >>$output_script <<EOF
ulimit -s 2097152
ulimit -c 0
$(case $use_flags_group in
    (debug)
      printf 'ulimit -c unlimited'
      ;;
  esac)
EOF
}

#=============================================================================
set_run_target_mpipc()
{

  # *.mpimet.mpg.de with Linux (workstations and breeze)  
  icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=/pool/data/ICON}

  set_default use_nproma 64
  use_nodes=1
  
  if [[ $use_mpi == "yes" ]]; then
    set_default use_mpi_procs_pernode 2
  fi
  if [[ $use_openmp == "yes" ]]; then
    set_default use_openmp_threads 2
  else
    set_default use_openmp_threads 1
  fi

  # create header
  start_header

cat >> $output_script << EOF
EOF
  
  if [[ "$use_mpi" == "yes" ]] ; then
    cat >> $output_script << EOF
#----------------------------------
# start mpd (MPI deamon for MPICH2)
export MPD_CON_EXT=job_\$\$
\${mpi_root}/bin/mpd &
sleep 2
mpdid=\$(\${mpi_root}/bin/mpdtrace -l | awk '{print \$1}')
echo "mpd started as \$mpdid"
#-----------------------------------
EOF
  fi
}

#=============================================================================
set_run_target_dole()
{  
  set_default use_nproma 8
  set_default use_nodes 1
   
  if [[ $use_openmp == "yes" ]]; then
    use_mpi="no"
    set_default use_openmp_threads 16
    queue="-q smp"
    resources=" -pe smp 16"
  elif [[ $use_mpi == "yes" ]]; then
    set_default use_mpi_procs_pernode 8
    if [[ "$use_mpi_procs_pernode" == "1" ]] ; then
      queue="-q serial"
      resources=""    
    else
      queue="-q cluster"
      resources="-pe orte8 ${use_mpi_procs_pernode}"
    fi
  else
    queue="-q serial"
    resources=""      
  fi

  # create header
  start_header
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="-l s_cpu=$use_cpu_time"
  fi

  cat >> $output_script << EOF
# ===========================
# dole specific setup 
 basedir=/scratch/${USER}/trunk/icon-dev
 calldir=\$basedir
 thisdir=\$basedir
# ===========================
# dole.cscs.ch batch job parameters
# ---------------------------
#PBS -V
#PBS -N $job_name
#PBS -S /bin/tcsh
#PBS -j oe
#PBS -l cpunum_job=$cpunum_job
#PBS -o LOG.$job_name.o
#PBS -q normal
#PBS -l walltime=02:00:00
# ==========================
EOF
}

#=============================================================================
set_run_target_mpimac()
{
  # some Macs at MPIM 
  icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=/pool/data/ICON}
    
  set_default use_nproma 48
  use_nodes=1
  
  if [[ "$use_mpi" == "yes" ]]; then
    set_default use_mpi_procs_pernode 2
  fi
  if [[ "$use_openmp" == "yes" ]]; then
    set_default use_openmp_threads 2
  else
    set_default use_openmp_threads 1
  fi

  # create header
  start_header
}

#=============================================================================
set_run_target_oflws()
{
  # *.dwd.de with Linux (workstations)
  icon_data_rootFolder=${icon_data_rootFolder:=""}
  icon_data_poolFolder=${icon_data_poolFolder:=""}
    
  set_default use_nproma 64

  if [[ "$use_mpi" == "yes" ]]; then
    set_default use_mpi_procs_pernode 4
  fi
  if [[ "$use_openmp" == "yes" ]]; then
    set_default use_openmp_threads 2
  fi

  # create header
  start_header
}

#=============================================================================
set_run_target_hpc()
{
  # xce.dwd.de
  icon_data_rootFolder=${icon_data_rootFolder:=""}
  icon_data_poolFolder=${icon_data_poolFolder:=""}
    
  set_default use_nproma 64
  set_default use_nodes 1
  set_default use_mpi_procs_pernode 1
  set_default use_openmp_threads 1

  cdo="/e/uhome/hanlauf/X86_64/bin/cdo"
  cdo_diff="diff"

    
  cat >> $output_script << EOF
# ===========================
#### BATCH_SYSTEM=PBS ####
#-----------------------------------------------------------------------------
#PBS -q lang
#PBS -j oe
#PBS -o LOG.$job_name.o
#PBS -l select=$use_nodes:ncpus=$use_mpi_procs_pernode
#PBS -m n
# ===========================
#-----------------------------------------------------------------------------
# for PBS change to directory where job was submitted
# (without this job is started in HOME)
if [[ -n \${PBS_O_WORKDIR} ]] ; then
  cd \${PBS_O_WORKDIR}
fi
export F_PROGINF=DETAIL
#-----------------------------------------------------------------------------
EOF
  
  # create header
  start_header

}

#=============================================================================
set_run_target_daint_cpu()
{
  # daint.cscs.ch
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 16
  set_default use_nodes 1
  # use_OMP_SCHEDULE="static,1"
  use_OMP_SCHEDULE="static,12"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 2
      set_default use_openmp_threads 3
    else
      set_default use_mpi_procs_pernode 6
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 6
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  case _$use_compiler in
    "_intel" )
        use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_nodes="SBATCH --nodes=1"
    SBATCH_ntasks_per_node=""
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_nodes="SBATCH --nodes=${use_nodes}"
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi
  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 15 minutes on Daint (SLURM delays)
    s_cpu_time="SBATCH --time=00:15:00"
  fi
  # ------------------------------------

  use_nodes=\"\${SLURM_JOB_NUM_NODES}\"
  use_mpi_procs_pernode=\"\${SLURM_NTASKS_PER_NODE}\"

  cat >> $output_script << EOF
# =====================================
# daint batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --constraint=gpu
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
# #SBATCH --workdir=$output_folder
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
#========================================
EOF
  # create header
  start_header

}

#=============================================================================
set_run_target_daint_gpu()
{
  # daint.cscs.ch with GPU
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 256
  set_default use_nodes 1
  use_OMP_SCHEDULE="static,1"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 1
      set_default use_openmp_threads 1
    else
      set_default use_mpi_procs_pernode 1
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 1
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  case _$use_compiler in
    "_intel" )
        use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_nodes="SBATCH --nodes=1"
    SBATCH_ntasks_per_node=""
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_nodes="SBATCH --nodes=$use_nodes"
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi

  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 45 minutes
    s_cpu_time="SBATCH --time=00:45:00"
  fi

  # ------------------------------------

  use_nodes=\"\${SLURM_JOB_NUM_NODES}\"
  use_mpi_procs_pernode=\"\${SLURM_NTASKS_PER_NODE}\"

  cat >> $output_script << EOF
# =====================================
# daint batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --constraint=gpu
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --workdir=$output_folder
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
#========================================
EOF
  # create header
  start_header
}

#=============================================================================
set_run_target_dom_gpu()
{
  # dom.cscs.ch
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 256
  set_default use_nodes 1
  use_OMP_SCHEDULE="static,1"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 1
      set_default use_openmp_threads 1
    else
      set_default use_mpi_procs_pernode 1
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 1
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_ntasks_per_node=""
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=1"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi

  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 45 minutes
    s_cpu_time="SBATCH --time=00:45:00"
  fi

  # ------------------------------------

  cat >> $output_script << EOF
# =====================================
# dom batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --job-name=$job_name
#SBATCH --output=$job_log_name.o
#SBATCH --error=$job_log_name.o
#SBATCH --nodes=$use_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
#========================================
EOF
  # create header
  start_header
}

#=============================================================================
set_run_target_tave_knl()
{
  # tyve.cscs.ch
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 16
  set_default use_nodes 1
  use_OMP_SCHEDULE="static,2"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 60
      set_default use_openmp_threads 2
    else
      set_default use_mpi_procs_pernode 60
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 60
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  case _$use_compiler in
    "_intel" )
        use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_ntasks_per_node=""
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=32"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi
  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 45 minutes
    s_cpu_time="SBATCH --time=00:45:00"
  fi
  # ------------------------------------

  cat >> $output_script << EOF
# =====================================
# daint batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
# #SBATCH --workdir=$output_folder
#SBATCH --nodes=$use_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
#========================================
EOF
  # create header
  start_header

}

#=============================================================================
set_run_target_kesch_cpu()
{
  # kesch.cscs.ch
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 16
  set_default use_nodes 1
  # use_OMP_SCHEDULE="static,1"
  use_OMP_SCHEDULE="static,12"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 2
      set_default use_openmp_threads 6
    else
      set_default use_mpi_procs_pernode 24
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 24
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  case _$use_compiler in
    "_intel" )
        use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_nodes="SBATCH --nodes=1"
    SBATCH_ntasks_per_node=""
    SBATCH_cpus_per_task="SBATCH --cpus-per-task=1"
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_nodes="SBATCH --nodes=${use_nodes}"
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
    SBATCH_cpus_per_task="SBATCH --cpus-per-task=1"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi
  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 45 minutes
    s_cpu_time="SBATCH --time=00:45:00"
  fi
  # ------------------------------------

  use_nodes=\"\${SLURM_JOB_NUM_NODES}\"
  use_mpi_procs_pernode=\"\${SLURM_NTASKS_PER_NODE}\"

  cat >> $output_script << EOF
# =====================================
# kesch batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --partition=debug
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#$SBATCH_nodes
#SBATCH --ntasks-per-core=1
#$SBATCH_ntasks_per_node
#$SBATCH_cpus_per_task
#$s_cpu_time
#========================================
EOF
  # create header
  start_header

}

#=============================================================================
set_run_target_kesch_gpu()
{

  # kesch.cscs.ch with GPU
  typeset poolFolder_prefix=/users/icontest
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/pool/data/ICON}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/pool/data/ICON}

  set_default use_nproma 256
  set_default use_nodes 1
  use_OMP_SCHEDULE="static,1"

  is_serial_run="no"
  if [[ $use_mpi == "yes" ]]; then
    if [[ $use_openmp == "yes" ]]; then
      set_default use_mpi_procs_pernode 1
      set_default use_openmp_threads 1
    else
      set_default use_mpi_procs_pernode 1
    fi
  else
    if [[ $use_openmp == "yes" ]]; then
      set_default use_openmp_threads 1
      set_default use_mpi_procs_pernode 1
     else
       is_serial_run="yes"
   fi
  fi

  case _$use_compiler in
    "_intel" )
        use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
        add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
        add_free_var KMP_LIBRARY "turnaround"
        add_free_var KMP_KMP_SETTINGS "1"
        add_free_var OMP_WAIT_POLICY "active"
      ;;
  esac

  if [[ "$is_serial_run" == "yes"  ]] ; then
    use_nodes=1
    tasks_per_node=1
    SBATCH_nodes="SBATCH --nodes=1"
    SBATCH_ntasks_per_node=""
  else
    job_type=parallel
    tasks_per_node=$use_mpi_procs_pernode
    SBATCH_nodes="SBATCH --nodes=${use_nodes}"
    SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=1"
  fi

  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this buildbot, do not use job id
    job_log_name="LOG.$job_name"
  else
    job_log_name="LOG.$job_name.%j"
  fi

  # ------------------------------------
  s_cpu_time=""
  if [[ "x$use_cpu_time" != "x" ]] ; then
     s_cpu_time="SBATCH --time=$use_cpu_time"
  fi
  if [[ "x$BB_SYSTEM" != "x" ]]; then
    # this is buldbot, do not run more than 45 minutes
    s_cpu_time="SBATCH --time=00:45:00"
  fi

  use_nodes=\"\${SLURM_JOB_NUM_NODES}\"
  use_mpi_procs_pernode=\"\${SLURM_NTASKS_PER_NODE}\"

  # ------------------------------------

  cat >> $output_script << EOF
# =====================================
# daint batch job parameters
#-----------------------------------------------------------------------------
#SBATCH --gres=gpu:1
#SBATCH --partition=debug
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --workdir=$output_folder
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
#========================================
EOF
  # create header
  start_header
}

#=============================================================================
set_run_target_pacluster()
{  
  set_default use_nproma 32
  set_default use_nodes 1
  set_default use_cpu_time 04:00:00
   
  if [[ $use_openmp == "yes" ]]; then
    use_mpi="no"
    set_default use_openmp_threads 24
    queue=""
    resources=""
  elif [[ $use_mpi == "yes" ]]; then
    set_default use_mpi_procs_pernode 12
    if [[ "$use_mpi_procs_pernode" == "1" ]] ; then
      queue=""
      resources=""
    else
      queue=""
      resources=""
    fi
  else
    queue="-q s8"
    resources=""      
  fi

  cat >> $output_script << EOF
#############################################################################
# DLR Linux Cluster batch job parameters
# EMBEDDED FLAGS FOR PBS Pro
#############################################################################
################# shell to use
#PBS -S /bin/ksh
################# export all  environment  variables to job-script
#PBS -V
################# name of the log file
#PBS -o ./${job_name}.\${PBS_JOBID}.log
################# join standard and error stream (oe, eo) ?
#PBS -j oe
################# do not rerun job if system failure occurs
#PBS -r n    
################# send e-mail when [(a)borting|(b)eginning|(e)nding] job
### #PBS -m ae
### #PBS -M my_userid@my_institute.my_toplevel_domain
################# always ppn=12 tasks per node!
#PBS -l nodes=$use_nodes:ppn=$use_mpi_procs_pernode
#PBS -l walltime=$use_cpu_time
#############################################################################
#-----------------------------------------------------------------------------
# for PBS change to directory where job was submitted
# (without this job is started in HOME)
if [[ -n \${PBS_O_WORKDIR} ]] ; then
  cd \${PBS_O_WORKDIR}
fi
#-----------------------------------------------------------------------------
EOF

  # create header
  start_header
}

#=============================================================================
set_run_target_default()
{

  set_default use_nproma 8
  set_default use_nodes 1
  set_default use_mpi_procs_pernode 2
  
  if [[ $use_openmp == "yes" ]]; then
    set_default use_openmp_threads 2
  fi

  # create header
  start_header
}
#=============================================================================
set_run_target_fh2()
{

  icon_data_rootFolder=${icon_data_rootFolder:=/pfs/work6/workspace/scratch/ln1297-boundary_mistral-0/}
  icon_data_poolFolder=${icon_data_poolFolder:=/pfs/work6/workspace/scratch/ln1297-boundary_mistral-0/}

  set_default use_nproma 8
  use_nodes=1
  
  if [[ $use_mpi == "yes" ]]; then
    set_default use_mpi_procs_pernode 2
  fi
  if [[ $use_openmp == "yes" ]]; then
    set_default use_openmp_threads 2
  else
    set_default use_openmp_threads 1
fi
output_script_name=$(basename "$output_script")
#| sed -r "s/(.+)\/.+/\1/"
stripped_output="LOG.$output_script_name.err"
cat >> $output_script << EOF
#MSUB -e $stripped_output 
#MSUB -l walltime=00:00:05:00
#MSUB -l nodes=4:ppn=20
EOF

  start_header


}


#=============================================================================
create_target_header()
{
  exit_status=0 # exit status of database, 0=ok, 1=target not known

  cdo="cdo"
  cdo_diff="diffn"
  use_OMP_SCHEDULE="static"

  if [[ "$use_mpi" == "no" ]]; then
    use_nodes=1
    use_mpi_procs_pernode=1
  fi

  if [[ "$use_openmp" == "no" ]]; then
    use_openmp_threads=1
  fi

# set default values
  set_default use_shell "/bin/ksh"
  set_default use_memory_model "default"
      
cat > $output_script << EOF
#!$use_shell
#=============================================================================
EOF

  set_run_target_${use_target}

}

