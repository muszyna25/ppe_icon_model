!>
!!               The module <i>mo_model_import_domain</i>.
!! provides functionality to import information about the models computational
!! domain. This information is read from several files that were generated by
!! the patch generator programm. The data types describing the model domain are
!! contained in <i>mo_domain_model</i>.
!!
!! @par Revision History
!! Initial version  by: Peter Korn,  MPI-M, Hamburg, June 2005
!! Modification by Thomas Heinze (2006-02-21):
!! - renamed m_modules to mo_modules
!! Modification by Thomas Heinze (2006-09-20):
!! - added method_name grid_and_patch_diagnosis
!! Modification by Pilar Ripodas, DWD, (2007-01-31)
!! - addapted to the new TYPE grid_edges (tangent_orientation added)
!! Modification by Peter Korn,  MPI-M, (2006-12)
!! - implementation of topography and boundary treatment, i.e.
!!   initialization of the grid & patch components that carry
!!   information about topography and the lateral boundaries of
!!   the domain; this is not related to patch boundaries.
!!   topography can either be computed by analytical l,eans or
!!   by reading from database files.
!! Modification by Hui Wan, MPI-M, (2007-02-23)
!! - Subroutine <i>init_import</i> was changed to <i>setup_grid</i>.
!!   Namelist hierarchy_ini was renamed to <i>grid_ctl</i>, moved
!!   from <i>mo_io_utilities</i> to this module and now read from
!!   an external file in subroutine <i>setup_grid</i>.
!! - Some changes in <i>init_ocean_patch_component</i> after
!!   discussion with Peter.
!! - Calculation of the min. primal edge length was added to
!!   <i>import_patches</i>. However, shouldn't it be an array with
!!   one element for each patch, rather than a scalar?
!! Modification by P. Ripodas, DWD, (2007-03-14):
!! - Now the output of "import_patches" is the min_dual_edge_lenght
!!   instead of the min_primal_edge_lenght. It will be used to set
!!   the horizontal diffusion parameter. Now it is done as it was
!!   in the prototype.
!! Modification by Almut Gassmann, MPI-M (2007-04)
!! - removed loptimize to make compatible with new grid generator
!! - removed itoa for good programming style
!! - reorganized patch input to be compatible with the new patch generator
!! - cleaning up "destruct_patches"
!! Modification by Almut Gassmann, MPI-M (2007-04-13)
!! - remove grid type and perform related adaptations
!!   (grid information comes now inside a patch)
!! - changed subroutine name form setup_grid to setup_files
!! Modified by Hui Wan, MPI-M, (2008-04-04)
!!  - topography_file_dir renamed topo_file_dir
!!  - for the hydro_atmos, control variable testtype renamed ctest_name.
!! Modified by Almut Gassmann, MPI-M, (2008-04-23)
!!  - itopo distinguishes now shallow water (itopo=1) orography function
!!    from hydro_atmos orography function (itopo=2)
!! Modification by Jochen Foerstner, DWD, (2008-07-16)
!!  - new fields in the derived type for the edges:
!!    grid_edges%primal_cart_normal (Cartesian normal to edge),
!!    grid_edges%quad_idx, grid_edges%quad_area and grid_edges%quad_orientation
!!    (indices of edges and area of the quadrilateral formed by two adjacent cells)
!!    up to now these new fields are initialized in the new routines
!!    calculate_primal_cart_normal and init_quad_twoadjcells
!!    rather than read from a grid/patch file.
!! Modification by Almut Gassmann, MPI-M, (2008-09-21)
!!  - remove reference to mask and height files, they are never used
!!  - use global_cell_type to distinguish cells as triangles or hexagons
!! Modification by Almut Gassmann, MPI-M (2008-10-30)
!!  - add subroutine init_coriolis to initialize Coriolis parameter
!! Modification by Stephan Lorenz, MPI-M (2010-02-18)
!!  - add subroutine init_ocean_patch to initialize ocean patch extensions
!! Modification by Daniel Reinert, DWD (2010-07-21)
!! - removed call of init_topography. This will be handeled within init_ext_data
!!
!! @par Copyright and License
!!
!! This code is subject to the DWD and MPI-M-Software-License-Agreement in
!! its most recent form.
!! Please see the file LICENSE in the root of the source tree for this code.
!! Where software is supplied by third parties, it is indicated in the
!! headers of the routines.
!!
!!

!----------------------------
#include "omp_definitions.inc"
!----------------------------

MODULE mo_model_domimp_patches
  !-------------------------------------------------------------------------

  USE mo_kind,               ONLY: wp
  !USE mo_io_units,           ONLY: filename_max
  USE mo_impl_constants,     ONLY: success,                &
    &                              min_rlcell, max_rlcell, &
    &                              min_rledge, max_rledge, &
    &                              min_rlvert, max_rlvert, &
    &                              max_dom,                &
    &                              min_rlcell_int,         &
    &                              min_rledge_int,         &
    &                              min_rlvert_int
  USE mo_exception,          ONLY: message_text, message, warning, finish, em_warn
  USE mo_model_domain,       ONLY: t_patch, t_pre_patch, p_patch_local_parent, &
       c_num_edges, c_parent, c_child, c_phys_id, c_neighbor, c_edge, &
       c_vertex, c_center, c_refin_ctrl, e_parent, e_child, e_cell, &
       e_refin_ctrl, v_cell, v_num_edges, v_vertex, v_refin_ctrl
  USE mo_decomposition_tools,ONLY: t_glb2loc_index_lookup, &
    &                              get_valid_local_index, &
    &                              t_grid_domain_decomp_info, get_local_index
  USE mo_parallel_config,    ONLY: nproma, p_test_run
  USE mo_model_domimp_setup, ONLY: init_quad_twoadjcells, init_coriolis, &
    & set_verts_phys_id, init_butterfly_idx, fill_grid_subsets
  USE mo_grid_tools,         ONLY: calculate_patch_cartesian_positions, rescale_grid
  USE mo_grid_config,        ONLY: start_lev, nroot, n_dom, n_dom_start, &
    & l_limited_area, max_childdom, dynamics_parent_grid_id, &
    & lplane, grid_length_rescale_factor, is_plane_torus, grid_sphere_radius, &
    & use_duplicated_connectivity
  USE mo_dynamics_config,    ONLY: lcoriolis
  USE mo_run_config,         ONLY: grid_generatingCenter, grid_generatingSubcenter, &
    &                              number_of_grid_used, msg_level, check_uuid_gracefully
  USE mo_master_control,     ONLY: my_process_is_ocean
  USE mo_sync,               ONLY: disable_sync_checks, enable_sync_checks
  USE mo_communication,      ONLY: idx_no, blk_no, idx_1d, makeScatterPattern
  USE mo_util_uuid,          ONLY: uuid_string_length, uuid_parse, clear_uuid
  USE mo_name_list_output_config, ONLY: is_grib_output

  USE mo_grid_geometry_info, ONLY: planar_torus_geometry, sphere_geometry, &
    &  set_grid_geometry_derived_info, copy_grid_geometry_info,            &
    & parallel_read_geometry_info, triangular_cell, planar_channel_geometry
  USE mo_alloc_patches,      ONLY: set_patches_grid_filename, &
    & allocate_pre_patch, allocate_remaining_patch
  USE mo_math_constants,     ONLY: pi
  USE mo_reorder_patches,    ONLY: reorder_cells, reorder_edges, &
    &                              reorder_verts
  USE mo_mpi,                ONLY: p_pe_work, my_process_is_mpi_parallel, &
    &                              p_comm_work_test, p_comm_work
  USE mo_reshuffle, ONLY: reshuffle
#ifdef HAVE_PARALLEL_NETCDF
  USE mo_mpi,                ONLY: p_comm_input_bcast
#endif
  USE mo_complete_subdivision, ONLY: generate_comm_pat_cvec1
  USE mo_read_netcdf_distributed, ONLY: setup_distrib_read
  USE mo_read_interface, ONLY: t_stream_id, p_t_patch, openInputFile, &
    &                          closeFile, on_cells, on_edges, on_vertices, &
    &                          var_data_2d_int, var_data_2d_wp, &
    &                          var_data_3d_int, var_data_3d_wp, &
    &                          read_2D, read_2D_int, read_2D_extdim, &
    &                          read_2D_extdim_int
#ifndef __NO_ICON_ATMO__
  USE mo_interpol_config,    ONLY: nudge_zone_width
#endif
  USE ppm_distributed_array,  ONLY: dist_mult_array_local_ptr, &
    &                               dist_mult_array_expose

#ifndef NOMPI
  ! The USE statement below lets this module use the routines from
  ! mo_netcdf_parallel where only 1 processor is reading and
  ! broadcasting the results
#ifndef HAVE_PARALLEL_NETCDF
  USE mo_netcdf_parallel, ONLY:                      &
    & nf_nowrite, nf_global, nf_noerr, nf_strerror,  &
    & nf_inq_attid        => p_nf_inq_attid,          &
    & nf_open             => p_nf_open,               &
    & nf_close            => p_nf_close,              &
    & nf_inq_dimid        => p_nf_inq_dimid,          &
    & nf_inq_dimlen       => p_nf_inq_dimlen,         &
    & nf_inq_varid        => p_nf_inq_varid,          &
    & nf_get_att_text     => p_nf_get_att_text,       &
    & nf_get_att_int      => p_nf_get_att_int,        &
    & nf_get_var_int      => p_nf_get_var_int,        &
    & nf_get_vara_int     => p_nf_get_vara_int,       &
    & nf_get_var_double   => p_nf_get_var_double,     &
    & nf_get_vara_double  => p_nf_get_vara_double_
#endif
#endif
#ifndef NOMPI
#ifdef __SUNPRO_F95
    INCLUDE "mpif.h"
#else
    USE mpi, ONLY: MPI_INFO_NULL
#endif
#endif


  IMPLICIT NONE

  PRIVATE

#if defined(NOMPI) || defined(HAVE_PARALLEL_NETCDF)
  INCLUDE 'netcdf.inc'
#endif

  !modules interface-------------------------------------------
  !subroutines
  PUBLIC :: import_pre_patches
  PUBLIC :: complete_patches
  PUBLIC :: reorder_patch_refin_ctrl

  INTEGER, SAVE :: ishift_child_id

  !-------------------------------------------------------------------------

CONTAINS

  !-------------------------------------------------------------------------
  !>
  !!               This subroutine provides patch information to the model.
  !!
  !! Which data are required by the model is described in module
  !! <i>mo_model_domain</i>. The components of the patch are initialized
  !! with data stored in several patch files.
  !!
  !! @par Revision History
  !! Developed  by  Peter Korn, MPI-M (2005).
  !! @par
  !! Modified by L. Bonaventura, MPI-M (2005),
  !! to match completed patch
  !! structure in advanced patch generator.
  !! Modified by A. Gassmann, MPI-M (2007)
  !! - cleaning up the code
  !! Modified by A. Gassmann, MPI-M (2007-04)
  !! - grid information belongs now to the patch type
  !! Modified by Almut Gassmann, MPI-M (2008-09-21)
  !! - min_dual_edge_length no longer needed for new Diffusion
  !! Modified by Almut Gassmann, MPI-M (2008-10-30)
  !! - new subroutine for Coriolis initialization
  !! Modification by Stephan Lorenz, MPI-M (2010-02-06)
  !!  - new subroutine for initialization of ocean patch
  !! Modification by Rainer Johanni (2011-12-04)
  !!  - renamed to import_basic_patches
  !!  - only basic patch information for subdivision is read here
  !!    into the full (undivided, global) patch data structure
  !!
  SUBROUTINE import_pre_patches( patch_pre,num_lev,nshift,lsep_grfinfo)

    INTEGER,                   INTENT(in)    :: num_lev(:), nshift(:)
    TYPE(t_pre_patch), TARGET, INTENT(inout) :: patch_pre(n_dom_start:)
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,                   INTENT(OUT)   :: lsep_grfinfo
    ! local variables:
    CHARACTER(LEN=*), PARAMETER :: method_name = 'mo_model_domimp_patches/import_basic_patch'
    INTEGER                           :: jg, jg1, n_chd, n_chdc
    INTEGER                           :: jgp, jgc            ! parent/child patch index
    CHARACTER(LEN=uuid_string_length) :: uuid_grid(0:max_dom), &
      &                                  uuid_par(0:max_dom),  &
      &                                  uuid_chi(0:max_dom,5)
    TYPE(t_pre_patch), POINTER        :: p_single_patch => NULL()

    !-----------------------------------------------------------------------

    CALL message ('mo_model_domimp_patches:import_pre_patches', &
      & 'start to import patches')

    ! Set some basic flow control variables on the patch

    max_childdom = 0

    IF(n_dom_start==0) THEN
      ! The physics parent (parent of the root patch) should also be read
      patch_pre(0)%id = 0
      patch_pre(0)%level = start_lev-1
      patch_pre(0)%parent_id = -1
      patch_pre(0)%parent_child_index = 0
      patch_pre(0)%n_childdom = 1
      patch_pre(0)%n_chd_total = n_dom
      patch_pre(0)%child_id(1) = 1
      DO jg = 1, n_dom
        patch_pre(0)%child_id_list(jg) = jg
      ENDDO
      patch_pre(1)%parent_child_index = 1
    ELSE
      patch_pre(1)%parent_child_index = 0
    ENDIF

    DO jg = 1, n_dom

      patch_pre(jg)%id = jg

      IF (jg == 1) THEN
        patch_pre(jg)%level = start_lev
        patch_pre(jg)%parent_id = 0
      ELSE
        patch_pre(jg)%level = patch_pre(dynamics_parent_grid_id(jg))%level + 1
        patch_pre(jg)%parent_id = dynamics_parent_grid_id(jg)
      ENDIF

      n_chd = 0

      DO jg1 = jg+1, n_dom
        IF (jg == dynamics_parent_grid_id(jg1)) THEN
          n_chd = n_chd + 1
          patch_pre(jg)%child_id(n_chd) = jg1
          patch_pre(jg1)%parent_child_index = n_chd
        ENDIF
      ENDDO

      patch_pre(jg)%n_childdom = n_chd
      max_childdom = MAX(1,max_childdom,n_chd)

      !
      ! store information about vertical levels
      !
      patch_pre(jg)%nlev   = num_lev(jg)
      patch_pre(jg)%nlevp1 = num_lev(jg) + 1

      IF (jg > 1) THEN
        IF (nshift(jg) > 0 ) THEN
          ! nshift has been modified via Namelist => use it
          patch_pre(jg)%nshift = nshift(jg)
        ELSE
          ! set default value, assuming
          !- superimposed vertical levels
          !- 1 nested domain per grid level
          patch_pre(jg)%nshift = num_lev(patch_pre(jg)%parent_id) - num_lev(jg)
        ENDIF

        jgp = patch_pre(jg)%parent_id
        patch_pre(jg)%nshift_total = patch_pre(jgp)%nshift_total + patch_pre(jg)%nshift
      ELSE
        ! Note: the first nshift-value refers to the global domain
        patch_pre(jg)%nshift = 0
        patch_pre(jg)%nshift_total = 0
      ENDIF

    ENDDO

    ! Set information about total number of child domains (called recursively)
    ! and corresponding index lists

    ! Initialization
    DO jg = 1, n_dom
      patch_pre(jg)%n_chd_total      = 0
      patch_pre(jg)%child_id_list(:) = 0
    ENDDO

    DO jg = n_dom, 2, -1
      jg1 = patch_pre(jg)%parent_id
      n_chd = patch_pre(jg1)%n_chd_total
      n_chdc = patch_pre(jg)%n_chd_total
      patch_pre(jg1)%child_id_list(n_chd+1) = jg
      IF (n_chdc > 0) THEN
        patch_pre(jg1)%child_id_list(n_chd+2:n_chd+1+n_chdc) = &
          patch_pre(jg)%child_id_list(1:n_chdc)
      ENDIF
      patch_pre(jg1)%n_chd_total = n_chd+1+n_chdc
    ENDDO


    DO jg = 1, n_dom

      ! make nshift parameter also available for the parent patch
      IF (patch_pre(jg)%n_childdom >= 1) THEN
        patch_pre(jg)%nshift_child = patch_pre(patch_pre(jg)%child_id(1))%nshift
        DO jg1 = 1, patch_pre(jg)%n_childdom
          IF (patch_pre(patch_pre(jg)%child_id(jg1))%nshift /= &
            & patch_pre(jg)%nshift_child) &
            & CALL finish ('mo_model_domimp_patches:import_pre_patches', &
            & 'multiple nests at the same level must have the same nshift')
        ENDDO
      ELSE
        patch_pre(jg)%nshift_child = 0
      ENDIF

    ENDDO

    IF (n_dom_start == 0) THEN ! reduced grid for radiation
      ! In case of n_dom_start == 0 nlev, nlevp1, nshift need to be copied from
      ! jg=1 to jg=0
      patch_pre(0)%nlev   = patch_pre(1)%nlev
      patch_pre(0)%nlevp1 = patch_pre(1)%nlevp1
      patch_pre(0)%nshift = patch_pre(1)%nshift
      ! The reduced grid always has the same levels as the global one
      patch_pre(0)%nshift_child = 0
    ENDIF

    patch_pre(n_dom_start:n_dom)%max_childdom =  max_childdom


    !init patch by reading data from file
    !required: path to patch directory and file names, see top of module
    ! l_exist = .FALSE.

    ! IF (lplane) THEN
    !   gridtype='plan'
    ! ELSE
    !   gridtype='icon'
    ! END IF

    CALL set_patches_grid_filename(patch_pre)

    ishift_child_id = 0

    grid_level_loop: DO jg = n_dom_start, n_dom

      !   jlev = patch_pre(jg)%level

      ! Allow file names without "DOM" specifier if n_dom=1.
      !   IF (n_dom == 1) THEN
      !     ! Check if file name without "DOM" specifier exists.
      !     WRITE (patch_file,'(a,a,i0,a,i2.2,a)') &
      !          & TRIM(gridtype),'R',nroot,'B',jlev,'-grid.nc'
      !     INQUIRE (FILE=patch_file, EXIST=l_exist)
      !     ! Otherwise use file name with "DOM" specifier
      !     IF (.NOT. l_exist)                                            &
      !          & WRITE (patch_file,'(a,a,i0,2(a,i2.2),a)')              &
      !          & TRIM(gridtype),'R',nroot,'B',jlev,'_DOM',jg,'-grid.nc'
      !   ELSE
      !     ! n_dom >1 --> "'_DOM',jg" required in file name
      !     WRITE (patch_file,'(a,a,i0,2(a,i2.2),a)') &
      !          & TRIM(gridtype),'R',nroot,'B',jlev,'_DOM',jg,'-grid.nc'
      !   ENDIF


      p_single_patch => patch_pre(jg)

      CALL read_pre_patch( jg, p_single_patch, uuid_grid(jg), uuid_par(jg), uuid_chi(jg,:), lsep_grfinfo )

    ENDDO grid_level_loop

    IF (lsep_grfinfo) THEN ! perform uuid crosscheck for parent-child connectivities
      DO jg = n_dom_start, n_dom
        IF (jg > n_dom_start) THEN
          jgp = patch_pre(jg)%parent_id
          IF (TRIM(uuid_par(jg)) /= TRIM(uuid_grid(jgp))) THEN
            IF (check_uuid_gracefully) THEN
              CALL warning('import_pre_patches','incorrect uuids in parent-child connectivity file')
            ELSE
              CALL finish('import_pre_patches','incorrect uuids in parent-child connectivity file')
            END IF
          ENDIF
        ENDIF
      ENDDO
    ENDIF

  END SUBROUTINE import_pre_patches
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  !>
  !! This method_name completes basic patches by
  !! - allocating the remaining arrays
  !! - reading the remaining arrays which are not in the basic patch
  !! - calculating arrays which are not read from input file
  !! - generate basic data structures required for the distributed
  !!   read operation

  SUBROUTINE complete_patches(patch, is_ocean_decomposition, lsep_grfinfo)

    TYPE(t_patch), TARGET,     INTENT(inout) :: patch(n_dom_start:)
    LOGICAL,                   INTENT(IN)    :: is_ocean_decomposition
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,                   INTENT(IN)    :: lsep_grfinfo

    INTEGER :: jg, jgp, n_lp, id_lp(max_dom)
    CHARACTER(LEN=*), PARAMETER :: method_name = 'mo_model_domimp_patches:complete_patches'

    DO jg = n_dom_start, n_dom

      ! Allocate and preset remaining arrays in patch
      ! operation mode=3: allocate all but the parallelization-related arrays
      !                   (the parallelization-related arrays are allocated in mo_setup_subdivision)
      CALL allocate_remaining_patch(patch(jg),3)
      IF (jg > n_dom_start)  &
        CALL allocate_remaining_patch(p_patch_local_parent(jg),3)
    ENDDO

    DO jg = n_dom_start, n_dom

      ALLOCATE(patch(jg)%cells%dist_io_data, patch(jg)%edges%dist_io_data, &
        &      patch(jg)%verts%dist_io_data)

      CALL setup_distrib_read(patch(jg)%n_patch_cells_g, &
                              patch(jg)%cells%decomp_info, &
                              patch(jg)%cells%dist_io_data)
      CALL setup_distrib_read(patch(jg)%n_patch_edges_g, &
                              patch(jg)%edges%decomp_info, &
                              patch(jg)%edges%dist_io_data)
      CALL setup_distrib_read(patch(jg)%n_patch_verts_g, &
                              patch(jg)%verts%decomp_info, &
                              patch(jg)%verts%dist_io_data)
      IF (jg > n_dom_start) THEN

        ALLOCATE(p_patch_local_parent(jg)%cells%dist_io_data, &
          &      p_patch_local_parent(jg)%edges%dist_io_data, &
          &      p_patch_local_parent(jg)%verts%dist_io_data)

        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_cells_g, &
                                p_patch_local_parent(jg)%cells%decomp_info, &
                                p_patch_local_parent(jg)%cells%dist_io_data)
        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_edges_g, &
                                p_patch_local_parent(jg)%edges%decomp_info, &
                                p_patch_local_parent(jg)%edges%dist_io_data)
        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_verts_g, &
                                p_patch_local_parent(jg)%verts%decomp_info, &
                                p_patch_local_parent(jg)%verts%dist_io_data)
      END IF
    ENDDO

    ! initialise scatter patterns (required for io used in read_remaining_patch)
    DO jg = n_dom_start, n_dom
      CALL set_comm_pat_scatter(patch(jg), jg)
      IF (jg > n_dom_start) &
        CALL set_comm_pat_scatter(p_patch_local_parent(jg), jg)
    ENDDO

    ! Fill the subsets information
    DO jg = n_dom_start, n_dom
      CALL fill_grid_subsets(patch(jg))
      IF (jg > n_dom_start)  CALL fill_grid_subsets( p_patch_local_parent(jg) )
    ENDDO

    ! we read in the remaining data before generating the communication patterns
    ! because some data that is read in by allocate_remaining_patch is required
    ! by complete_parallel_setup
    DO jg = n_dom_start, n_dom
      n_lp = 0 ! Number of local parents on the same level
      ! Assemble a list of local parents living on the same level as the current patch
      DO jgp = n_dom_start+1, n_dom
        IF(patch(jgp)%parent_id == jg) THEN
          n_lp = n_lp+1
          id_lp(n_lp) = jgp  ! these are children of the current patch
        ENDIF
      ENDDO

      ! Get all patch information not read by read_pre_patch
      CALL read_remaining_patch( jg, patch(jg), n_lp, id_lp, lsep_grfinfo )
    ENDDO

    ! set parent-child relationships
    DO jg = n_dom_start, n_dom

      IF(jg == n_dom_start) THEN

        ! parent_loc/glb_idx/blk is set to 0 since it just doesn't exist,
        patch(jg)%cells%parent_glb_idx = 0
        patch(jg)%cells%parent_glb_blk = 0
        patch(jg)%edges%parent_glb_idx = 0
        patch(jg)%edges%parent_glb_blk = 0
        patch(jg)%cells%parent_loc_idx = 0
        patch(jg)%cells%parent_loc_blk = 0
        patch(jg)%edges%parent_loc_idx = 0
        patch(jg)%edges%parent_loc_blk = 0

        ! For parallel runs, child_idx/blk is set to 0 since it makes
        ! sense only on the local parent
        IF (.NOT. my_process_is_mpi_parallel() .OR. &
            is_ocean_decomposition) THEN
          patch(jg)%cells%child_idx  = 0
          patch(jg)%cells%child_blk  = 0
          patch(jg)%edges%child_idx  = 0
          patch(jg)%edges%child_blk  = 0
        END IF

      ELSE

        CALL set_parent_child_relations(p_patch_local_parent(jg), patch(jg))
      ENDIF
    END DO

    DO jg = n_dom_start, n_dom
      CALL set_owner_mask(patch(jg)%cells%decomp_info)
      CALL set_owner_mask(patch(jg)%verts%decomp_info)
      CALL set_owner_mask(patch(jg)%edges%decomp_info)
      IF (jg > n_dom_start) THEN
        CALL set_owner_mask(p_patch_local_parent(jg)%cells%decomp_info)
        CALL set_owner_mask(p_patch_local_parent(jg)%verts%decomp_info)
        CALL set_owner_mask(p_patch_local_parent(jg)%edges%decomp_info)
      END IF
    ENDDO

    ! rescale grids
    DO jg = n_dom_start, n_dom
      CALL rescale_grid( patch(jg), grid_length_rescale_factor  )
      IF (jg > n_dom_start)  CALL rescale_grid( p_patch_local_parent(jg), grid_length_rescale_factor )
    ENDDO

    ! do other stuff
    DO jg = n_dom_start, n_dom
      CALL set_verts_phys_id( patch(jg) )
    ENDDO

    ! generates comm_pat_c/v/e/c1 required by the following initialization
    ! routines (these patterns will be rebuild after the reordering by routine
    ! complete_parallel_setup)
    CALL generate_comm_pat_cvec1(patch, is_ocean_decomposition)

    IF (.not. my_process_is_ocean()) THEN
      DO jg = n_dom_start, n_dom
        ! Initialize the data for the quadrilateral cells
        ! formed by the two adjacent cells of an edge.
        ! (later this should be provided by the grid generator)
        CALL init_quad_twoadjcells( patch(jg) )
        ! Initialize butterfly data structure, formed by the
        ! 4 cells sharing the 2 vertices which bound a given edge.
        IF (patch(jg)%geometry_info%cell_type == 3) THEN
          ! not useful for hexagonal grid
          CALL init_butterfly_idx( patch(jg) )
        ENDIF

        CALL init_coriolis( lcoriolis, lplane, patch(jg) )

        ! The same has to be done for local parents in parallel runs
        !
        ! Please note: The call to init_quad_twoadjcells involves boundary
        ! exchange and is not repeated here for local parents.
        ! The arrays calculated there are transfered to the local parent
        ! in transfer_interpol_state where also a lot of other arrays
        ! from the patch state are transferred

        IF (jg>n_dom_start) THEN
          CALL disable_sync_checks
          CALL init_coriolis( lcoriolis, lplane, p_patch_local_parent(jg) )
          CALL set_verts_phys_id( p_patch_local_parent(jg) )
          CALL enable_sync_checks
        ENDIF

      ENDDO
    ENDIF

  CONTAINS

    !-------------------------------------------------------------------------------------------------
    !> Sets the gather communication patterns of a patch

    SUBROUTINE set_comm_pat_scatter(p, jg)
      TYPE(t_patch), INTENT(INOUT):: p
	  INTEGER, VALUE :: jg

      INTEGER :: communicator

      IF(p_test_run) THEN
          communicator = p_comm_work_test
      ELSE
          communicator = p_comm_work
      ENDIF

      p%comm_pat_scatter_c => &
        makeScatterPattern(jg, p%n_patch_cells, p%cells%decomp_info%glb_index, &
        &                  communicator)
      p%comm_pat_scatter_e => &
        makeScatterPattern(jg, p%n_patch_edges, p%edges%decomp_info%glb_index, &
        &                  communicator)
      p%comm_pat_scatter_v => &
        makeScatterPattern(jg, p%n_patch_verts, p%verts%decomp_info%glb_index, &
        &                  communicator)

    END SUBROUTINE set_comm_pat_scatter

  END SUBROUTINE complete_patches

  !-------------------------------------------------------------------------------------------------
  !
  !> Sets parent_loc_idx/blk in child and child_idx/blk in parent patches.

  SUBROUTINE set_parent_child_relations(p_pp, p_pc)

    TYPE(t_patch), INTENT(INOUT) :: p_pp   !> divided local parent patch
    TYPE(t_patch), INTENT(INOUT) :: p_pc   !> divided child patch

    INTEGER :: i, j, jl, jb, jc, jc_g, jp, jp_g, ierr 
    INTEGER                           :: jc_c, jb_c, jc_p, jb_p, communicator
    INTEGER, ALLOCATABLE :: in_child_id(:), parent_glb_idx(:), out_child_id(:)

    ! Before this call, child_idx/child_blk still point to the global values.
    ! This is changed here.

    ! Attention:
    ! Only inner cells/edges get a valid child index,
    ! indexes for boundary cells/edges are not set.
    ! Therefore when these indexes are used, the code must assure that they are
    ! used only for inner cells/edges!
    ! The main reason for this is that - depending on the number of ghost rows -
    ! there are cells/edges in the parent boundary with missing childs (n_ghost_rows==1)

    ! Set child indices in parent ...

    ! ... cells

    DO j = 1, p_pp%n_patch_cells

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      IF(p_pp%cells%decomp_info%decomp_domain(jl,jb)>0) THEN
        p_pp%cells%child_idx(jl,jb,:) = 0
        p_pp%cells%child_blk(jl,jb,:) = 0
        CYCLE
      ENDIF

      DO i= 1, 4
        jc_g = idx_1d(p_pp%cells%child_idx(jl,jb,i),p_pp%cells%child_blk(jl,jb,i))
        IF(jc_g<1 .OR. jc_g>p_pc%n_patch_cells_g) &
          & CALL finish('set_parent_child_relations','Invalid cell child index in global parent')
        jc = get_local_index(p_pc%cells%decomp_info%glb2loc_index, jc_g)
        IF(jc <= 0) &
          & CALL finish('set_parent_child_relations','cell child index outside child domain')
        p_pp%cells%child_blk(jl,jb,i) = blk_no(jc)
        p_pp%cells%child_idx(jl,jb,i) = idx_no(jc)
      ENDDO

    ENDDO

    ! ... edges

    DO j = 1, p_pp%n_patch_edges

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      IF(p_pp%edges%decomp_info%decomp_domain(jl,jb)>1) THEN
        p_pp%edges%child_idx(jl,jb,:) = 0
        p_pp%edges%child_blk(jl,jb,:) = 0
        CYCLE ! only inner edges get a valid parent index
      ENDIF

      DO i= 1, 4

        IF(i==4 .AND. p_pp%edges%refin_ctrl(jl,jb) == -1) THEN
          p_pp%edges%child_blk(jl,jb,i) = blk_no(0)
          p_pp%edges%child_idx(jl,jb,i) = idx_no(0)
          CYCLE
        ENDIF

        jc_g = idx_1d(p_pp%edges%child_idx(jl,jb,i),p_pp%edges%child_blk(jl,jb,i))
        jc = get_valid_local_index(p_pc%edges%decomp_info%glb2loc_index, &
          &                        jc_g, .TRUE.)
        IF(jc == 0) &
          & CALL finish('set_parent_child_relations','edge child index outside child domain')
        p_pp%edges%child_blk(jl,jb,i) = blk_no(jc)
        p_pp%edges%child_idx(jl,jb,i) = SIGN(idx_no(jc),jc_g)
      ENDDO

      p_pp%edges%child_id(jl,jb) = p_pc%id

    ENDDO

    ! Set parent indices in child ...

    ! ... cells

    DO j = 1, p_pc%n_patch_cells

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      jp_g = idx_1d(p_pc%cells%parent_glb_idx(jl,jb), &
        &           p_pc%cells%parent_glb_blk(jl,jb))
      IF(jp_g<1 .OR. jp_g>p_pp%n_patch_cells_g) &
        & CALL finish('set_parent_child_relations','Inv. cell parent index in global child')

      jp = get_local_index(p_pp%cells%decomp_info%glb2loc_index, jp_g)
      IF(jp <= 0) THEN
        p_pc%cells%parent_loc_blk(jl,jb) = 0
        p_pc%cells%parent_loc_idx(jl,jb) = 0
      ELSE
        p_pc%cells%parent_loc_blk(jl,jb) = blk_no(jp)
        p_pc%cells%parent_loc_idx(jl,jb) = idx_no(jp)
      ENDIF

    ENDDO

    ! TODO [FP]
    !
    ! calculate cells%child_id
    !
    ! To calculate the child_id for each child cell, we loop over the
    ! cells of the child domain and collect the global indices of
    ! their parent cells. Then we send the ID of the child domain to
    ! these parent cells. This involves parallel comm., since the
    ! child cells and their parent cells may "live" on different PEs.

    IF(p_test_run) THEN
      communicator = p_comm_work_test
    ELSE
      communicator = p_comm_work
    ENDIF

    ALLOCATE(parent_glb_idx(p_pc%n_patch_cells), in_child_id(p_pc%n_patch_cells), &
      &      out_child_id(p_pp%n_patch_cells))
    DO j = 1, p_pc%n_patch_cells
      jc_c = idx_no(j)
      jb_c = blk_no(j)
      parent_glb_idx(j) = idx_1d(p_pc%cells%parent_glb_idx(jc_c,jb_c), &
        &                          p_pc%cells%parent_glb_blk(jc_c,jb_c))
    END DO

    in_child_id(:) = p_pc%id
    CALL reshuffle(parent_glb_idx, in_child_id, p_pp%cells%decomp_info%glb_index, &
      &            p_pp%n_patch_cells_g, communicator, out_child_id, ierr)
    IF (ierr /= 0)  CALL finish('', 'ierr /= 0')

    DO j = 1, p_pp%n_patch_cells
      jc_p = idx_no(j)
      jb_p = blk_no(j)
      IF (out_child_id(j) /= p_pp%cells%child_id(jc_p,jb_p)) THEN
        WRITE (0,*) "out_child_id(j) /= p_pp%cells%child_id(jc_p,jb_p): ", &
          &  out_child_id(j), p_pp%cells%child_id(jc_p,jb_p), jc_p, jb_p
      END IF
    END DO
    DEALLOCATE(in_child_id, parent_glb_idx, out_child_id)

    ! ... edges

    DO j = 1, p_pc%n_patch_edges

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      jp_g = idx_1d(p_pc%edges%parent_glb_idx(jl,jb), &
        &           p_pc%edges%parent_glb_blk(jl,jb))
      IF(jp_g<1 .OR. jp_g>p_pp%n_patch_edges_g) &
        & CALL finish('set_parent_child_relations','Inv. edge parent index in global child')

      jp = get_local_index(p_pp%edges%decomp_info%glb2loc_index, jp_g)
      IF(jp <= 0) THEN
        p_pc%edges%parent_loc_blk(jl,jb) = 0
        p_pc%edges%parent_loc_idx(jl,jb) = 0
      ELSE
        p_pc%edges%parent_loc_blk(jl,jb) = blk_no(jp)
        p_pc%edges%parent_loc_idx(jl,jb) = idx_no(jp)
      ENDIF

    ENDDO

    ! TODO [FP]
    !
    ! calculate edges%child_id
    !
    ! To calculate the child_id for each child edge, we loop over the
    ! edges of the child domain and collect the global indices of
    ! their parent edges. Then we send the ID of the child domain to
    ! these parent edges. This involves parallel comm., since the
    ! child edges and their parent edges may "live" on different PEs.

    ALLOCATE(parent_glb_idx(p_pc%n_patch_edges), in_child_id(p_pc%n_patch_edges), &
      &      out_child_id(p_pp%n_patch_edges))
    DO j = 1, p_pc%n_patch_edges
      jc_c = idx_no(j)
      jb_c = blk_no(j)
      parent_glb_idx(j) = idx_1d(p_pc%edges%parent_glb_idx(jc_c,jb_c), &
        &                        p_pc%edges%parent_glb_blk(jc_c,jb_c))
    END DO

    in_child_id(:) = p_pc%id
    CALL reshuffle(parent_glb_idx, in_child_id, p_pp%edges%decomp_info%glb_index, &
      &            p_pp%n_patch_edges_g, communicator, out_child_id, ierr)
    IF (ierr /= 0)  CALL finish('', 'ierr /= 0')

    DO j = 1, p_pp%n_patch_edges
      jc_p = idx_no(j)
      jb_p = blk_no(j)
      IF (out_child_id(j) /= p_pp%edges%child_id(jc_p,jb_p)) THEN
        WRITE (0,*) "out_child_id(j) /= p_pp%edges%child_id(jc_p,jb_p): ", &
          &  out_child_id(j), p_pp%edges%child_id(jc_p,jb_p), jc_p, jb_p
      END IF
    END DO
    DEALLOCATE(in_child_id, parent_glb_idx, out_child_id)

    ! Although this is not really necessary, we set the child index in child
    ! and the parent index in parent to 0 since these have no significance
    ! in the parallel code (and must not be used as they are).

    IF (my_process_is_mpi_parallel()) THEN
      p_pc%cells%child_idx  = 0
      p_pc%cells%child_blk  = 0
      p_pp%cells%parent_glb_idx = 0
      p_pp%cells%parent_glb_blk = 0
      p_pp%cells%parent_loc_idx = 0
      p_pp%cells%parent_loc_blk = 0

      p_pc%edges%child_idx  = 0
      p_pc%edges%child_blk  = 0
      p_pp%edges%parent_glb_idx = 0
      p_pp%edges%parent_glb_blk = 0
      p_pp%edges%parent_loc_idx = 0
      p_pp%edges%parent_loc_blk = 0
    END IF

  END SUBROUTINE set_parent_child_relations

  !-----------------------------------------------------------------------------
  !>
  !! Sets the owner mask
  SUBROUTINE set_owner_mask(decomp_info)

    TYPE(t_grid_domain_decomp_info), INTENT(inout) :: decomp_info

    INTEGER :: j, jb, jl

    decomp_info%owner_mask = .false.

    DO j = 1, SIZE(decomp_info%glb_index)

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch
      decomp_info%owner_mask(jl,jb) = decomp_info%owner_local(j) == p_pe_work
    ENDDO
  END SUBROUTINE set_owner_mask
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !> reorder patch data structure according to the refin_ctrl flags.
  !
  !  Only interior cells/edges/vertices are sorted in decreasing
  !  order.
  !
  !  @author F. Prill, DWD (2013-07-31)
  !
  !  @todo OpenMP parallelization!
  !
  !
  SUBROUTINE reorder_patch_refin_ctrl(patch, child_patch)
    TYPE(t_patch),         INTENT(INOUT) :: patch, child_patch   ! patch data structures
    ! local variables
    INTEGER, ALLOCATABLE :: old2new(:)
    INTEGER              :: n, irefin, ic, ntot, ninterior

    ! -- reorder cells
    ntot      = idx_1d( patch%cells%end_index(min_rlcell), &
      &                 patch%cells%end_block(min_rlcell) )
    ninterior = idx_1d( patch%cells%start_index(min_rlcell_int-1), &
      &                 patch%cells%start_block(min_rlcell_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rlcell_int,-1
      DO ic = 1,ninterior
        IF (patch%cells%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%cells%end_index(irefin)     = idx_no(n)
      patch%cells%end_block(irefin)     = blk_no(n)
      patch%cells%start_index(irefin-1) = idx_no(n+1)
      patch%cells%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_cells(patch, old2new, opt_child_pp=child_patch)
    DEALLOCATE(old2new)

    ! -- reorder edges
    ntot      = idx_1d( patch%edges%end_index(min_rledge), &
      &                 patch%edges%end_block(min_rledge) )
    ninterior = idx_1d( patch%edges%start_index(min_rledge_int-1), &
      &                 patch%edges%start_block(min_rledge_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rledge_int,-1
      DO ic = 1,ninterior
        IF (patch%edges%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%edges%end_index(irefin)     = idx_no(n)
      patch%edges%end_block(irefin)     = blk_no(n)
      patch%edges%start_index(irefin-1) = idx_no(n+1)
      patch%edges%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_edges(patch, old2new, opt_child_pp=child_patch)
    DEALLOCATE(old2new)

    ! -- reorder verts
    ntot      = idx_1d( patch%verts%end_index(min_rlvert), &
      &                 patch%verts%end_block(min_rlvert) )
    ninterior = idx_1d( patch%verts%start_index(min_rlvert_int-1), &
      &                 patch%verts%start_block(min_rlvert_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rlvert_int,-1
      DO ic = 1,ninterior
        IF (patch%verts%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%verts%end_index(irefin)     = idx_no(n)
      patch%verts%end_block(irefin)     = blk_no(n)
      patch%verts%start_index(irefin-1) = idx_no(n+1)
      patch%verts%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_verts(patch, old2new)
    DEALLOCATE(old2new)

    ! Copy index bounds to the old 2D fields until the restructuring is completed
    DO n = 1, patch%max_childdom
      patch%cells%end_idx(:,n)   = patch%cells%end_index(:)
      patch%cells%end_blk(:,n)   = patch%cells%end_block(:)
      patch%cells%start_idx(:,n) = patch%cells%start_index(:)
      patch%cells%start_blk(:,n) = patch%cells%start_block(:)
      patch%edges%end_idx(:,n)   = patch%edges%end_index(:)
      patch%edges%end_blk(:,n)   = patch%edges%end_block(:)
      patch%edges%start_idx(:,n) = patch%edges%start_index(:)
      patch%edges%start_blk(:,n) = patch%edges%start_block(:)
      patch%verts%end_idx(:,n)   = patch%verts%end_index(:)
      patch%verts%end_blk(:,n)   = patch%verts%end_block(:)
      patch%verts%start_idx(:,n) = patch%verts%start_index(:)
      patch%verts%start_blk(:,n) = patch%verts%start_block(:)
    ENDDO

  END SUBROUTINE reorder_patch_refin_ctrl
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !>
  ! calculate mean geometry properties for old grids,
  ! the new grids should have these values filled
  ! All the patches should have the same geometry type
  SUBROUTINE set_missing_geometry_info( patch )
    TYPE(t_patch), INTENT(inout), TARGET ::  patch

    CHARACTER(LEN=*), PARAMETER :: method_name = 'mo_model_domimp_patches:set_missing_geometry_info'

    !-----------------------------------------------------------------------
    SELECT CASE(patch%geometry_info%geometry_type)

    CASE (planar_torus_geometry, planar_channel_geometry)

      CALL finish(method_name, "planar_torus_geometry should be read from the grid file")

    CASE (sphere_geometry)
      ! if geometry_info is missing then the grid is trianguler by default
      patch%geometry_info%cell_type = triangular_cell
      IF (patch%cells%max_connectivity /= 3) &
        CALL finish("set_missing_geometry_info","cells%max_cell_connectivity /= 3")

      ! note that the grid_sphere_radius is already rescaled
      patch%geometry_info%sphere_radius = grid_sphere_radius / grid_length_rescale_factor
      ! divide the sphere surface by the number of cells
      ! Note: this works only for old grids
      patch%geometry_info%mean_cell_area = &
        & (4._wp * pi * patch%geometry_info%sphere_radius**2) &
        & / (REAL(20*nroot**2,wp)*4._wp**patch%level)

      patch%geometry_info%domain_length  = 2.0_wp * pi * patch%geometry_info%sphere_radius
      patch%geometry_info%domain_height  = patch%geometry_info%domain_length

      ! Note: the mean_edge_length is not used for the sphere geometry,
      ! and calculating will require global communication. Set to 0
      patch%geometry_info%mean_edge_length = 0.0_wp

    CASE default
      CALL finish(method_name, "Undefined geometry type")

    END SELECT

  END SUBROUTINE set_missing_geometry_info

  !-------------------------------------------------------------------------
  !>
  !! Initialization of the patch components with data stored
  !! in files.
  !!
  !! @par Revision History
  !! Developed  by  Peter Korn, MPI-M (2005).
  !! Modified by L. Bonaventura, MPI-M (2005),
  !! to match completed patch
  !! structure in advanced patch generator.
  !! Modified by A. Gassmann, MPI-M (2007-04-03)
  !! - cleaning up and adaptations for reading multiple patches
  !! Modified by A. Gassmann, MPI-M (2007-04-03)
  !! - patch owns grid information, global grid is obsolete.
  !! - changed name from init_patch to read_patch
  !! Modified by A. Gassmann, MPI-M (2008-09-21)
  !! - remove all not netcdf stuff
  !! - HERE we must think of how to use different 'global_cell_type's
  !! Modified by A. Gassmann, MPI-M (2008-10-30)
  !! - read in grid for either triangles or hexagons
  !! Modified by R. Johanni (2011-12-04)
  !! - split into read_basic_patch for reading the basic patch information
  !!   for subdivision into the fully allocated patch and read_remaining_patch
  !!   for reading the remaining information
  !!
  SUBROUTINE read_pre_patch( ig, patch_pre, uuid_grid, uuid_par, uuid_chi, lsep_grfinfo )

    CHARACTER(LEN=*), PARAMETER :: method_name = 'mo_model_domimp_patches:read_pre_patch'
    INTEGER,                           INTENT(in)    ::  ig                  ! domain ID
    TYPE(t_pre_patch), TARGET,         INTENT(inout) ::  patch_pre           ! patch data structure
    CHARACTER(LEN=uuid_string_length), INTENT(inout) :: uuid_grid, uuid_par, uuid_chi(5)
    !> If .true., read fields related to grid refinement from separate  grid files:
    LOGICAL,                           INTENT(OUT)   :: lsep_grfinfo

    ! local variables
    INTEGER, ALLOCATABLE :: &
      & start_idx_c(:,:), end_idx_c(:,:), &  ! temporary arrays to read in index lists
      & start_idx_e(:,:), end_idx_e(:,:), &
      & start_idx_v(:,:), end_idx_v(:,:)

    ! dummy values for number of internal halo cells, edges, vertices, not actually used
!    INTEGER :: n_e_halo_cells
!    INTEGER :: n_e_halo_edges
!    INTEGER :: n_e_halo_verts

    ! INTEGER :: patch_unit

    ! LOGICAL :: lnetcdf = .TRUE.
    ! CHARACTER(len=filename_max) :: file

    CHARACTER(LEN=uuid_string_length) :: uuid_string, uuid_string_grfinfo
    CHARACTER(LEN=1) :: child_id

    ! status variables
    INTEGER :: ist, netcd_status

    INTEGER :: ncid, ncid_grf, dimid, varid, max_cell_connectivity, max_verts_connectivity
    INTEGER :: ji
    INTEGER :: jc, ic
    INTEGER :: icheck, ilev, igrid_level, iparent_id, ipar_id, dim_idxlist
    INTEGER, POINTER :: local_ptr(:), local_ptr_2d(:,:)
    REAL(wp), POINTER :: local_ptr_wp(:), local_ptr_wp_2d(:, :)
    !-----------------------------------------------------------------------

    ! set dummy values to zero
!    n_e_halo_cells = 0
!    n_e_halo_edges = 0
!    n_e_halo_verts = 0

    ilev = patch_pre%level
    ipar_id = patch_pre%parent_id


    CALL message (TRIM(method_name), 'start to init patch_pre')

    WRITE(message_text,'(a,a)') 'Read grid file ', TRIM(patch_pre%grid_filename)
    CALL message ('', TRIM(message_text))

#if HAVE_PARALLEL_NETCDF
    CALL nf(nf_open_par(TRIM(patch_pre%grid_filename), &
       &                IOR(nf_nowrite, nf_mpiio), &
       &                p_comm_input_bcast, MPI_INFO_NULL, ncid))
#else
    CALL nf(nf_open(TRIM(patch_pre%grid_filename), nf_nowrite, ncid))
#endif

    ! Test, if grid refinement information is available in the NetCDF
    ! file. If not, try to open "patch_pre%grid_filename_grfinfo":
    lsep_grfinfo = (nf_inq_varid(ncid, 'refin_c_ctrl', varid) /= nf_noerr)
    IF (lsep_grfinfo) THEN
      WRITE(message_text,'(a,a)') 'Read gridref info from file ', TRIM(patch_pre%grid_filename_grfinfo)
      CALL message ('', TRIM(message_text))
#if HAVE_PARALLEL_NETCDF
      CALL nf(nf_open_par(TRIM(patch_pre%grid_filename_grfinfo), &
         &                IOR(nf_nowrite, nf_mpiio), p_comm_input_bcast, &
         &                MPI_INFO_NULL, ncid_grf))
#else
      CALL nf(nf_open(TRIM(patch_pre%grid_filename_grfinfo), nf_nowrite, ncid_grf))
#endif
    ELSE
      ncid_grf = ncid
    END IF

    uuid_string = 'warning: not given ...' ! To avoid null characters in the standard output

    IF (nf_get_att_text(ncid, nf_global, 'uuidOfHGrid', uuid_string) /= nf_noerr) THEN
      IF (is_grib_output()) THEN
        CALL message(TRIM(method_name), "Warning: uuidOfHGrid not set as an attribute!")
      END IF
      CALL clear_uuid(patch_pre%grid_uuid)
    ELSE
      CALL uuid_parse(uuid_string, patch_pre%grid_uuid)
      WRITE(message_text,'(a,a)') 'grid uuid: ', TRIM(uuid_string)
      CALL message  (TRIM(method_name), message_text)
    END IF

    IF (lsep_grfinfo) THEN ! check correspondence of uuids between main grid file and connectivity info file
      CALL nf(nf_get_att_text(ncid_grf, nf_global, 'uuidOfHGrid', uuid_string_grfinfo))
      IF (TRIM(uuid_string_grfinfo) /= TRIM(uuid_string)) THEN
        WRITE(message_text,'(a,a)') 'uuidOfHGrid of grfinfo file does not match uuidOfHGrid of basic grid file'
        IF (check_uuid_gracefully) THEN
          CALL warning(TRIM(method_name), TRIM(message_text))
        ELSE
          CALL finish (TRIM(method_name), TRIM(message_text))
        END IF
      ENDIF
      uuid_grid = uuid_string_grfinfo
      ! Read also parent and child grid uuids for subsequent crosscheck
      CALL nf(nf_get_att_text(ncid_grf, nf_global, 'uuidOfParHGrid', uuid_par))
    ENDIF

    ! Read additional grid identifiers
    ! grid_generatingCenter
    ! grid_generatingSubcenter
    ! number_of_grid_used
    netcd_status = nf_get_att_int(ncid, nf_global, 'centre', &
      &                           grid_generatingCenter(ig)  )
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'generating center of patch ', ig, ': ',grid_generatingCenter(ig)
      CALL message  (TRIM(method_name), TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: generating center of patch ', ig, ' not found'
      CALL message  (TRIM(method_name), TRIM(message_text))
      ! set default value
      grid_generatingCenter(ig) = 78    ! DWD
    ENDIF

    netcd_status = nf_get_att_int(ncid, nf_global, 'subcentre', &
      &                           grid_generatingSubcenter(ig)  )
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'generating subcenter of patch ', ig, ': ',grid_generatingSubcenter(ig)
      CALL message  (TRIM(method_name), TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: generating subcenter of patch ', ig, ' not found'
      CALL message  (TRIM(method_name), TRIM(message_text))
      ! set default value
      grid_generatingSubcenter(ig) = 255
    ENDIF

    netcd_status = nf_get_att_int(ncid, nf_global, 'number_of_grid_used', &
      &            number_of_grid_used(ig))
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'number_of_grid_used of patch ', ig, ': ',number_of_grid_used(ig)
      CALL message  (TRIM(method_name), TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: number_of_grid_used of patch ', ig, ' not found'
      CALL message  (TRIM(method_name), TRIM(message_text))
      ! set default value
      number_of_grid_used(ig) = 42
    ENDIF

    CALL nf(nf_get_att_int(ncid, nf_global, 'grid_root', icheck))
    IF (icheck /= nroot) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'grid_root attribute:', icheck,', R:',nroot
      CALL message  (TRIM(method_name), TRIM(message_text))
      WRITE(message_text,'(a)') &
        & 'Mismatch between "grid_root" attribute and "R" parameter in the filename'
      CALL finish  (TRIM(method_name), TRIM(message_text))
    END IF

    CALL nf(nf_get_att_int(ncid, nf_global, 'grid_level', igrid_level))
    IF (igrid_level /= ilev) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'grid_level attribute:', igrid_level,', B:',ilev
      CALL message  (TRIM(method_name), TRIM(message_text))
      WRITE(message_text,'(a)') &
        & 'Mismatch between "grid_level" attribute and "B" parameter in the filename'
      CALL finish  (TRIM(method_name), TRIM(message_text))
    END IF

    !--------------------------------------
    ! get number of cells, edges and vertices
    CALL nf(nf_inq_dimid(ncid, 'edge', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_edges_g))
    CALL nf(nf_inq_dimid(ncid, 'cell', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_cells_g))
    CALL nf(nf_inq_dimid(ncid, 'vertex', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_verts_g))
    CALL nf(nf_inq_dimid(ncid, 'nv', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, max_cell_connectivity))
    patch_pre%cells%max_connectivity = max_cell_connectivity
    CALL nf(nf_inq_dimid(ncid, 'ne', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, max_verts_connectivity))
    patch_pre%verts%max_connectivity = max_verts_connectivity
    ! dimension of start/end index list fields (always 1 in new patch files, but this
    ! provides backward compatibility)
    CALL nf(nf_inq_dimid(ncid, 'max_chdom', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, dim_idxlist))
    IF (dim_idxlist>1) THEN
      WRITE(message_text,'(a)') &
        & 'WARNING: you are using an old grid file with multiple nesting'
      CALL message  (TRIM(method_name), TRIM(message_text))
    ENDIF

    !
    ! calculate and save values for the blocking
    !
    ! ... for the cells
    patch_pre%nblks_c = ( patch_pre%n_patch_cells_g - 1 ) / nproma + 1
    patch_pre%npromz_c = patch_pre%n_patch_cells_g - &
      &                  (patch_pre%nblks_c - 1)*nproma
    patch_pre%alloc_cell_blocks = patch_pre%nblks_c

    ! ... for the edges
    patch_pre%nblks_e = ( patch_pre%n_patch_edges_g - 1 ) / nproma + 1
    patch_pre%npromz_e = patch_pre%n_patch_edges_g - &
      &                  (patch_pre%nblks_e - 1) * nproma

    ! ... for the vertices
    patch_pre%nblks_v = ( patch_pre%n_patch_verts_g - 1 ) / nproma + 1
    patch_pre%npromz_v = patch_pre%n_patch_verts_g - &
      &                  (patch_pre%nblks_v - 1)*nproma

    !
    ! allocate temporary arrays to read in data form the grid/patch generator
    !
    ! integer arrays for index lists
    ALLOCATE( start_idx_c(min_rlcell:max_rlcell,dim_idxlist),  &
      & end_idx_c  (min_rlcell:max_rlcell,dim_idxlist),  &
      & start_idx_e(min_rledge:max_rledge,dim_idxlist),  &
      & end_idx_e  (min_rledge:max_rledge,dim_idxlist),  &
      & start_idx_v(min_rlvert:max_rlvert,dim_idxlist),  &
      & end_idx_v  (min_rlvert:max_rlvert,dim_idxlist),  &
      & stat=ist )

    IF (ist /= success) THEN
      CALL finish (TRIM(method_name), 'allocation for array_[cev]_indlist failed')
    ENDIF

!     write(0,*) "allocate_pre_patch..."
    !
    ! Allocate patch arrays which are read here
    !
    CALL allocate_pre_patch( patch_pre )

!     write(0,*) "get idx..."
    ! patch_pre%cells%start(:)
    ! patch_pre%cells%end(:)
    ! nesting does not work for hex grids
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_c', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlcell - min_rlcell + 1, 1/), &
      &                     patch_pre%cells%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_c', varid))
    CALL nf(nf_get_var_int(ncid_grf, varid, end_idx_c(:,:)))
    ! Needed for backward compatibility of old grids
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlcell - min_rlcell + 1, 1/), &
      &                     patch_pre%cells%end(:)))
    IF (dim_idxlist > 1) &
      patch_pre%cells%end(min_rlcell_int) = patch_pre%n_patch_cells_g

    ! patch_pre%edges%start(:)
    ! patch_pre%edges%end(:)
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_e', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rledge - min_rledge + 1, 1/), &
      &                     patch_pre%edges%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_e', varid))
    CALL nf(nf_get_var_int(ncid_grf, varid, end_idx_e(:,:)))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rledge - min_rledge + 1, 1/), &
      &                     patch_pre%edges%end(:)))
    ! Needed for backward compatibility of old grids
    IF (dim_idxlist > 1) &
      patch_pre%edges%end(min_rledge_int) = patch_pre%n_patch_edges_g

    ! patch_pre%verts%start(:)
    ! patch_pre%verts%end(:)
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_v', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlvert - min_rlvert + 1, 1/), &
      &                     patch_pre%verts%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_v', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlvert - min_rlvert + 1, 1/), &
      &                     patch_pre%verts%end(:)))
    ! Needed for backward compatibility of old grids
    IF (dim_idxlist > 1) &
      patch_pre%verts%end(min_rlvert_int) = patch_pre%n_patch_verts_g

!     write(0,*) "get phys_cell_id..."
    ! patch_pre%cells%phys_id(:)
    CALL nf(nf_inq_varid(ncid_grf, 'phys_cell_id', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_phys_id, local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                     local_ptr(:)))

    ! patch_pre%cells%neighbor
    CALL nf(nf_inq_varid(ncid, 'neighbor_cell_index', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_neighbor, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN
      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy cells arising in case of a pentagon
          IF (local_ptr_2d(ic,ji) == 0) THEN
            IF ( ji /= 6 ) THEN
              local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
              ! this should not happen
              ! CALL finish(method_name, "cells%neighbor_idx=0 not at the end")
            END IF
            ! Fill dummy neighbor with an existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF
        END DO  ! ji = 1, 6
      END DO ! cells
    END IF

!     write(0,*) "get edge_of_cell..."
    ! patch_pre%cells%edge
    CALL nf(nf_inq_varid(ncid, 'edge_of_cell', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_edge, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN

      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy edges arising in case of a pentagon
          IF ( local_ptr_2d(ic,ji) == 0 ) THEN
            IF ( ji /= 6 ) local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
            ! Fill dummy edge with existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF

        END DO  ! ji = 1, 6

      END DO ! cells
    ENDIF

    !----------------------------------------------------------------------------------
    ! compute cells%num_edges
    ! works for general unstructured grid
!     write(0,*) "compute cells%num_edges..."
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_num_edges, local_ptr)
    DO jc = patch_pre%cells%local_chunk(1,1)%first, &
      patch_pre%cells%local_chunk(1,1)%first + &
      patch_pre%cells%local_chunk(1,1)%size - 1
      local_ptr(jc) = COUNT(local_ptr_2d(jc, 1:max_cell_connectivity) > 0)
    END DO

    ! patch_pre%cells%vertex
!     write(0,*) "get vertex_of_cell..."
    CALL nf(nf_inq_varid(ncid, 'vertex_of_cell', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_vertex, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN

      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy verts arising in case of a pentagon
          IF ( local_ptr_2d(ic,ji) == 0 ) THEN
            IF ( ji /= 6 ) local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
            ! Fill dummy edge with existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF

        END DO  ! ji = 1, 6

      END DO ! cells
    ENDIF

    ! patch_pre%cells%center latitude
    CALL nf(nf_inq_varid(ncid, 'lat_cell_centre', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_center, local_ptr_wp_2d)
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                        (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 1)))

    ! patch_pre%cells%center longitude
    CALL nf(nf_inq_varid(ncid, 'lon_cell_centre', varid))
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                        (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 2)))

    ! patch_pre%verts%vertex(:)%lat
    CALL nf(nf_inq_varid(ncid, 'latitude_vertices', varid))
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_vertex, &
         local_ptr_wp_2d)
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                        (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 1)))

    ! patch_pre%verts%vertex(:)%lon
    CALL nf(nf_inq_varid(ncid, 'longitude_vertices', varid))
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                        (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 2)))

    !------------------------------------------
    ! nesting/lateral boundary indexes
    IF (max_cell_connectivity == 3) THEN ! triangular grid

!       write(0,*) "get parent_cell_index..."
      ! patch_pre%cells%parent
      CALL nf(nf_inq_varid(ncid_grf, 'parent_cell_index', varid))
      CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_parent, local_ptr)
      CALL nf(nf_get_vara_int(ncid_grf, varid, &
        &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
        &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
        &                     local_ptr))
      ! patch_pre%cells%child(:,:)
      CALL nf(nf_inq_varid(ncid_grf, 'child_cell_index', varid))
      CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_child, local_ptr_2d)
      CALL nf(nf_get_vara_int(ncid_grf, varid, &
        &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
        &                     (/patch_pre%cells%local_chunk(1,1)%size, 4/), &
        &                     local_ptr_2d))

    ELSE
      CALL message ('read_patch',&
        & 'nesting incompatible with non-triangular grid')
    ENDIF

!     write(0,*) "dist_mult_array_expose(patch_pre%cells%dist)..."
    CALL dist_mult_array_expose(patch_pre%cells%dist)

    ! patch_pre%cells%refin_ctrl
!     write(0,*) "refin_c_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_c_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! patch_pre%edges%parent
!     write(0,*) "parent_edge_index..."
    CALL nf(nf_inq_varid(ncid_grf, 'parent_edge_index', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_parent, local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! patch_pre%edges%child
!     write(0,*) "child_edge_index..."
    CALL nf(nf_inq_varid(ncid_grf, 'child_edge_index', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_child, local_ptr_2d)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size, 4/), &
      &                     local_ptr_2d))

    ! First ensure that child edge indices are all positive;
    ! if there are negative values, grid files are too old
    IF(ANY(local_ptr_2d(:,1:4)<0)) THEN
      CALL finish (TRIM(method_name), &
        & 'negative child edge indices detected - patch files are too old')
    ENDIF

    ! patch_pre%edges%refin_ctrl
!     write(0,*) "refin_e_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_e_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! patch_pre%verts%refin_ctrl
!     write(0,*) "refin_v_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_v_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                     (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! BEGIN NEW SUBDIV
    
!     write(0,*) max_verts_connectivity, "cells_of_vertex..."
    CALL nf(nf_inq_varid(ncid, 'cells_of_vertex', varid))
!     write(0,*) max_verts_connectivity, "  dist_mult_array_local_ptr..."
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_cell, local_ptr_2d)
!     write(0,*) max_verts_connectivity, "  nf_get_vara_int..."
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%verts%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%verts%local_chunk(1,1)%size, &
      &                       max_verts_connectivity/), &
      &                     local_ptr_2d))
    ! eliminate indices < 0, this should not happen but some older grid files
    ! seem to contain such indices
!     write(0,*) "SIZE(local_ptr_2d):", SIZE(local_ptr_2d,1), SIZE(local_ptr_2d,2)
!     write(0,*) max_verts_connectivity, "  WHERE..."
    WHERE(local_ptr_2d(:, :) < 0) local_ptr_2d(:, :) = 0
    ! account for dummy cells arising in case of a pentagon
    ! Fill dummy cell with existing index to simplify do loops
    ! Note, however, that related multiplication factors must be zero
!     write(0,*) "move_dummies_to_end verts%local_chunk..."
    CALL move_dummies_to_end(local_ptr_2d, &
      patch_pre%verts%local_chunk(1,1)%size, max_verts_connectivity, &
      use_duplicated_connectivity)

    !
    ! Set verts%num_edges
!     write(0,*) "Set verts%num_edges..."
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_num_edges, local_ptr)
    DO ji = patch_pre%verts%local_chunk(1,1)%first, &
      patch_pre%verts%local_chunk(1,1)%first + &
      patch_pre%verts%local_chunk(1,1)%size - 1
      local_ptr(ji) = COUNT(local_ptr_2d(ji, 1:max_verts_connectivity) > 0)
    END DO

    ! patch_pre%edges%cell(:,:)
    CALL nf(nf_inq_varid(ncid, 'adjacent_cell_of_edge', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_cell, local_ptr_2d)
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size, 2/), &
      &                     local_ptr_2d))
    WHERE(local_ptr_2d(:, :) < 0) local_ptr_2d(:, :) = 0

    CALL dist_mult_array_expose(patch_pre%verts%dist)
    CALL dist_mult_array_expose(patch_pre%edges%dist)

    ! END NEW SUBDIV


    CALL nf(nf_close(ncid))
    IF (lsep_grfinfo) CALL nf(nf_close(ncid_grf))

    !
    ! deallocate temporary arrays to read in data form the grid/patch generator
    !
    ! index lists arrays
    DEALLOCATE( start_idx_c, end_idx_c, start_idx_e, end_idx_e, start_idx_v, end_idx_v, &
      & stat=ist )
    IF (ist /= success) THEN
      CALL finish (TRIM(method_name), 'deallocation for array_[cev]_indlist failed')
    ENDIF

    CALL message (TRIM(method_name), 'read_patches finished')

  END SUBROUTINE read_pre_patch
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !> Reads the remaining patch information into the divided patch
  SUBROUTINE read_remaining_patch( ig, patch, n_lp, id_lp, lsep_grfinfo )

    INTEGER,       INTENT(in)    ::  ig       ! domain ID
    TYPE(t_patch), INTENT(inout), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,       INTENT(IN)    :: lsep_grfinfo

    INTEGER :: ncid, varid, ncid_grf
    TYPE(t_stream_id) :: stream_id, stream_id_grf
    INTEGER :: ip, jv, idx, blk
    INTEGER :: max_cell_connectivity, max_verts_connectivity

    INTEGER :: return_status

    TYPE(t_patch), POINTER :: p_p, patch0
    TYPE(p_t_patch) :: patches(0:n_lp)
    TYPE(var_data_2d_int)  :: multivar_2d_data_int(n_lp+1)
    TYPE(var_data_2d_wp)  :: multivar_2d_data_wp(n_lp+1)
    TYPE(var_data_3d_int) :: multivar_3d_data_int(n_lp+1)
    TYPE(var_data_3d_wp) :: multivar_3d_data_wp(n_lp+1)


!    REAL(wp), POINTER :: tmp_check_array(:,:)
!    REAL(wp) :: max_diff

    CHARACTER(LEN=*), PARAMETER :: method_name = &
      'mo_model_domimp_patches/read_remaining_patch'
    !-----------------------------------------------------------------------

    CALL message ('mo_model_domimp_patches:read_remaining_patch', &
      & 'Read gridmap file '//TRIM(patch%grid_filename))

    DO ip = 0, n_lp
      patches(ip)%p => get_patch_ptr(patch, id_lp, ip)
    ENDDO

    CALL nf(nf_open(TRIM(patch%grid_filename), nf_nowrite, ncid))
    stream_id = openInputFile(TRIM(patch%grid_filename), patches)

    IF (lsep_grfinfo) THEN
      CALL nf(nf_open(TRIM(patch%grid_filename_grfinfo), nf_nowrite, ncid_grf))
      stream_id_grf = openInputFile(TRIM(patch%grid_filename_grfinfo), patches)
    ELSE
      ncid_grf = ncid
      stream_id_grf = stream_id
    ENDIF

    max_cell_connectivity = patch%cells%max_connectivity
    max_verts_connectivity = patch%verts%max_connectivity

    patch%boundary_depth_index = 0
#ifndef __NO_ICON_ATMO__
    patch%boundary_depth_index = nudge_zone_width
    return_status = nf_inq_attid(ncid, nf_global, 'boundary_depth_index', varid)
    IF (return_status == nf_noerr) THEN
       CALL nf(nf_get_att_int(ncid, nf_global, 'boundary_depth_index', patch%boundary_depth_index))
       IF (nudge_zone_width < 0) THEN
         nudge_zone_width = patch%boundary_depth_index - 4
       ENDIF
!       IF ( nudge_zone_width > patch%boundary_depth_index - 4) THEN
!         CALL finish ('mo_model_domain_import:read_patch',  &
!           & 'nudge_zone_width > patch%boundary_depth_index - 4')
!       ENDIF
    ENDIF
#endif

    IF (max_cell_connectivity /= 3) & ! not triangular grid
      CALL message ('read_remaining_patch',&
        & 'nesting incompatible with non-triangular grid')

    ! TODO [FP] : We need an implementation of this shifting without
    !             using the NetCDF attribute "grid_ID"

    !! ! Preparation: In limited-area mode, check if child domain ID's read
    !! ! from the grid files need to be shifted
    !! IF (ig == 1 .AND. l_limited_area .AND. patch%n_childdom>0) THEN
    !!   CALL nf(nf_get_att_int(ncid_grf, nf_global, 'grid_ID', igrid_id))
    !!   ishift_child_id = igrid_id - 1
    !!   WRITE(message_text,'(a,i4)') 'Limited-area mode: child cell IDs are shifted by ',ishift_child_id
    !!   CALL message ('', TRIM(message_text))
    !! ENDIF

    !------------------------------------------
    ! nesting/lateral boundary indexes

    ! TODO [FP] : We need an implementation of this shifting without
    !             using the NetCDF field

    ! patch%cells%child_id(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_int(ip+1)%data => p_p%cells%child_id(:,:)
    END DO
    CALL read_2D_int(stream_id_grf, on_cells, 'child_cell_id', n_lp+1, &
      &              multivar_2d_data_int(:))
    IF(ishift_child_id /= 0 .AND. patch%n_childdom>0) THEN
      DO ip = 0, n_lp
        WHERE (multivar_2d_data_int(ip+1)%data(:,:) > 0) &
          multivar_2d_data_int(ip+1)%data(:,:) = &
          multivar_2d_data_int(ip+1)%data(:,:) - ishift_child_id
      END DO
    ENDIF

    ! p_p%cells%phys_id(:,:)
    IF (ig <= 1) THEN
      DO ip = 0, n_lp
        p_p => get_patch_ptr(patch, id_lp, ip)
        p_p%cells%phys_id(:,:) = ig
      ENDDO
    ELSE
      DO ip = 0, n_lp
        p_p => get_patch_ptr(patch, id_lp, ip)
        multivar_2d_data_int(ip+1)%data => p_p%cells%phys_id(:,:)
      END DO
      CALL read_2D_int(stream_id_grf, on_cells, 'phys_cell_id', n_lp+1, &
        &              multivar_2d_data_int(:))
      IF(ishift_child_id > 0) THEN
        DO ip = 0, n_lp
          multivar_2d_data_int(ip+1)%data(:,:) = &
            multivar_2d_data_int(ip+1)%data(:,:) - ishift_child_id
        END DO
      ENDIF
    END IF

    ! p_p%cells%edge_orientation(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_wp(ip+1)%data => &
        p_p%cells%edge_orientation(:,:,1:max_cell_connectivity)
    END DO
    CALL read_2D_extdim(stream_id, on_cells, 'orientation_of_normal', &
      &                 n_lp+1, fill_array=multivar_3d_data_wp(:), &
      &                 start_extdim=1, end_extdim=max_cell_connectivity)

    ! p_p%cells%area(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%cells%area(:,:)
    END DO
    CALL read_2D(stream_id, on_cells, 'cell_area_p', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! TODO [FP] : We need an implementation of this shifting without
    !             using the NetCDF field

    ! p_p%edges%child_id(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_int(ip+1)%data => p_p%edges%child_id(:,:)
    END DO
    CALL read_2D_int(stream_id_grf, on_edges, 'child_edge_id', n_lp+1, &
      &              multivar_2d_data_int(:))
    IF(ishift_child_id /= 0 .AND. patch%n_childdom>0) THEN
      DO ip = 0, n_lp
        multivar_2d_data_int(ip+1)%data(:,:) = &
          multivar_2d_data_int(ip+1)%data(:,:) - ishift_child_id
      END DO
    ENDIF

    ! p_p%edges%phys_id(:,:)
    IF (ig <= 1) THEN
      DO ip = 0, n_lp
        p_p => get_patch_ptr(patch, id_lp, ip)
        p_p%edges%phys_id(:,:) = ig
      ENDDO
    ELSE
      DO ip = 0, n_lp
        p_p => get_patch_ptr(patch, id_lp, ip)
        multivar_2d_data_int(ip+1)%data => p_p%edges%phys_id(:,:)
      END DO
      CALL read_2D_int(stream_id_grf, on_edges, 'phys_edge_id', n_lp+1, &
        &              multivar_2d_data_int(:))
      IF(ishift_child_id > 0) THEN
        DO ip = 0, n_lp
          multivar_2d_data_int(ip+1)%data(:,:) = &
            multivar_2d_data_int(ip+1)%data(:,:) - ishift_child_id
        END DO
      ENDIF
    END IF

    ! p_p%edges%cell_idx(:,:,:)
    ! p_p%edges%cell_blk(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data => p_p%edges%cell_idx(:,:,1:2)
    END DO
    CALL read_2D_extdim_int(stream_id, on_edges, 'adjacent_cell_of_edge', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=2)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data(:,:,1:2) = &
        (get_valid_local_index(p_p%cells%decomp_info%glb2loc_index, &
         &                     multivar_3d_data_int(ip+1)%data(:,:,1:2), &
         &                     .TRUE.))
      p_p%edges%cell_blk(:,:,1:2) = &
        blk_no(multivar_3d_data_int(ip+1)%data(:,:,1:2))
      p_p%edges%cell_idx(:,:,1:2) = &
        idx_no(multivar_3d_data_int(ip+1)%data(:,:,1:2))
    END DO

    ! p_p%edges%vertex_idx(:,:,:)
    ! p_p%edges%vertex_blk(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data => p_p%edges%vertex_idx(:,:,1:2)
    END DO
    CALL read_2D_extdim_int(stream_id, on_edges, 'edge_vertices', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=2)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data(:,:,1:2) = &
        (get_valid_local_index(p_p%verts%decomp_info%glb2loc_index, &
         &                     multivar_3d_data_int(ip+1)%data(:,:,1:2), &
         &                     .TRUE.))
      p_p%edges%vertex_blk(:,:,1:2) = &
        blk_no(multivar_3d_data_int(ip+1)%data(:,:,1:2))
      p_p%edges%vertex_idx(:,:,1:2) = &
        idx_no(multivar_3d_data_int(ip+1)%data(:,:,1:2))
    END DO

    ! p_p%edges%tangent_orientation(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%tangent_orientation(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'edge_system_orientation', n_lp+1, &
      &          multivar_2d_data_wp(:))

#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      ALLOCATE(multivar_2d_data_wp(ip+1)%data(nproma, p_p%nblks_e))
      multivar_2d_data_wp(ip+1)%data(:,:) = 0.0_wp
    END DO
#endif

    ! p_p%edges%center(:,:)%lon
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%center(:,:)%lon
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'lon_edge_centre', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%center(:,:)%lon = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%center(:,:)%lat
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%center(:,:)%lat
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'lat_edge_centre', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%center(:,:)%lat = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_normal(:,:)%v1
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_normal(:,:)%v1
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'zonal_normal_primal_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%primal_normal(:,:)%v1 = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_normal(:,:)%v2
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_normal(:,:)%v2
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'meridional_normal_primal_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%primal_normal(:,:)%v2 = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_normal(:,:)%v1
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_normal(:,:)%v1
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'zonal_normal_dual_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%dual_normal(:,:)%v1 = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_normal(:,:)%v2
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_normal(:,:)%v2
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'meridional_normal_dual_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%dual_normal(:,:)%v2 = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip+1)%data)
    END DO
#endif

    ! p_p%edges%primal_edge_length(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_edge_length(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'edge_length', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! p_p%edges%dual_edge_length(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_edge_length(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'dual_edge_length', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! p_p%edges%edge_vert_length(:,:,1:2)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_wp(ip+1)%data => p_p%edges%edge_vert_length(:,:,1:2)
    END DO
    CALL read_2D_extdim(stream_id, on_edges, 'edge_vert_distance', n_lp+1, &
      &                 fill_array=multivar_3d_data_wp(:), start_extdim=1, &
      &                 end_extdim=2)

    ! p_p%edges%edge_cell_length(:,:,1:2)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_wp(ip+1)%data => p_p%edges%edge_cell_length(:,:,1:2)
    END DO
    CALL read_2D_extdim(stream_id, on_edges, 'edge_cell_distance', n_lp+1, &
      &                 fill_array=multivar_3d_data_wp(:), start_extdim=1, &
      &                 end_extdim=2)

    ! p_p%verts%neighbor_idx(:,:,:)
    ! p_p%verts%neighbor_blk(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data => &
        p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'vertices_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      CALL convert_to_local_index( &
        p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        p_p%verts%decomp_info%glb2loc_index, use_duplicated_connectivity)
      CALL move_dummies_to_end_idxblk( &
        p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        use_duplicated_connectivity)
      p_p%verts%neighbor_blk(:,:,1:max_verts_connectivity) = &
        blk_no(p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity))
      p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity) = &
        idx_no(p_p%verts%neighbor_idx(:,:,1:max_verts_connectivity))
    END DO

    ! p_p%verts%cell_idx(:,:,:)
    ! p_p%verts%cell_blk(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data => &
        p_p%verts%cell_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'cells_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      CALL convert_to_local_index( &
        p_p%verts%cell_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        p_p%cells%decomp_info%glb2loc_index, use_duplicated_connectivity)
      ! account for dummy cells arising in case of a pentagon
      ! Fill dummy cell with existing index to simplify do loops
      ! Note, however, that related multiplication factors must be zero
      CALL move_dummies_to_end_idxblk( &
        p_p%verts%cell_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        use_duplicated_connectivity)
      p_p%verts%cell_blk(:,:,1:max_verts_connectivity) = &
        blk_no(p_p%verts%cell_idx(:,:,1:max_verts_connectivity))
      p_p%verts%cell_idx(:,:,1:max_verts_connectivity) = &
        idx_no(p_p%verts%cell_idx(:,:,1:max_verts_connectivity))
    END DO

    ! p_p%verts%edge_idx(:,:,:)
    ! p_p%verts%edge_blk(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_3d_data_int(ip+1)%data => &
        p_p%verts%edge_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'edges_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      CALL convert_to_local_index( &
        p_p%verts%edge_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        p_p%edges%decomp_info%glb2loc_index, use_duplicated_connectivity)
      ! account for dummy cells arising in case of a pentagon
      ! Fill dummy cell with existing index to simplify do loops
      ! Note, however, that related multiplication factors must be zero
      CALL move_dummies_to_end_idxblk( &
        p_p%verts%edge_idx(:,:,1:max_verts_connectivity), &
        p_p%n_patch_verts, max_verts_connectivity, &
        use_duplicated_connectivity)
      p_p%verts%edge_blk(:,:,1:max_verts_connectivity) = &
        blk_no(p_p%verts%edge_idx(:,:,1:max_verts_connectivity))
      p_p%verts%edge_idx(:,:,1:max_verts_connectivity) = &
        idx_no(p_p%verts%edge_idx(:,:,1:max_verts_connectivity))
    END DO

    ! p_p%verts%num_edges
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      IF (use_duplicated_connectivity) THEN
        DO jv = 1, p_p%n_patch_verts
          idx = idx_no(jv)
          blk = blk_no(jv)
          p_p%verts%num_edges(idx,blk) = &
            COUNT((p_p%verts%edge_idx(idx,blk,1:max_verts_connectivity) &
              &    /= p_p%verts%edge_idx(idx,blk,max_verts_connectivity)) .OR.&
              &   (p_p%verts%edge_blk(idx,blk,1:max_verts_connectivity) &
              &    /= p_p%verts%edge_blk(idx,blk,max_verts_connectivity))) + 1
        END DO
      ELSE
        DO jv = 1, p_p%n_patch_verts
          idx = idx_no(jv)
          blk = blk_no(jv)
          p_p%verts%num_edges(idx,blk) = &
            COUNT(p_p%verts%edge_idx(idx,blk,1:max_verts_connectivity) /= 0)
        END DO
      END IF
    END DO

    ! p_p%verts%edge_orientation(:,:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      ALLOCATE( &
        multivar_3d_data_int(ip+1)%data(SIZE(p_p%verts%edge_orientation, 1), &
        &                               SIZE(p_p%verts%edge_orientation, 2), &
        &                               max_verts_connectivity))
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'edge_orientation', n_lp+1, &
      &                     fill_array=multivar_3d_data_int(:), start_extdim=1,&
      &                     end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      ! move dummy edges to end and set edge orientation to zero
      CALL move_dummies_to_end_idxblk( &
        multivar_3d_data_int(ip+1)%data(:,:,:), p_p%n_patch_verts, &
        max_verts_connectivity, .FALSE.)
      p_p%verts%edge_orientation(:,:,1:max_verts_connectivity) = &
        REAL(multivar_3d_data_int(ip+1)%data(:,:,:), wp)
      DEALLOCATE(multivar_3d_data_int(ip+1)%data)
    END DO

    ! p_p%verts%dual_area(:,:)
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%verts%dual_area(:,:)
    END DO
    CALL read_2D(stream_id, on_vertices, 'dual_area_p', n_lp+1, &
      &          fill_array=multivar_2d_data_wp(:))

    !-------------------------------------------------
    ! read geometry parameters
    patch0 => get_patch_ptr(patch, id_lp, 0)
    return_status = parallel_read_geometry_info(ncid, patch0%geometry_info)
    IF (return_status /= 0 ) THEN
      ! the information was missing from the file (ie old grids)
      ! calclulate basic settings
!       CALL finish("","did not read from file")
      CALL set_missing_geometry_info(patch0)
    ENDIF
    CALL set_grid_geometry_derived_info(patch0%geometry_info)

    DO ip = 1, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      CALL copy_grid_geometry_info(from_geometry_info = patch0%geometry_info, &
        &                            to_geometry_info = p_p%geometry_info)
!       write(0,*) "-------------------------------------------------------"
!       write(0,*) "area, char_length=", p_p%geometry_info%mean_cell_area, &
!         & p_p%geometry_info%mean_characteristic_length
!       write(0,*) "-------------------------------------------------------"
    ENDDO
    !---------------------------------------------------
    ! cartesian positions
    IF (gridfile_has_cartesian_info(ncid)) THEN
      CALL read_cartesian_positions(stream_id, patch, n_lp, id_lp)
    ELSE
      CALL calculate_cartesian_positions(patch, n_lp, id_lp)
    END IF
    !-------------------------------------------------


    CALL nf(nf_close(ncid))
    CALL closeFile(stream_id)
    IF (lsep_grfinfo) THEN
      CALL nf(nf_close(ncid_grf))
      CALL closeFile(stream_id_grf)
    END IF
    !-------------------------------------------------
     !Check for plane_torus case
    IF(p_p%geometry_info%geometry_type == planar_torus_geometry .AND. .NOT. is_plane_torus) THEN
      CALL message(TRIM(method_name), &
        & "Grid is plane torus: turning on is_plane_torus automatically")
      is_plane_torus = .TRUE.
    END IF

    IF(p_p%geometry_info%geometry_type /= planar_torus_geometry .AND. is_plane_torus) &
      CALL finish(TRIM(method_name),"Input grid is NOT plane torus, Stopping")
    !-------------------------------------------------

    CALL message ('mo_model_domimp_patches:read_remaining_patch', 'read finished')

  END SUBROUTINE read_remaining_patch
  !-------------------------------------------------------------------------

  SUBROUTINE convert_to_local_index(array, array_size, max_connectivity, &
    &                               glb2loc_index, duplicate)

    INTEGER, INTENT(in) :: array_size, max_connectivity
    INTEGER, INTENT(inout) :: array(:,:,:)
    TYPE(t_glb2loc_index_lookup), INTENT(in) :: glb2loc_index
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: idx, blk, i

    IF (duplicate) THEN
      DO i = 1, array_size
          idx = idx_no(i)
          blk = blk_no(i)
          array(idx,blk,1:max_connectivity) = &
            get_valid_local_index(glb2loc_index, &
            &                     array(idx,blk,1:max_connectivity), .TRUE.)
      END DO
    ELSE
      DO i = 1, array_size
          idx = idx_no(i)
          blk = blk_no(i)
          array(idx,blk,1:max_connectivity) = &
            MAX(get_local_index(glb2loc_index, &
              &                 array(idx,blk,1:max_connectivity)), 0)
      END DO
    END IF
  END SUBROUTINE convert_to_local_index
  !-------------------------------------------------------------------------
  ! Checks for the pentagon case and moves dummy cells to end.
  ! The dummy entry is either set to 0 or duplicated from the last one
  SUBROUTINE move_dummies_to_end(array, array_size, max_connectivity, duplicate)

    INTEGER, INTENT(in) :: array_size, max_connectivity
    INTEGER, INTENT(inout) :: array(1:array_size,1:max_connectivity)
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: i, zeros(1:max_connectivity), num_non_zero
    CHARACTER(LEN=*), PARAMETER :: method_name = &
      'mo_model_domimp_patches:move_dummies_to_end'

    IF (duplicate) THEN
      DO i = 1, array_size
        num_non_zero = COUNT(array(i,1:max_connectivity) /= 0)
        IF (num_non_zero /= max_connectivity) THEN
          IF (num_non_zero == 0) THEN
            ! CALL warning(method_name, "no connectivity found")
            CYCLE
          END IF
          array(i,1:num_non_zero) = PACK(array(i,1:max_connectivity), &
            &                            array(i,1:max_connectivity) /= 0)
          array(i,num_non_zero+1:max_connectivity) = array(i,num_non_zero)
        END IF
      END DO
    ELSE
      zeros(:) = 0
      DO i = 1,  array_size
        IF (ANY(array(i,1:max_connectivity) /= 0)) THEN

          array(i,1:max_connectivity) = PACK(array(i,1:max_connectivity), &
            &                                array(i,1:max_connectivity) /= 0, &
            &                                zeros(:))
        END IF
      END DO
    END IF

  END SUBROUTINE move_dummies_to_end
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  ! Checks for the pentagon case and moves dummy cells to end.
  ! The dummy entry is either set to 0 or duplicated from the last one
  SUBROUTINE move_dummies_to_end_idxblk(array, array_size, max_connectivity, &
    &                                   duplicate)

    INTEGER, INTENT(inout) :: array(:,:,:)
    INTEGER, INTENT(in) :: array_size, max_connectivity
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: i, idx, blk, zeros(1:max_connectivity), num_non_zero
    CHARACTER(LEN=*), PARAMETER :: method_name = &
      'mo_model_domimp_patches:move_dummies_to_end_idxblk'

    IF (duplicate) THEN
      DO i = 1, array_size
        idx = idx_no(i)
        blk = blk_no(i)
        num_non_zero = COUNT(array(idx,blk,1:max_connectivity) /= 0)
        IF (num_non_zero /= max_connectivity) THEN
          IF (num_non_zero == 0) THEN
            ! CALL warning(method_name, "no connectivity found")
            CYCLE
          END IF
          array(idx,blk,1:num_non_zero) = &
            PACK(array(idx,blk,1:max_connectivity), &
            &    array(idx,blk,1:max_connectivity) /= 0)
          array(idx,blk,num_non_zero+1:max_connectivity) = &
            array(idx,blk,num_non_zero)
        END IF
      END DO
    ELSE
      zeros(:) = 0
      DO i = 1,  array_size
        idx = idx_no(i)
        blk = blk_no(i)
        IF (ANY(array(idx,blk,1:max_connectivity) /= 0)) THEN
          array(idx,blk,1:max_connectivity) = &
            PACK(array(idx,blk,1:max_connectivity), &
            &    array(idx,blk,1:max_connectivity) /= 0, zeros(:))
        END IF
      END DO
    END IF

  END SUBROUTINE move_dummies_to_end_idxblk
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  LOGICAL FUNCTION gridfile_has_cartesian_info(ncid)
    INTEGER, INTENT(in)    :: ncid

    INTEGER :: varid
    REAL(wp) :: x(3)

    gridfile_has_cartesian_info = &
      nf_inq_varid(ncid, 'cell_circumcenter_cartesian_x', varid) == nf_noerr

!     IF (gridfile_has_cartesian_info) THEN
!
!       CALL nf(nf_inq_varid(ncid, 'edge_primal_normal_cartesian_x', varid))
!       CALL nf(nf_get_vara_double(ncid, varid, (/1/), (/1/), x(1)))
!       CALL nf(nf_inq_varid(ncid, 'edge_primal_normal_cartesian_y', varid))
!       CALL nf(nf_get_vara_double(ncid, varid, (/1/), (/1/), x(2)))
!       CALL nf(nf_inq_varid(ncid, 'edge_primal_normal_cartesian_z', varid))
!       CALL nf(nf_get_vara_double(ncid, varid, (/1/), (/1/), x(3)))
!
!       gridfile_has_cartesian_info = ANY(ABS(x(:)) >= 0.001_wp)
!
!     END IF

  END FUNCTION gridfile_has_cartesian_info
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE read_cartesian_positions(stream_id, patch, n_lp, id_lp)

    TYPE(t_stream_id) :: stream_id
    TYPE(t_patch), INTENT(inout), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level

    TYPE(var_data_2d_wp)  :: multivar_2d_data_wp(n_lp+1)
    INTEGER :: ip
    TYPE(t_patch), POINTER :: p_p

    CHARACTER(LEN=*), PARAMETER :: method_name = &
      'mo_model_domimp_patches:read_cartesian_positions'
    !-----------------------------------------------------------------------

#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      ALLOCATE(multivar_2d_data_wp(ip+1)%data(nproma, p_p%alloc_cell_blocks))
      multivar_2d_data_wp(ip+1)%data(:,:) = 0.0_wp
    END DO
#endif

    ! p_p%cells%cartesian_center(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%cells%cartesian_center(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%cells%cartesian_center(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%cells%cartesian_center(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%cells%cartesian_center(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%cells%cartesian_center(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%cells%cartesian_center(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%cells%cartesian_center(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%cells%cartesian_center(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip+1)%data)
      p_p => get_patch_ptr(patch, id_lp, ip)
      ALLOCATE(multivar_2d_data_wp(ip+1)%data(nproma,p_p%nblks_e))
      multivar_2d_data_wp(ip+1)%data(:,:) = 0.0_wp
    END DO
#endif

    ! p_p%edges%cartesian_center(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_center(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_center(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%cartesian_center(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_center(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_center(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%cartesian_center(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_center(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_center(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%cartesian_dual_middle(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_dual_middle(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_dual_middle(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%cartesian_dual_middle(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_dual_middle(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_dual_middle(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%cartesian_dual_middle(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%cartesian_dual_middle(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%cartesian_dual_middle(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_cart_normal(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_cart_normal(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%primal_cart_normal(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_cart_normal(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_cart_normal(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%primal_cart_normal(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_cart_normal(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%primal_cart_normal(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%primal_cart_normal(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_cart_normal(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_cart_normal(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%dual_cart_normal(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_cart_normal(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_cart_normal(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%dual_cart_normal(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_cart_normal(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%edges%dual_cart_normal(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%edges%dual_cart_normal(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip+1)%data)
      p_p => get_patch_ptr(patch, id_lp, ip)
      ALLOCATE(multivar_2d_data_wp(ip+1)%data(nproma,p_p%nblks_v))
      multivar_2d_data_wp(ip+1)%data(:,:) = 0.0_wp
    END DO
#endif

    ! p_p%verts%cartesian(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%verts%cartesian(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_x_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%verts%cartesian(:,:)%x(1) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%verts%cartesian(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%verts%cartesian(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_y_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%verts%cartesian(:,:)%x(2) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

    ! p_p%verts%cartesian(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      multivar_2d_data_wp(ip+1)%data => p_p%verts%cartesian(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_z_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)
      p_p%verts%cartesian(:,:)%x(3) = multivar_2d_data_wp(ip+1)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip+1)%data)
    END DO
#endif

  END SUBROUTINE read_cartesian_positions
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE calculate_cartesian_positions(patch, n_lp, id_lp)

    TYPE(t_patch), INTENT(inout), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level

    TYPE(t_patch), POINTER :: p_p

    INTEGER :: ip
    CHARACTER(LEN=*), PARAMETER :: method_name = 'mo_model_domimp_patches:calculate_cartesian_positions'
    !-----------------------------------------------------------------------
    ! GZ: This routine is always called with grids from the global grid generator, so this warning
    ! is more confusing than useful
    IF (msg_level >= 15 .OR. patch%geometry_info%geometry_type /= sphere_geometry) THEN
      CALL message(method_name, " is called")
    ENDIF
    IF (patch%geometry_info%geometry_type /= sphere_geometry) &
      CALL finish(method_name, "geometry_type /= sphere_geometry")

    DO ip = 0, n_lp
      p_p => get_patch_ptr(patch, id_lp, ip)

      ! calculate Cartesian components of primal normal
      ! (these are old grids)
      CALL calculate_patch_cartesian_positions( p_p )

    ENDDO

  END SUBROUTINE calculate_cartesian_positions
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  ! get_patch_ptr returns a pointer to patch for idx=0,
  ! a pointer to a local parent patch from the list otherwise
  FUNCTION get_patch_ptr(patch, id_lp, idx) result(patch_ptr)
    TYPE(t_patch), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in) ::  id_lp(:) ! IDs of local parents on the same level
    TYPE(t_patch), POINTER :: patch_ptr
    INTEGER, INTENT(in)    :: idx

    IF (idx == 0) THEN
      patch_ptr => patch
    ELSE
      patch_ptr => p_patch_local_parent(id_lp(idx))
    ENDIF
  END FUNCTION get_patch_ptr
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE nf(STATUS, warnonly, silent)

    INTEGER, INTENT(in)           :: STATUS
    LOGICAL, INTENT(in), OPTIONAL :: warnonly
    LOGICAL, INTENT(in), OPTIONAL :: silent

    LOGICAL :: lwarnonly, lsilent

    lwarnonly = .FALSE.
    lsilent   = .FALSE.
    IF(PRESENT(warnonly)) lwarnonly = .TRUE.
    IF(PRESENT(silent))   lsilent   = silent

    IF (lsilent) RETURN
    IF (STATUS /= nf_noerr) THEN
      IF (lwarnonly) THEN
        CALL message('mo_model_domain_import netCDF error', nf_strerror(STATUS), &
          & level=em_warn)
      ELSE
        CALL finish('mo_model_domain_import netCDF error', nf_strerror(STATUS))
      ENDIF
    ENDIF

  END SUBROUTINE nf

END MODULE mo_model_domimp_patches


