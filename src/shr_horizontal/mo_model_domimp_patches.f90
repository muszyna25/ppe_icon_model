!>
!!               The module <i>mo_model_import_domain</i>.
!! provides functionality to import information about the models computational
!! domain. This information is read from several files that were generated by
!! the patch generator programm. The data types describing the model domain are
!! contained in <i>mo_domain_model</i>.
!!
!! @par Revision History
!! Initial version  by: Peter Korn,  MPI-M, Hamburg, June 2005
!! Modification by Thomas Heinze (2006-02-21):
!! - renamed m_modules to mo_modules
!! Modification by Thomas Heinze (2006-09-20):
!! - added method_name grid_and_patch_diagnosis
!! Modification by Pilar Ripodas, DWD, (2007-01-31)
!! - addapted to the new TYPE grid_edges (tangent_orientation added)
!! Modification by Peter Korn,  MPI-M, (2006-12)
!! - implementation of topography and boundary treatment, i.e.
!!   initialization of the grid & patch components that carry
!!   information about topography and the lateral boundaries of
!!   the domain; this is not related to patch boundaries.
!!   topography can either be computed by analytical l,eans or
!!   by reading from database files.
!! Modification by Hui Wan, MPI-M, (2007-02-23)
!! - Subroutine <i>init_import</i> was changed to <i>setup_grid</i>.
!!   Namelist hierarchy_ini was renamed to <i>grid_ctl</i>, moved
!!   from <i>mo_io_utilities</i> to this module and now read from
!!   an external file in subroutine <i>setup_grid</i>.
!! - Some changes in <i>init_ocean_patch_component</i> after
!!   discussion with Peter.
!! - Calculation of the min. primal edge length was added to
!!   <i>import_patches</i>. However, shouldn't it be an array with
!!   one element for each patch, rather than a scalar?
!! Modification by P. Ripodas, DWD, (2007-03-14):
!! - Now the output of "import_patches" is the min_dual_edge_lenght
!!   instead of the min_primal_edge_lenght. It will be used to set
!!   the horizontal diffusion parameter. Now it is done as it was
!!   in the prototype.
!! Modification by Almut Gassmann, MPI-M (2007-04)
!! - removed loptimize to make compatible with new grid generator
!! - removed itoa for good programming style
!! - reorganized patch input to be compatible with the new patch generator
!! - cleaning up "destruct_patches"
!! Modification by Almut Gassmann, MPI-M (2007-04-13)
!! - remove grid type and perform related adaptations
!!   (grid information comes now inside a patch)
!! - changed subroutine name form setup_grid to setup_files
!! Modified by Hui Wan, MPI-M, (2008-04-04)
!!  - topography_file_dir renamed topo_file_dir
!!  - for the hydro_atmos, control variable testtype renamed ctest_name.
!! Modified by Almut Gassmann, MPI-M, (2008-04-23)
!!  - itopo distinguishes now shallow water (itopo=1) orography function
!!    from hydro_atmos orography function (itopo=2)
!! Modification by Jochen Foerstner, DWD, (2008-07-16)
!!  - new fields in the derived type for the edges:
!!    grid_edges%primal_cart_normal (Cartesian normal to edge),
!!    grid_edges%quad_idx, grid_edges%quad_area and grid_edges%quad_orientation
!!    (indices of edges and area of the quadrilateral formed by two adjacent cells)
!!    up to now these new fields are initialized in the new routines
!!    calculate_primal_cart_normal and init_quad_twoadjcells
!!    rather than read from a grid/patch file.
!! Modification by Almut Gassmann, MPI-M, (2008-09-21)
!!  - remove reference to mask and height files, they are never used
!!  - use global_cell_type to distinguish cells as triangles or hexagons
!! Modification by Almut Gassmann, MPI-M (2008-10-30)
!!  - add subroutine init_coriolis to initialize Coriolis parameter
!! Modification by Stephan Lorenz, MPI-M (2010-02-18)
!!  - add subroutine init_ocean_patch to initialize ocean patch extensions
!! Modification by Daniel Reinert, DWD (2010-07-21)
!! - removed call of init_topography. This will be handeled within init_ext_data
!!
!! @par Copyright and License
!!
!! This code is subject to the DWD and MPI-M-Software-License-Agreement in
!! its most recent form.
!! Please see the file LICENSE in the root of the source tree for this code.
!! Where software is supplied by third parties, it is indicated in the
!! headers of the routines.
!!
!!

!----------------------------
#include "omp_definitions.inc"
!----------------------------

MODULE mo_model_domimp_patches
  !-------------------------------------------------------------------------

  USE mo_kind,               ONLY: wp
  !USE mo_io_units,           ONLY: filename_max
  USE mo_impl_constants,     ONLY: success,                &
    &                              min_rlcell, max_rlcell, &
    &                              min_rledge, max_rledge, &
    &                              min_rlvert, max_rlvert, &
    &                              max_dom,                &
    &                              min_rlcell_int,         &
    &                              min_rledge_int,         &
    &                              min_rlvert_int
  USE mo_exception,          ONLY: message_text, message, warning, finish, em_warn
  USE mo_util_string,        ONLY: int2string
  USE mo_model_domain,       ONLY: t_patch, t_pre_patch, p_patch_local_parent, &
       c_num_edges, c_parent, c_phys_id, c_neighbor, c_edge, &
       c_vertex, c_center, c_refin_ctrl, e_parent, e_cell, &
       e_refin_ctrl, v_cell, v_num_edges, v_vertex, v_refin_ctrl
  USE mo_decomposition_tools,ONLY: t_glb2loc_index_lookup, &
    &                              get_valid_local_index, &
    &                              t_grid_domain_decomp_info, get_local_index
  USE mo_parallel_config,    ONLY: nproma, p_test_run
  USE mo_model_domimp_setup, ONLY: init_quad_twoadjcells, init_coriolis, &
    & set_verts_phys_id, init_butterfly_idx, fill_grid_subsets
  USE mo_grid_tools,         ONLY: calculate_patch_cartesian_positions, rescale_grid
  USE mo_grid_config,        ONLY: start_lev, nroot, n_dom, n_dom_start, &
    & max_childdom, dynamics_parent_grid_id, &
    & lplane, grid_length_rescale_factor, is_plane_torus, grid_sphere_radius, &
    & use_duplicated_connectivity, set_patches_grid_filename
  USE mo_dynamics_config,    ONLY: lcoriolis
  USE mo_run_config,         ONLY: grid_generatingCenter, grid_generatingSubcenter, &
    &                              number_of_grid_used, msg_level, check_uuid_gracefully
  USE mo_master_control,     ONLY: my_process_is_ocean
  USE mo_reshuffle,          ONLY: reshuffle
  USE mo_sync,               ONLY: disable_sync_checks, enable_sync_checks
  USE mo_communication,      ONLY: idx_no, blk_no, idx_1d, makeScatterPattern
  USE mo_util_uuid_types,    ONLY: uuid_string_length
  USE mo_util_uuid,          ONLY: uuid_parse, clear_uuid
  USE mo_name_list_output_config, ONLY: is_grib_output

  USE mo_grid_geometry_info, ONLY: planar_torus_geometry, sphere_geometry, &
    &  set_grid_geometry_derived_info, copy_grid_geometry_info,            &
    & parallel_read_geometry_info, triangular_cell, planar_channel_geometry
  USE mo_alloc_patches,      ONLY: allocate_pre_patch, allocate_remaining_patch
  USE mo_math_constants,     ONLY: pi
  USE mo_reorder_patches,    ONLY: reorder_cells, reorder_edges, &
    &                              reorder_verts
  USE mo_mpi,                ONLY: p_pe_work, my_process_is_mpi_parallel, &
    &                              p_comm_work_test, p_comm_work,         &
    &                              my_process_is_stdio
  USE mo_complete_subdivision, ONLY: generate_comm_pat_cvec1
#ifndef NOMPI
  USE mo_complete_subdivision, ONLY: create_work2test_patterns
#endif
  USE mo_read_netcdf_distributed, ONLY: setup_distrib_read
  USE mo_read_interface, ONLY: t_stream_id, p_t_patch, openInputFile, &
    &                          closeFile, on_cells, on_edges, on_vertices, &
    &                          var_data_2d_int, var_data_2d_wp, &
    &                          var_data_3d_int, var_data_3d_wp, &
    &                          read_2D, read_2D_int, read_2D_extdim, &
    &                          read_2D_extdim_int
#ifndef __NO_ICON_ATMO__
  USE mo_interpol_config,    ONLY: nudge_zone_width
#endif
  USE ppm_distributed_array,  ONLY: dist_mult_array_local_ptr, &
    &                               dist_mult_array_expose

#ifndef NOMPI
  ! The USE statement below lets this module use the routines from
  ! mo_netcdf_parallel where only 1 processor is reading and
  ! broadcasting the results
#ifndef HAVE_PARALLEL_NETCDF
  USE mo_netcdf_parallel, ONLY:                      &
    & nf_nowrite, nf_global, nf_noerr, nf_strerror,  &
    & nf_inq_attid        => p_nf_inq_attid,          &
    & nf_open             => p_nf_open,               &
    & nf_close            => p_nf_close,              &
    & nf_inq_dimid        => p_nf_inq_dimid,          &
    & nf_inq_dimlen       => p_nf_inq_dimlen,         &
    & nf_inq_varid        => p_nf_inq_varid,          &
    & nf_get_att_text     => p_nf_get_att_text,       &
    & nf_get_att_int      => p_nf_get_att_int,        &
    & nf_get_var_int      => p_nf_get_var_int,        &
    & nf_get_vara_int     => p_nf_get_vara_int,       &
    & nf_get_vara_double  => p_nf_get_vara_double_
#endif
#endif
#ifndef NOMPI
#ifdef __SUNPRO_F95
    INCLUDE "mpif.h"
#else
    USE mpi, ONLY: MPI_INFO_NULL
#endif
#endif


  IMPLICIT NONE

  PRIVATE

#if defined(NOMPI) || defined(HAVE_PARALLEL_NETCDF)
  INCLUDE 'netcdf.inc'
#endif

  ! derived type: some grid metadata used for consistency checks
  TYPE t_grid_metadata
    CHARACTER(LEN=uuid_string_length) :: uuid_grid  !< UUID of grid
    CHARACTER(LEN=uuid_string_length) :: uuid_par   !< UUID of parent grid
    INTEGER                           :: grid_root  !< grid root subdivision
    INTEGER                           :: grid_level !< grid bisection level
  END TYPE t_grid_metadata


  !modules interface-------------------------------------------
  !subroutines
  PUBLIC :: import_pre_patches
  PUBLIC :: complete_patches
  PUBLIC :: reorder_patch_refin_ctrl
  PUBLIC :: set_parent_loc_idx

  !-------------------------------------------------------------------------

  !> module name string
  CHARACTER(LEN=*), PARAMETER :: modname = 'mo_model_domimp_patches'

CONTAINS

  !-------------------------------------------------------------------------
  !>
  !!               This subroutine provides patch information to the model.
  !!
  !! Which data are required by the model is described in module
  !! <i>mo_model_domain</i>. The components of the patch are initialized
  !! with data stored in several patch files.
  !!
  !! @par Revision History
  !! Developed  by  Peter Korn, MPI-M (2005).
  !! @par
  !! Modified by L. Bonaventura, MPI-M (2005),
  !! to match completed patch
  !! structure in advanced patch generator.
  !! Modified by A. Gassmann, MPI-M (2007)
  !! - cleaning up the code
  !! Modified by A. Gassmann, MPI-M (2007-04)
  !! - grid information belongs now to the patch type
  !! Modified by Almut Gassmann, MPI-M (2008-09-21)
  !! - min_dual_edge_length no longer needed for new Diffusion
  !! Modified by Almut Gassmann, MPI-M (2008-10-30)
  !! - new subroutine for Coriolis initialization
  !! Modification by Stephan Lorenz, MPI-M (2010-02-06)
  !!  - new subroutine for initialization of ocean patch
  !! Modification by Rainer Johanni (2011-12-04)
  !!  - renamed to import_basic_patches
  !!  - only basic patch information for subdivision is read here
  !!    into the full (undivided, global) patch data structure
  !!
  SUBROUTINE import_pre_patches( patch_pre,num_lev,nshift,lsep_grfinfo)

    INTEGER,                   INTENT(in)    :: num_lev(:), nshift(:)
    TYPE(t_pre_patch), TARGET, INTENT(inout) :: patch_pre(n_dom_start:)
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,                   INTENT(OUT)   :: lsep_grfinfo
    ! local variables:
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':import_pre_patches'
    INTEGER                           :: jg, jg1, n_chd, n_chdc
    INTEGER                           :: jgp            ! parent/child patch index
    TYPE(t_grid_metadata)             :: grid_metadata(0:max_dom)
    TYPE(t_pre_patch), POINTER        :: p_single_patch => NULL()

    !-----------------------------------------------------------------------

    CALL message (routine, 'start to import patches')

    ! Set some basic flow control variables on the patch

    max_childdom = 0

    IF(n_dom_start==0) THEN
      ! The physics parent (parent of the root patch) should also be read
      patch_pre(0)%id = 0
      patch_pre(0)%level = start_lev-1
      patch_pre(0)%parent_id = -1
      patch_pre(0)%parent_child_index = 0
      patch_pre(0)%n_childdom = 1
      patch_pre(0)%n_chd_total = n_dom
      patch_pre(0)%child_id(1) = 1
      DO jg = 1, n_dom
        patch_pre(0)%child_id_list(jg) = jg
      ENDDO
      patch_pre(1)%parent_child_index = 1
    ELSE
      patch_pre(1)%parent_child_index = 0
    ENDIF

    DO jg = 1, n_dom

      patch_pre(jg)%id = jg

      IF (jg == 1) THEN
        patch_pre(jg)%level = start_lev
        patch_pre(jg)%parent_id = 0
      ELSE
        patch_pre(jg)%level = patch_pre(dynamics_parent_grid_id(jg))%level + 1
        patch_pre(jg)%parent_id = dynamics_parent_grid_id(jg)
      ENDIF

      n_chd = 0

      DO jg1 = jg+1, n_dom
        IF (jg == dynamics_parent_grid_id(jg1)) THEN
          n_chd = n_chd + 1
          patch_pre(jg)%child_id(n_chd) = jg1
          patch_pre(jg1)%parent_child_index = n_chd
        ENDIF
      ENDDO

      patch_pre(jg)%n_childdom = n_chd
      max_childdom = MAX(1,max_childdom,n_chd)

      !
      ! store information about vertical levels
      !
      patch_pre(jg)%nlev   = num_lev(jg)
      patch_pre(jg)%nlevp1 = num_lev(jg) + 1

      IF (jg > 1) THEN
        IF (nshift(jg) > 0 ) THEN
          ! nshift has been modified via Namelist => use it
          patch_pre(jg)%nshift = nshift(jg)
        ELSE
          ! set default value, assuming
          !- superimposed vertical levels
          !- 1 nested domain per grid level
          patch_pre(jg)%nshift = num_lev(patch_pre(jg)%parent_id) - num_lev(jg)
        ENDIF

        jgp = patch_pre(jg)%parent_id
        patch_pre(jg)%nshift_total = patch_pre(jgp)%nshift_total + patch_pre(jg)%nshift
      ELSE
        ! Note: the first nshift-value refers to the global domain
        patch_pre(jg)%nshift = 0
        patch_pre(jg)%nshift_total = 0
      ENDIF

    ENDDO

    ! Set information about total number of child domains (called recursively)
    ! and corresponding index lists

    ! Initialization
    DO jg = 1, n_dom
      patch_pre(jg)%n_chd_total      = 0
      patch_pre(jg)%child_id_list(:) = 0
    ENDDO

    DO jg = n_dom, 2, -1
      jg1 = patch_pre(jg)%parent_id
      n_chd = patch_pre(jg1)%n_chd_total
      n_chdc = patch_pre(jg)%n_chd_total
      patch_pre(jg1)%child_id_list(n_chd+1) = jg
      IF (n_chdc > 0) THEN
        patch_pre(jg1)%child_id_list(n_chd+2:n_chd+1+n_chdc) = &
          patch_pre(jg)%child_id_list(1:n_chdc)
      ENDIF
      patch_pre(jg1)%n_chd_total = n_chd+1+n_chdc
    ENDDO


    DO jg = 1, n_dom

      ! make nshift parameter also available for the parent patch
      IF (patch_pre(jg)%n_childdom >= 1) THEN
        patch_pre(jg)%nshift_child = patch_pre(patch_pre(jg)%child_id(1))%nshift
        DO jg1 = 1, patch_pre(jg)%n_childdom
          IF (patch_pre(patch_pre(jg)%child_id(jg1))%nshift /= &
            & patch_pre(jg)%nshift_child) &
            & CALL finish (routine, 'multiple nests at the same level must have the same nshift')
        ENDDO
      ELSE
        patch_pre(jg)%nshift_child = 0
      ENDIF

    ENDDO

    IF (n_dom_start == 0) THEN ! reduced grid for radiation
      ! In case of n_dom_start == 0 nlev, nlevp1, nshift need to be copied from
      ! jg=1 to jg=0
      patch_pre(0)%nlev   = patch_pre(1)%nlev
      patch_pre(0)%nlevp1 = patch_pre(1)%nlevp1
      patch_pre(0)%nshift = patch_pre(1)%nshift
      ! The reduced grid always has the same levels as the global one
      patch_pre(0)%nshift_child = 0
    ENDIF

    patch_pre(n_dom_start:n_dom)%max_childdom =  max_childdom

    CALL set_patches_grid_filename(patch_pre(n_dom_start:n_dom)%grid_filename, &
      &                            patch_pre(n_dom_start:n_dom)%grid_filename_grfinfo)

    ! nullify UUID buffer and other metadata vars
    DO jg = n_dom_start, n_dom
      grid_metadata(jg)%uuid_grid      = ""
      grid_metadata(jg)%uuid_par       = ""
      grid_metadata(jg)%grid_level     = -1
      grid_metadata(jg)%grid_level     = -1
    END DO

    grid_level_loop: DO jg = n_dom_start, n_dom

      p_single_patch => patch_pre(jg)
      CALL read_pre_patch( jg, p_single_patch, grid_metadata(jg), lsep_grfinfo )

    ENDDO grid_level_loop

    ! Perform consistency checks for parent-child connectivities
    !
    ! Note: this metadata is not necessarily available in the grid
    ! file. For reasons of backward compatibility, this check will
    ! then be skipped.

    DO jg = n_dom_start+1, n_dom
      jgp = patch_pre(jg)%parent_id
      
      ! perform UUID crosscheck for parent-child connectivities
      IF ((TRIM(grid_metadata(jg)%uuid_par) /= TRIM(grid_metadata(jgp)%uuid_grid)) .AND. &
        & (LEN_TRIM(grid_metadata(jg)%uuid_par) > 0) .AND. (LEN_TRIM(grid_metadata(jgp)%uuid_grid) > 0)) THEN
        IF (check_uuid_gracefully) THEN
          IF (my_process_is_stdio()) THEN
            CALL warning(routine, 'incorrect uuids in parent-child connectivity file')
          END IF
        ELSE
          WRITE (0,*) "parent grid UUID in child file: ", grid_metadata(jg)%uuid_par
          WRITE (0,*) "parent grid UUID: ", grid_metadata(jgp)%uuid_grid
          CALL finish(routine,  'incorrect uuids in parent-child connectivity file')
        END IF
      ENDIF

      ! check matching grid root and bisection level:
      IF ((grid_metadata(jg)%grid_root  /= grid_metadata(jgp)%grid_root)  .OR.   &
        & (grid_metadata(jg)%grid_level /= (grid_metadata(jgp)%grid_level+1))) THEN
        CALL finish(routine, "incorrect grid root and/or bisection level!")
      END IF
    ENDDO
  END SUBROUTINE import_pre_patches
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  !>
  !! This method_name completes basic patches by
  !! - allocating the remaining arrays
  !! - reading the remaining arrays which are not in the basic patch
  !! - calculating arrays which are not read from input file
  !! - generate basic data structures required for the distributed
  !!   read operation

  SUBROUTINE complete_patches(patch, is_ocean_decomposition, lsep_grfinfo)

    TYPE(t_patch), TARGET,     INTENT(inout) :: patch(n_dom_start:)
    LOGICAL,                   INTENT(IN)    :: is_ocean_decomposition
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,                   INTENT(IN)    :: lsep_grfinfo

    INTEGER :: jg, jgp, n_lp, id_lp(max_dom)
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':complete_patches'

    DO jg = n_dom_start, n_dom

      ! Allocate and preset remaining arrays in patch
      ! operation mode=3: allocate all but the parallelization-related arrays
      !                   (the parallelization-related arrays are allocated in mo_setup_subdivision)
      CALL allocate_remaining_patch(patch(jg),3)
      IF (jg > n_dom_start)  &
        CALL allocate_remaining_patch(p_patch_local_parent(jg),3)
    ENDDO

    DO jg = n_dom_start, n_dom

      ALLOCATE(patch(jg)%cells%dist_io_data, patch(jg)%edges%dist_io_data, &
        &      patch(jg)%verts%dist_io_data)

      CALL setup_distrib_read(patch(jg)%n_patch_cells_g, &
                              patch(jg)%cells%decomp_info, &
                              patch(jg)%cells%dist_io_data)
      CALL setup_distrib_read(patch(jg)%n_patch_edges_g, &
                              patch(jg)%edges%decomp_info, &
                              patch(jg)%edges%dist_io_data)
      CALL setup_distrib_read(patch(jg)%n_patch_verts_g, &
                              patch(jg)%verts%decomp_info, &
                              patch(jg)%verts%dist_io_data)
      IF (jg > n_dom_start) THEN

        ALLOCATE(p_patch_local_parent(jg)%cells%dist_io_data, &
          &      p_patch_local_parent(jg)%edges%dist_io_data, &
          &      p_patch_local_parent(jg)%verts%dist_io_data)

        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_cells_g, &
                                p_patch_local_parent(jg)%cells%decomp_info, &
                                p_patch_local_parent(jg)%cells%dist_io_data)
        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_edges_g, &
                                p_patch_local_parent(jg)%edges%decomp_info, &
                                p_patch_local_parent(jg)%edges%dist_io_data)
        CALL setup_distrib_read(p_patch_local_parent(jg)%n_patch_verts_g, &
                                p_patch_local_parent(jg)%verts%decomp_info, &
                                p_patch_local_parent(jg)%verts%dist_io_data)
      END IF
    ENDDO

    ! initialise scatter patterns (required for io used in read_remaining_patch)
    DO jg = n_dom_start, n_dom
      CALL set_comm_pat_scatter(patch(jg), jg)
      IF (jg > n_dom_start) &
        CALL set_comm_pat_scatter(p_patch_local_parent(jg), jg)
    ENDDO

    ! Fill the subsets information
    DO jg = n_dom_start, n_dom
      CALL fill_grid_subsets(patch(jg))
      IF (jg > n_dom_start)  CALL fill_grid_subsets( p_patch_local_parent(jg) )
    ENDDO

    ! we read in the remaining data before generating the communication patterns
    ! because some data that is read in by allocate_remaining_patch is required
    ! by complete_parallel_setup
    DO jg = n_dom_start, n_dom
      n_lp = 0 ! Number of local parents on the same level
      ! Assemble a list of local parents living on the same level as the current patch
      DO jgp = n_dom_start+1, n_dom
        IF(patch(jgp)%parent_id == jg) THEN
          n_lp = n_lp+1
          id_lp(n_lp) = jgp  ! these are children of the current patch
        ENDIF
      ENDDO

      ! Get all patch information not read by "read_pre_patch"
      CALL read_remaining_patch( jg, patch(jg), n_lp, id_lp, lsep_grfinfo )
    ENDDO

    ! In parent grids: compute "refin_e_ctrl", "refin_v_ctrl" flags
    ! for overlap with nested domain:
    !   
    ! Note: we need to compute these values here, since necessary
    ! indices are only read by "read_remaining_patch" above, but the
    ! resulting "refin_ctrl" flags are used by the
    ! "set_parent_child_relations" subroutine below.
    !   
    DO jg = n_dom_start,n_dom
      jgp = patch(jg)%parent_id
      IF (jgp >= n_dom_start) THEN
        CALL set_parent_refin_ev_ctrl("patch", patch(jgp))
        CALL set_parent_refin_ev_ctrl("local parent patch", p_patch_local_parent(jg))
      END IF
    END DO

    ! set parent-child relationships
    DO jg = n_dom_start, n_dom
      
      IF(jg == n_dom_start) THEN

        ! deallocate parent_loc/glb_idx/blk since it just doesn't exist,
        DEALLOCATE(patch(jg)%cells%parent_glb_idx, &
             patch(jg)%cells%parent_glb_blk, &
             patch(jg)%edges%parent_glb_idx, &
             patch(jg)%edges%parent_glb_blk, &
             patch(jg)%cells%parent_loc_idx, &
             patch(jg)%cells%parent_loc_blk, &
             patch(jg)%edges%parent_loc_idx, &
             patch(jg)%edges%parent_loc_blk)

        ! For parallel runs, child_idx/blk is invalid since it makes
        ! sense only on the local parent
        IF (.NOT. my_process_is_mpi_parallel() .OR. &
            is_ocean_decomposition) THEN
          DEALLOCATE(patch(jg)%cells%child_idx, patch(jg)%cells%child_blk, &
               patch(jg)%edges%child_idx, patch(jg)%edges%child_blk)
        END IF
        
      ELSE

        CALL set_parent_child_relations(p_patch_local_parent(jg), patch(jg))
        CALL set_parent_loc_idx(p_patch_local_parent(jg), patch(jg))

      ENDIF
    END DO

    DO jg = n_dom_start, n_dom
      CALL set_owner_mask(patch(jg)%cells%decomp_info)
      CALL set_owner_mask(patch(jg)%verts%decomp_info)
      CALL set_owner_mask(patch(jg)%edges%decomp_info)
      IF (jg > n_dom_start) THEN
        CALL set_owner_mask(p_patch_local_parent(jg)%cells%decomp_info)
        CALL set_owner_mask(p_patch_local_parent(jg)%verts%decomp_info)
        CALL set_owner_mask(p_patch_local_parent(jg)%edges%decomp_info)
      END IF
    ENDDO

    ! rescale grids
    DO jg = n_dom_start, n_dom
      CALL rescale_grid( patch(jg), grid_length_rescale_factor  )
      IF (jg > n_dom_start)  CALL rescale_grid( p_patch_local_parent(jg), grid_length_rescale_factor )
    ENDDO

    ! do other stuff
    DO jg = n_dom_start, n_dom
      ! set phys_id for verts since this is not read from input
      CALL set_verts_phys_id( patch(jg) )
    ENDDO

    ! generates comm_pat_c/v/e/c1 required by the following initialization
    ! routines (these patterns will be rebuild after the reordering by routine
    ! complete_parallel_setup)
    CALL generate_comm_pat_cvec1(patch, is_ocean_decomposition)

#ifndef NOMPI
    DO jg = n_dom_start, n_dom
      IF (p_test_run) CALL create_work2test_patterns(patch(jg))
    END DO
#endif

    IF (.not. my_process_is_ocean()) THEN
      DO jg = n_dom_start, n_dom
        ! Initialize the data for the quadrilateral cells
        ! formed by the two adjacent cells of an edge.
        ! (later this should be provided by the grid generator)
        CALL init_quad_twoadjcells( patch(jg) )
        ! Initialize butterfly data structure, formed by the
        ! 4 cells sharing the 2 vertices which bound a given edge.
        IF (patch(jg)%geometry_info%cell_type == 3) THEN
          ! not useful for hexagonal grid
          CALL init_butterfly_idx( patch(jg) )
        ENDIF

        CALL init_coriolis( lcoriolis, lplane, patch(jg) )

        ! The same has to be done for local parents in parallel runs
        !
        ! Please note: The call to init_quad_twoadjcells involves boundary
        ! exchange and is not repeated here for local parents.
        ! The arrays calculated there are transfered to the local parent
        ! in transfer_interpol_state where also a lot of other arrays
        ! from the patch state are transferred

        IF (jg>n_dom_start) THEN
          CALL disable_sync_checks
          CALL init_coriolis( lcoriolis, lplane, p_patch_local_parent(jg) )
          
          ! set phys_id for verts since this is not read from input
          CALL set_verts_phys_id( p_patch_local_parent(jg) )

          CALL enable_sync_checks
        ENDIF

      ENDDO
    ENDIF

  CONTAINS

    !-------------------------------------------------------------------------------------------------
    !> Sets the gather communication patterns of a patch

    SUBROUTINE set_comm_pat_scatter(p, jg)
      TYPE(t_patch), INTENT(INOUT):: p
      INTEGER, VALUE :: jg

      p%comm_pat_scatter_c => &
        makeScatterPattern(jg, p%n_patch_cells, p%cells%decomp_info%glb_index, &
        &                  p_comm_work)
      p%comm_pat_scatter_e => &
        makeScatterPattern(jg, p%n_patch_edges, p%edges%decomp_info%glb_index, &
        &                  p_comm_work)
      p%comm_pat_scatter_v => &
        makeScatterPattern(jg, p%n_patch_verts, p%verts%decomp_info%glb_index, &
        &                  p_comm_work)

    END SUBROUTINE set_comm_pat_scatter

  END SUBROUTINE complete_patches

  !-------------------------------------------------------------------------------------------------
  !
  !> Sets child_idx/blk in parent patches.

  SUBROUTINE set_parent_child_relations(p_pp, p_pc)

    TYPE(t_patch), INTENT(INOUT) :: p_pp       !> divided local parent patch
    TYPE(t_patch), INTENT(INOUT) :: p_pc       !> divided child patch

    CHARACTER(LEN=*), PARAMETER :: routine = modname//"::set_parent_child_relations"
    INTEGER   :: i, j, jl, jb, jc, jc_g

    ! Before this call, child_idx/child_blk still point to the global values.
    ! This is changed here.

    ! Attention:
    ! Only inner cells/edges get a valid child index,
    ! indexes for boundary cells/edges are not set.
    ! Therefore when these indexes are used, the code must assure that they are
    ! used only for inner cells/edges!
    ! The main reason for this is that - depending on the number of ghost rows -
    ! there are cells/edges in the parent boundary with missing childs (n_ghost_rows==1)

    ! Set child indices in parent ...

    ! ... cells

    DO j = 1, p_pp%n_patch_cells

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      IF(p_pp%cells%decomp_info%decomp_domain(jl,jb)>0) THEN
        p_pp%cells%child_idx(jl,jb,:) = 0
        p_pp%cells%child_blk(jl,jb,:) = 0
        CYCLE
      ENDIF

      DO i= 1, 4
        jc_g = idx_1d(p_pp%cells%child_idx(jl,jb,i),p_pp%cells%child_blk(jl,jb,i))
        IF(jc_g<1 .OR. jc_g>p_pc%n_patch_cells_g) &
          & CALL finish(routine, 'Invalid cell child index in global parent')
        jc = get_local_index(p_pc%cells%decomp_info%glb2loc_index, jc_g)
        IF(jc <= 0) &
          & CALL finish(routine, 'cell child index outside child domain')
        p_pp%cells%child_blk(jl,jb,i) = blk_no(jc)
        p_pp%cells%child_idx(jl,jb,i) = idx_no(jc)
      ENDDO

    ENDDO

    ! ... edges

    DO j = 1, p_pp%n_patch_edges

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      IF(p_pp%edges%decomp_info%decomp_domain(jl,jb)>1) THEN
        p_pp%edges%child_idx(jl,jb,:) = 0
        p_pp%edges%child_blk(jl,jb,:) = 0
        CYCLE ! only inner edges get a valid parent index
      ENDIF

      DO i= 1, 4

        IF(i==4 .AND. p_pp%edges%refin_ctrl(jl,jb) == -1) THEN
          p_pp%edges%child_blk(jl,jb,i) = blk_no(0)
          p_pp%edges%child_idx(jl,jb,i) = idx_no(0)
          CYCLE
        ENDIF

        jc_g = idx_1d(p_pp%edges%child_idx(jl,jb,i),p_pp%edges%child_blk(jl,jb,i))
        jc = get_valid_local_index(p_pc%edges%decomp_info%glb2loc_index, &
          &                        jc_g, .TRUE.)
        IF(jc == 0) &
          & CALL finish(routine, 'edge child index outside child domain '//int2string(p_pc%id))
        p_pp%edges%child_blk(jl,jb,i) = blk_no(jc)
        p_pp%edges%child_idx(jl,jb,i) = SIGN(idx_no(jc),jc_g)
      ENDDO

    ENDDO

    ! Although this is not really necessary, we set the child index in child
    ! and the parent index in parent to 0 since these have no significance
    ! in the parallel code (and must not be used as they are).

    IF (my_process_is_mpi_parallel()) THEN
      p_pc%cells%child_idx  = 0
      p_pc%cells%child_blk  = 0
      p_pp%cells%parent_glb_idx = 0
      p_pp%cells%parent_glb_blk = 0

      p_pc%edges%child_idx  = 0
      p_pc%edges%child_blk  = 0
      p_pp%edges%parent_glb_idx = 0
      p_pp%edges%parent_glb_blk = 0
    END IF

  END SUBROUTINE set_parent_child_relations


  !-------------------------------------------------------------------------------------------------
  !
  !> Sets parent_loc_idx/blk in child patches.
  !
  SUBROUTINE set_parent_loc_idx(p_pp, p_pc)
    TYPE(t_patch), INTENT(INOUT) :: p_pp       !> divided local parent patch
    TYPE(t_patch), INTENT(INOUT) :: p_pc       !> divided child patch
    ! local variables
    CHARACTER(LEN=*), PARAMETER :: routine = modname//"::set_parent_loc_idx"
    INTEGER   :: j, jl, jb, jp, jp_g

    ! Set parent indices in child ...

    ! ... cells

    DO j = 1, p_pc%n_patch_cells

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      jp_g = idx_1d(p_pc%cells%parent_glb_idx(jl,jb), &
        &           p_pc%cells%parent_glb_blk(jl,jb))
      IF(jp_g<1 .OR. jp_g>p_pp%n_patch_cells_g) &
        & CALL finish(routine, 'Inv. cell parent index in global child')

      jp = get_local_index(p_pp%cells%decomp_info%glb2loc_index, jp_g)
      IF(jp <= 0) THEN
        p_pc%cells%parent_loc_blk(jl,jb) = 0
        p_pc%cells%parent_loc_idx(jl,jb) = 0
      ELSE
        p_pc%cells%parent_loc_blk(jl,jb) = blk_no(jp)
        p_pc%cells%parent_loc_idx(jl,jb) = idx_no(jp)
      ENDIF

    ENDDO

    ! ... edges

    DO j = 1, p_pc%n_patch_edges

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch

      jp_g = idx_1d(p_pc%edges%parent_glb_idx(jl,jb), &
        &           p_pc%edges%parent_glb_blk(jl,jb))
      IF(jp_g<1 .OR. jp_g>p_pp%n_patch_edges_g) &
        & CALL finish(routine, 'Inv. edge parent index in global child')

      jp = get_local_index(p_pp%edges%decomp_info%glb2loc_index, jp_g)
      IF(jp <= 0) THEN
        p_pc%edges%parent_loc_blk(jl,jb) = 0
        p_pc%edges%parent_loc_idx(jl,jb) = 0
      ELSE
        p_pc%edges%parent_loc_blk(jl,jb) = blk_no(jp)
        p_pc%edges%parent_loc_idx(jl,jb) = idx_no(jp)
      ENDIF

    ENDDO

    ! Although this is not really necessary, we deallocate
    ! the parent index in parent since these have no significance
    ! in the parallel code (and must not be used as they are).

    IF (my_process_is_mpi_parallel()) THEN
      DEALLOCATE(p_pp%cells%parent_loc_idx, p_pp%cells%parent_loc_blk, &
           p_pp%edges%parent_loc_idx, p_pp%edges%parent_loc_blk)
    END IF

  END SUBROUTINE set_parent_loc_idx


  !-----------------------------------------------------------------------------
  !>
  !! Sets the owner mask
  SUBROUTINE set_owner_mask(decomp_info)

    TYPE(t_grid_domain_decomp_info), INTENT(inout) :: decomp_info

    INTEGER :: j, jb, jl

    decomp_info%owner_mask = .false.

    DO j = 1, SIZE(decomp_info%glb_index)

      jb = blk_no(j) ! Block index in distributed patch
      jl = idx_no(j) ! Line  index in distributed patch
      decomp_info%owner_mask(jl,jb) = decomp_info%owner_local(j) == p_pe_work
    ENDDO
  END SUBROUTINE set_owner_mask
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !> reorder patch data structure according to the refin_ctrl flags.
  !
  !  Only interior cells/edges/vertices are sorted in decreasing
  !  order.
  !
  !  @author F. Prill, DWD (2013-07-31)
  !
  !  @todo OpenMP parallelization!
  !
  !
  SUBROUTINE reorder_patch_refin_ctrl(patch, child_patch)
    TYPE(t_patch),         INTENT(INOUT) :: patch, child_patch   ! patch data structures
    ! local variables
    INTEGER, ALLOCATABLE :: old2new(:)
    INTEGER              :: n, irefin, ic, ntot, ninterior

    ! -- reorder cells
    ntot      = idx_1d( patch%cells%end_index(min_rlcell), &
      &                 patch%cells%end_block(min_rlcell) )
    ninterior = idx_1d( patch%cells%start_index(min_rlcell_int-1), &
      &                 patch%cells%start_block(min_rlcell_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rlcell_int,-1
      DO ic = 1,ninterior
        IF (patch%cells%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%cells%end_index(irefin)     = idx_no(n)
      patch%cells%end_block(irefin)     = blk_no(n)
      patch%cells%start_index(irefin-1) = idx_no(n+1)
      patch%cells%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_cells(patch, old2new, opt_child_pp=child_patch)
    DEALLOCATE(old2new)

    ! -- reorder edges
    ntot      = idx_1d( patch%edges%end_index(min_rledge), &
      &                 patch%edges%end_block(min_rledge) )
    ninterior = idx_1d( patch%edges%start_index(min_rledge_int-1), &
      &                 patch%edges%start_block(min_rledge_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rledge_int,-1
      DO ic = 1,ninterior
        IF (patch%edges%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%edges%end_index(irefin)     = idx_no(n)
      patch%edges%end_block(irefin)     = blk_no(n)
      patch%edges%start_index(irefin-1) = idx_no(n+1)
      patch%edges%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_edges(patch, old2new, opt_child_pp=child_patch)
    DEALLOCATE(old2new)

    ! -- reorder verts
    ntot      = idx_1d( patch%verts%end_index(min_rlvert), &
      &                 patch%verts%end_block(min_rlvert) )
    ninterior = idx_1d( patch%verts%start_index(min_rlvert_int-1), &
      &                 patch%verts%start_block(min_rlvert_int-1) ) - 1
    ALLOCATE(old2new(ntot))
    DO ic = 1,ntot
      old2new(ic) = ic
    END DO
    n = 0
    DO irefin = -1,min_rlvert_int,-1
      DO ic = 1,ninterior
        IF (patch%verts%refin_ctrl(idx_no(ic), blk_no(ic)) == irefin) THEN
          n = n + 1
          old2new(ic) = n
        END IF
      END DO
      ! update start_index/block and end_index/block
      patch%verts%end_index(irefin)     = idx_no(n)
      patch%verts%end_block(irefin)     = blk_no(n)
      patch%verts%start_index(irefin-1) = idx_no(n+1)
      patch%verts%start_block(irefin-1) = blk_no(n+1)
    END DO
    CALL reorder_verts(patch, old2new)
    DEALLOCATE(old2new)

    ! Copy index bounds to the old 2D fields until the restructuring is completed
    DO n = 1, patch%max_childdom
      patch%cells%end_idx(:,n)   = patch%cells%end_index(:)
      patch%cells%end_blk(:,n)   = patch%cells%end_block(:)
      patch%cells%start_idx(:,n) = patch%cells%start_index(:)
      patch%cells%start_blk(:,n) = patch%cells%start_block(:)
      patch%edges%end_idx(:,n)   = patch%edges%end_index(:)
      patch%edges%end_blk(:,n)   = patch%edges%end_block(:)
      patch%edges%start_idx(:,n) = patch%edges%start_index(:)
      patch%edges%start_blk(:,n) = patch%edges%start_block(:)
      patch%verts%end_idx(:,n)   = patch%verts%end_index(:)
      patch%verts%end_blk(:,n)   = patch%verts%end_block(:)
      patch%verts%start_idx(:,n) = patch%verts%start_index(:)
      patch%verts%start_blk(:,n) = patch%verts%start_block(:)
    ENDDO

  END SUBROUTINE reorder_patch_refin_ctrl
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !>
  ! calculate mean geometry properties for old grids,
  ! the new grids should have these values filled
  ! All the patches should have the same geometry type
  SUBROUTINE set_missing_geometry_info( patch )
    TYPE(t_patch), INTENT(inout), TARGET ::  patch

    CHARACTER(LEN=*), PARAMETER :: routine = modname//':set_missing_geometry_info'

    !-----------------------------------------------------------------------
    SELECT CASE(patch%geometry_info%geometry_type)

    CASE (planar_torus_geometry, planar_channel_geometry)

      CALL finish(routine, "planar_torus_geometry should be read from the grid file")

    CASE (sphere_geometry)
      ! if geometry_info is missing then the grid is trianguler by default
      patch%geometry_info%cell_type = triangular_cell
      IF (patch%cells%max_connectivity /= 3) &
        CALL finish("set_missing_geometry_info","cells%max_cell_connectivity /= 3")

      ! note that the grid_sphere_radius is already rescaled
      patch%geometry_info%sphere_radius = grid_sphere_radius / grid_length_rescale_factor
      ! divide the sphere surface by the number of cells
      ! Note: this works only for old grids
      patch%geometry_info%mean_cell_area = &
        & (4._wp * pi * patch%geometry_info%sphere_radius**2) &
        & / (REAL(20*nroot**2,wp)*4._wp**patch%level)

      patch%geometry_info%domain_length  = 2.0_wp * pi * patch%geometry_info%sphere_radius
      patch%geometry_info%domain_height  = patch%geometry_info%domain_length

      ! Note: the mean_edge_length is not used for the sphere geometry,
      ! and calculating will require global communication. Set to 0
      patch%geometry_info%mean_edge_length = 0.0_wp

    CASE default
      CALL finish(routine, "Undefined geometry type")

    END SELECT

  END SUBROUTINE set_missing_geometry_info

  !-------------------------------------------------------------------------
  !>
  !! Initialization of the patch components with data stored
  !! in files.
  !!
  !! @par Revision History
  !! Developed  by  Peter Korn, MPI-M (2005).
  !! Modified by L. Bonaventura, MPI-M (2005),
  !! to match completed patch
  !! structure in advanced patch generator.
  !! Modified by A. Gassmann, MPI-M (2007-04-03)
  !! - cleaning up and adaptations for reading multiple patches
  !! Modified by A. Gassmann, MPI-M (2007-04-03)
  !! - patch owns grid information, global grid is obsolete.
  !! - changed name from init_patch to read_patch
  !! Modified by A. Gassmann, MPI-M (2008-09-21)
  !! - remove all not netcdf stuff
  !! - HERE we must think of how to use different 'global_cell_type's
  !! Modified by A. Gassmann, MPI-M (2008-10-30)
  !! - read in grid for either triangles or hexagons
  !! Modified by R. Johanni (2011-12-04)
  !! - split into read_basic_patch for reading the basic patch information
  !!   for subdivision into the fully allocated patch and read_remaining_patch
  !!   for reading the remaining information
  !!
  SUBROUTINE read_pre_patch( ig, patch_pre, grid_metadata, lsep_grfinfo )

    CHARACTER(LEN=*), PARAMETER :: routine = modname//':read_pre_patch'
    INTEGER,                           INTENT(in)    ::  ig                  ! domain ID
    TYPE(t_pre_patch), TARGET,         INTENT(inout) ::  patch_pre           ! patch data structure
    TYPE(t_grid_metadata),             intent(INOUT) ::  grid_metadata
    !> If .true., read fields related to grid refinement from separate  grid files:
    LOGICAL,                           INTENT(OUT)   ::  lsep_grfinfo

    ! local variables
    INTEGER, ALLOCATABLE :: &
      & start_idx_c(:,:), end_idx_c(:,:), &  ! temporary arrays to read in index lists
      & start_idx_e(:,:), end_idx_e(:,:), &
      & start_idx_v(:,:), end_idx_v(:,:)

    CHARACTER(LEN=uuid_string_length) :: uuid_string, uuid_string_grfinfo

    ! status variables
    INTEGER :: ist, netcd_status, ncid, ncid_grf, dimid, varid, max_cell_connectivity, &
      &        max_verts_connectivity, ji, jc, ic, ilev, dim_idxlist, ierr
    INTEGER,  POINTER :: local_ptr(:), local_ptr_2d(:,:)
    REAL(wp), POINTER :: local_ptr_wp_2d(:, :)
    LOGICAL :: lhave_phys_id
    !-----------------------------------------------------------------------

    ! set dummy values to zero
!    n_e_halo_cells = 0
!    n_e_halo_edges = 0
!    n_e_halo_verts = 0

    ilev = patch_pre%level

    CALL message (routine, 'start to init patch_pre')

    WRITE(message_text,'(a,a)') 'Read grid file ', TRIM(patch_pre%grid_filename)
    CALL message ('', TRIM(message_text))

#if HAVE_PARALLEL_NETCDF
    CALL nf(nf_open_par(TRIM(patch_pre%grid_filename), &
       &                IOR(nf_nowrite, nf_mpiio), &
       &                p_comm_work, MPI_INFO_NULL, ncid))
#else
    CALL nf(nf_open(TRIM(patch_pre%grid_filename), nf_nowrite, ncid))
#endif

    ! Test, if grid refinement information is available in the NetCDF
    ! file. If not, try to open "patch_pre%grid_filename_grfinfo":
    lsep_grfinfo = (nf_inq_varid(ncid, 'refin_c_ctrl', varid) /= nf_noerr)
    IF (lsep_grfinfo) THEN
      WRITE(message_text,'(a,a)') 'Read gridref info from file ', TRIM(patch_pre%grid_filename_grfinfo)
      CALL message ('', TRIM(message_text))
#if HAVE_PARALLEL_NETCDF
      CALL nf(nf_open_par(TRIM(patch_pre%grid_filename_grfinfo), &
         &                IOR(nf_nowrite, nf_mpiio), p_comm_work, &
         &                MPI_INFO_NULL, ncid_grf))
#else
      CALL nf(nf_open(TRIM(patch_pre%grid_filename_grfinfo), nf_nowrite, ncid_grf))
#endif
    ELSE
      ncid_grf = ncid
    END IF

    uuid_string = 'warning: not given ...' ! To avoid null characters in the standard output

    IF (nf_get_att_text(ncid, nf_global, 'uuidOfHGrid', uuid_string) /= nf_noerr) THEN
      IF (is_grib_output()) THEN
        CALL message(routine, "Warning: uuidOfHGrid not set as an attribute!")
      END IF
      CALL clear_uuid(patch_pre%grid_uuid)
    ELSE
      CALL uuid_parse(uuid_string, patch_pre%grid_uuid)
      WRITE(message_text,'(a,a)') 'grid uuid: ', TRIM(uuid_string)
      CALL message  (routine, message_text)
    END IF

    IF (lsep_grfinfo) THEN ! check correspondence of uuids between main grid file and connectivity info file
      CALL nf(nf_get_att_text(ncid_grf, nf_global, 'uuidOfHGrid', uuid_string_grfinfo))
      IF (TRIM(uuid_string_grfinfo) /= TRIM(uuid_string)) THEN
        WRITE(message_text,'(a,a)') 'uuidOfHGrid of grfinfo file does not match uuidOfHGrid of basic grid file'
        IF (check_uuid_gracefully) THEN
          CALL warning(routine, TRIM(message_text))
        ELSE
          CALL finish (routine, TRIM(message_text))
        END IF
      ENDIF
    END IF

    ! Read also parent grid UUID for subsequent crosscheck (if available):
    grid_metadata%uuid_grid = uuid_string
    ierr = nf_get_att_text(ncid_grf, nf_global, 'uuidOfParHGrid', grid_metadata%uuid_par)
    IF (ierr /= nf_noerr)  grid_metadata%uuid_par = ""

    ! Read additional grid identifiers
    ! grid_generatingCenter
    ! grid_generatingSubcenter
    ! number_of_grid_used
    netcd_status = nf_get_att_int(ncid, nf_global, 'centre', &
      &                           grid_generatingCenter(ig)  )
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'generating center of patch ', ig, ': ',grid_generatingCenter(ig)
      CALL message  (routine, TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: generating center of patch ', ig, ' not found'
      CALL message  (routine, TRIM(message_text))
      ! set default value
      grid_generatingCenter(ig) = 78    ! DWD
    ENDIF

    netcd_status = nf_get_att_int(ncid, nf_global, 'subcentre', &
      &                           grid_generatingSubcenter(ig)  )
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'generating subcenter of patch ', ig, ': ',grid_generatingSubcenter(ig)
      CALL message  (routine, TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: generating subcenter of patch ', ig, ' not found'
      CALL message  (routine, TRIM(message_text))
      ! set default value
      grid_generatingSubcenter(ig) = 255
    ENDIF

    netcd_status = nf_get_att_int(ncid, nf_global, 'number_of_grid_used', &
      &            number_of_grid_used(ig))
    IF (netcd_status == nf_noerr) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'number_of_grid_used of patch ', ig, ': ',number_of_grid_used(ig)
      CALL message  (routine, TRIM(message_text))
    ELSE
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'WARNING: number_of_grid_used of patch ', ig, ' not found'
      CALL message  (routine, TRIM(message_text))
      ! set default value
      number_of_grid_used(ig) = 42
    ENDIF

    CALL nf(nf_get_att_int(ncid, nf_global, 'grid_root', grid_metadata%grid_root))
    IF (grid_metadata%grid_root /= nroot) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'grid_root attribute:', grid_metadata%grid_root,', R:',nroot
      CALL message  (routine, TRIM(message_text))
      WRITE(message_text,'(a)') &
        & 'Mismatch between "grid_root" attribute and "R" parameter in the filename'
      CALL finish  (routine, TRIM(message_text))
    END IF

    CALL nf(nf_get_att_int(ncid, nf_global, 'grid_level', grid_metadata%grid_level))
    IF (grid_metadata%grid_level /= ilev) THEN
      WRITE(message_text,'(a,i4,a,i4)') &
        & 'grid_level attribute:', grid_metadata%grid_level,', B:',ilev
      CALL message  (routine, TRIM(message_text))
      WRITE(message_text,'(a)') &
        & 'Mismatch between "grid_level" attribute and "B" parameter in the filename'
      CALL finish  (routine, TRIM(message_text))
    END IF

    !--------------------------------------
    ! get number of cells, edges and vertices
    CALL nf(nf_inq_dimid(ncid, 'edge', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_edges_g))
    CALL nf(nf_inq_dimid(ncid, 'cell', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_cells_g))
    CALL nf(nf_inq_dimid(ncid, 'vertex', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, patch_pre%n_patch_verts_g))
    CALL nf(nf_inq_dimid(ncid, 'nv', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, max_cell_connectivity))
    patch_pre%cells%max_connectivity = max_cell_connectivity
    CALL nf(nf_inq_dimid(ncid, 'ne', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, max_verts_connectivity))
    patch_pre%verts%max_connectivity = max_verts_connectivity
    ! dimension of start/end index list fields (always 1 in new patch files, but this
    ! provides backward compatibility)
    CALL nf(nf_inq_dimid(ncid, 'max_chdom', dimid))
    CALL nf(nf_inq_dimlen(ncid, dimid, dim_idxlist))
    IF (dim_idxlist>1) THEN
      WRITE(message_text,'(a)') &
        & 'WARNING: you are using an old grid file with multiple nesting'
      CALL message  (routine, TRIM(message_text))
    ENDIF

    !
    ! calculate and save values for the blocking
    !
    ! ... for the cells
    patch_pre%nblks_c = ( patch_pre%n_patch_cells_g - 1 ) / nproma + 1
    patch_pre%npromz_c = patch_pre%n_patch_cells_g - &
      &                  (patch_pre%nblks_c - 1)*nproma
    patch_pre%alloc_cell_blocks = patch_pre%nblks_c

    ! ... for the edges
    patch_pre%nblks_e = ( patch_pre%n_patch_edges_g - 1 ) / nproma + 1
    patch_pre%npromz_e = patch_pre%n_patch_edges_g - &
      &                  (patch_pre%nblks_e - 1) * nproma

    ! ... for the vertices
    patch_pre%nblks_v = ( patch_pre%n_patch_verts_g - 1 ) / nproma + 1
    patch_pre%npromz_v = patch_pre%n_patch_verts_g - &
      &                  (patch_pre%nblks_v - 1)*nproma

    !
    ! allocate temporary arrays to read in data form the grid/patch generator
    !
    ! integer arrays for index lists
    ALLOCATE( start_idx_c(min_rlcell:max_rlcell,dim_idxlist),  &
      & end_idx_c  (min_rlcell:max_rlcell,dim_idxlist),  &
      & start_idx_e(min_rledge:max_rledge,dim_idxlist),  &
      & end_idx_e  (min_rledge:max_rledge,dim_idxlist),  &
      & start_idx_v(min_rlvert:max_rlvert,dim_idxlist),  &
      & end_idx_v  (min_rlvert:max_rlvert,dim_idxlist),  &
      & stat=ist )

    IF (ist /= success) THEN
      CALL finish (routine, 'allocation for array_[cev]_indlist failed')
    ENDIF

!     write(0,*) "allocate_pre_patch..."
    !
    ! Allocate patch arrays which are read here
    !
    CALL allocate_pre_patch( patch_pre )

!     write(0,*) "get idx..."
    ! patch_pre%cells%start(:)
    ! patch_pre%cells%end(:)
    ! nesting does not work for hex grids
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_c', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlcell - min_rlcell + 1, 1/), &
      &                     patch_pre%cells%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_c', varid))
    CALL nf(nf_get_var_int(ncid_grf, varid, end_idx_c(:,:)))
    ! Needed for backward compatibility of old grids
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlcell - min_rlcell + 1, 1/), &
      &                     patch_pre%cells%end(:)))
    IF (dim_idxlist > 1) &
      patch_pre%cells%end(min_rlcell_int) = patch_pre%n_patch_cells_g

    ! patch_pre%edges%start(:)
    ! patch_pre%edges%end(:)
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_e', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rledge - min_rledge + 1, 1/), &
      &                     patch_pre%edges%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_e', varid))
    CALL nf(nf_get_var_int(ncid_grf, varid, end_idx_e(:,:)))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rledge - min_rledge + 1, 1/), &
      &                     patch_pre%edges%end(:)))
    ! Needed for backward compatibility of old grids
    IF (dim_idxlist > 1) &
      patch_pre%edges%end(min_rledge_int) = patch_pre%n_patch_edges_g

    ! patch_pre%verts%start(:)
    ! patch_pre%verts%end(:)
    CALL nf(nf_inq_varid(ncid_grf, 'start_idx_v', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlvert - min_rlvert + 1, 1/), &
      &                     patch_pre%verts%start(:)))
    CALL nf(nf_inq_varid(ncid_grf, 'end_idx_v', varid))
    CALL nf(nf_get_vara_int(ncid_grf, varid, (/1,1/), &
      &                     (/max_rlvert - min_rlvert + 1, 1/), &
      &                     patch_pre%verts%end(:)))
    ! Needed for backward compatibility of old grids
    IF (dim_idxlist > 1) &
      patch_pre%verts%end(min_rlvert_int) = patch_pre%n_patch_verts_g

    ! patch_pre%cells%phys_id(:)
    !
    ! If no domain merging is used, the physical cell/edge ID must not
    ! necessarily specified in the grid file:
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_phys_id, local_ptr)
    lhave_phys_id = (nf_inq_varid(ncid_grf, 'phys_cell_id', varid) == nf_noerr)
    IF (lhave_phys_id) THEN
      CALL nf(nf_get_vara_int(ncid_grf, varid, &
        &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
        &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
        &                     local_ptr(:)))
    ELSE
      local_ptr(:) = ig
    END IF

    ! patch_pre%cells%neighbor
    CALL nf(nf_inq_varid(ncid, 'neighbor_cell_index', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_neighbor, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN
      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy cells arising in case of a pentagon
          IF (local_ptr_2d(ic,ji) == 0) THEN
            IF ( ji /= 6 ) THEN
              local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
              ! this should not happen
              ! CALL finish(routine, "cells%neighbor_idx=0 not at the end")
            END IF
            ! Fill dummy neighbor with an existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF
        END DO  ! ji = 1, 6
      END DO ! cells
    END IF

!     write(0,*) "get edge_of_cell..."
    ! patch_pre%cells%edge
    CALL nf(nf_inq_varid(ncid, 'edge_of_cell', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_edge, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN

      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy edges arising in case of a pentagon
          IF ( local_ptr_2d(ic,ji) == 0 ) THEN
            IF ( ji /= 6 ) local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
            ! Fill dummy edge with existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF

        END DO  ! ji = 1, 6

      END DO ! cells
    ENDIF

    !----------------------------------------------------------------------------------
    ! compute cells%num_edges
    ! works for general unstructured grid
!     write(0,*) "compute cells%num_edges..."
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_num_edges, local_ptr)
    DO jc = patch_pre%cells%local_chunk(1,1)%first, &
      patch_pre%cells%local_chunk(1,1)%first + &
      patch_pre%cells%local_chunk(1,1)%size - 1
      local_ptr(jc) = COUNT(local_ptr_2d(jc, 1:max_cell_connectivity) > 0)
    END DO

    ! patch_pre%cells%vertex
!     write(0,*) "get vertex_of_cell..."
    CALL nf(nf_inq_varid(ncid, 'vertex_of_cell', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_vertex, local_ptr_2d)
    local_ptr_2d(:,:) = 0
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size, &
      &                       max_cell_connectivity/), &
      &                     local_ptr_2d(:,1:max_cell_connectivity)))

    IF (max_cell_connectivity == 6 .AND. use_duplicated_connectivity) THEN

      DO ic = patch_pre%cells%local_chunk(1,1)%first, &
        patch_pre%cells%local_chunk(1,1)%first + &
        patch_pre%cells%local_chunk(1,1)%size - 1

        DO ji = 1, 6

          ! account for dummy verts arising in case of a pentagon
          IF ( local_ptr_2d(ic,ji) == 0 ) THEN
            IF ( ji /= 6 ) local_ptr_2d(ic,ji) = local_ptr_2d(ic,6)
            ! Fill dummy edge with existing index to simplify do loops
            ! Note, however, that related multiplication factors must be zero
            local_ptr_2d(ic,6) = local_ptr_2d(ic,5)
          END IF

        END DO  ! ji = 1, 6

      END DO ! cells
    ENDIF

    ! patch_pre%cells%center latitude
    CALL nf(nf_inq_varid(ncid, 'lat_cell_centre', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_center, local_ptr_wp_2d)
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                        (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 1)))

    ! patch_pre%cells%center longitude
    CALL nf(nf_inq_varid(ncid, 'lon_cell_centre', varid))
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                        (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 2)))

    ! patch_pre%verts%vertex(:)%lat
    CALL nf(nf_inq_varid(ncid, 'latitude_vertices', varid))
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_vertex, &
         local_ptr_wp_2d)
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                        (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 1)))

    ! patch_pre%verts%vertex(:)%lon
    CALL nf(nf_inq_varid(ncid, 'longitude_vertices', varid))
    CALL nf(nf_get_vara_double(ncid, varid, &
      &                        (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                        (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                        local_ptr_wp_2d(:, 2)))

    !------------------------------------------
    ! nesting/lateral boundary indexes
    IF (max_cell_connectivity == 3) THEN ! triangular grid
      ! patch_pre%cells%parent
      IF (ig > 0) THEN
        CALL nf(nf_inq_varid(ncid_grf, 'parent_cell_index', varid))
        CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_parent, local_ptr)
        CALL nf(nf_get_vara_int(ncid_grf, varid, &
          &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
          &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
          &                     local_ptr))
      ENDIF
    ELSE
      CALL message ('read_patch',&
        & 'nesting incompatible with non-triangular grid')
    ENDIF

!     write(0,*) "dist_mult_array_expose(patch_pre%cells%dist)..."
    CALL dist_mult_array_expose(patch_pre%cells%dist)

    ! patch_pre%cells%refin_ctrl
!     write(0,*) "refin_c_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_c_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%cells%dist, c_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%cells%local_chunk(1,1)%first/), &
      &                     (/patch_pre%cells%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! patch_pre%edges%parent
!     write(0,*) "parent_edge_index..."
    IF (ig > 0) THEN
      CALL nf(nf_inq_varid(ncid_grf, 'parent_edge_index', varid))
      CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_parent, local_ptr)
      CALL nf(nf_get_vara_int(ncid_grf, varid, &
        &                     (/patch_pre%edges%local_chunk(1,1)%first/), &
        &                     (/patch_pre%edges%local_chunk(1,1)%size/), &
        &                     local_ptr))
    ENDIF
    ! patch_pre%edges%refin_ctrl
!     write(0,*) "refin_e_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_e_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! patch_pre%verts%refin_ctrl
!     write(0,*) "refin_v_ctrl..."
    CALL nf(nf_inq_varid(ncid_grf, 'refin_v_ctrl', varid))
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_refin_ctrl, &
         local_ptr)
    CALL nf(nf_get_vara_int(ncid_grf, varid, &
      &                     (/patch_pre%verts%local_chunk(1,1)%first/), &
      &                     (/patch_pre%verts%local_chunk(1,1)%size/), &
      &                     local_ptr))

    ! BEGIN NEW SUBDIV

!     write(0,*) max_verts_connectivity, "cells_of_vertex..."
    CALL nf(nf_inq_varid(ncid, 'cells_of_vertex', varid))
!     write(0,*) max_verts_connectivity, "  dist_mult_array_local_ptr..."
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_cell, local_ptr_2d)
!     write(0,*) max_verts_connectivity, "  nf_get_vara_int..."
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%verts%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%verts%local_chunk(1,1)%size, &
      &                       max_verts_connectivity/), &
      &                     local_ptr_2d))
    ! eliminate indices < 0, this should not happen but some older grid files
    ! seem to contain such indices
!     write(0,*) "SIZE(local_ptr_2d):", SIZE(local_ptr_2d,1), SIZE(local_ptr_2d,2)
!     write(0,*) max_verts_connectivity, "  WHERE..."
    WHERE(local_ptr_2d(:, :) < 0) local_ptr_2d(:, :) = 0
    ! account for dummy cells arising in case of a pentagon
    ! Fill dummy cell with existing index to simplify do loops
    ! Note, however, that related multiplication factors must be zero
!     write(0,*) "move_dummies_to_end verts%local_chunk..."
    CALL move_dummies_to_end(local_ptr_2d, &
      patch_pre%verts%local_chunk(1,1)%size, max_verts_connectivity, &
      use_duplicated_connectivity)

    !
    ! Set verts%num_edges
!     write(0,*) "Set verts%num_edges..."
    CALL dist_mult_array_local_ptr(patch_pre%verts%dist, v_num_edges, local_ptr)
    DO ji = patch_pre%verts%local_chunk(1,1)%first, &
      patch_pre%verts%local_chunk(1,1)%first + &
      patch_pre%verts%local_chunk(1,1)%size - 1
      local_ptr(ji) = COUNT(local_ptr_2d(ji, 1:max_verts_connectivity) > 0)
    END DO

    ! patch_pre%edges%cell(:,:)
    CALL nf(nf_inq_varid(ncid, 'adjacent_cell_of_edge', varid))
    CALL dist_mult_array_local_ptr(patch_pre%edges%dist, e_cell, local_ptr_2d)
    CALL nf(nf_get_vara_int(ncid, varid, &
      &                     (/patch_pre%edges%local_chunk(1,1)%first, 1/), &
      &                     (/patch_pre%edges%local_chunk(1,1)%size, 2/), &
      &                     local_ptr_2d))
    WHERE(local_ptr_2d(:, :) < 0) local_ptr_2d(:, :) = 0

    CALL dist_mult_array_expose(patch_pre%verts%dist)
    CALL dist_mult_array_expose(patch_pre%edges%dist)

    ! END NEW SUBDIV


    CALL nf(nf_close(ncid))
    IF (lsep_grfinfo) CALL nf(nf_close(ncid_grf))

    !
    ! deallocate temporary arrays to read in data form the grid/patch generator
    !
    ! index lists arrays
    DEALLOCATE( start_idx_c, end_idx_c, start_idx_e, end_idx_e, start_idx_v, end_idx_v, &
      & stat=ist )
    IF (ist /= success) THEN
      CALL finish (routine, 'deallocation for array_[cev]_indlist failed')
    ENDIF

    CALL message (routine, 'read_patches finished')

  END SUBROUTINE read_pre_patch
  !-------------------------------------------------------------------------


  !-------------------------------------------------------------------------
  !> Reads the remaining patch information into the divided patch
  SUBROUTINE read_remaining_patch( ig, patch, n_lp, id_lp, lsep_grfinfo )

    INTEGER,       INTENT(in)    ::  ig       ! domain ID
    TYPE(t_patch), INTENT(inout), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level
    !> If .true., read fields related to grid refinement from separate  grid files
    LOGICAL,       INTENT(IN)    :: lsep_grfinfo

    INTEGER :: ncid, varid, ncid_grf
    TYPE(t_stream_id) :: stream_id, stream_id_grf
    INTEGER :: ip, jv, idx, blk
    INTEGER :: max_cell_connectivity, max_verts_connectivity

    INTEGER :: return_status

    TYPE(t_patch), POINTER :: p_p, patch0
    TYPE(p_t_patch), TARGET :: patches(0:n_lp)
    TYPE(var_data_2d_int)  :: multivar_2d_data_int(0:n_lp)
    TYPE(var_data_2d_wp)  :: multivar_2d_data_wp(0:n_lp)
    TYPE(var_data_3d_int) :: multivar_3d_data_int(0:n_lp)
    TYPE(var_data_3d_wp) :: multivar_3d_data_wp(0:n_lp)
    LOGICAL :: lhave_phys_id


!    REAL(wp), POINTER :: tmp_check_array(:,:)
!    REAL(wp) :: max_diff

    CHARACTER(LEN=*), PARAMETER :: routine = modname//':read_remaining_patch'
    !-----------------------------------------------------------------------

    CALL message ('mo_model_domimp_patches:read_remaining_patch', &
      & 'Read gridmap file '//TRIM(patch%grid_filename))

    DO ip = 0, n_lp
      patches(ip)%p => get_patch_ptr(patch, id_lp, ip)
    ENDDO

    CALL nf(nf_open(TRIM(patch%grid_filename), nf_nowrite, ncid))
    stream_id = openInputFile(TRIM(patch%grid_filename), patches)

    IF (lsep_grfinfo) THEN
      CALL nf(nf_open(TRIM(patch%grid_filename_grfinfo), nf_nowrite, ncid_grf))
      stream_id_grf = openInputFile(TRIM(patch%grid_filename_grfinfo), patches)
    ELSE
      ncid_grf = ncid
      stream_id_grf = stream_id
    ENDIF

    max_cell_connectivity = patch%cells%max_connectivity
    max_verts_connectivity = patch%verts%max_connectivity

    patch%boundary_depth_index = 0
#ifndef __NO_ICON_ATMO__
    patch%boundary_depth_index = nudge_zone_width
    return_status = nf_inq_attid(ncid, nf_global, 'boundary_depth_index', varid)
    IF (return_status == nf_noerr) THEN
       CALL nf(nf_get_att_int(ncid, nf_global, 'boundary_depth_index', patch%boundary_depth_index))
       IF (nudge_zone_width < 0) THEN
         nudge_zone_width = patch%boundary_depth_index - 4
       ENDIF
!       IF ( nudge_zone_width > patch%boundary_depth_index - 4) THEN
!         CALL finish ('mo_model_domain_import:read_patch',  &
!           & 'nudge_zone_width > patch%boundary_depth_index - 4')
!       ENDIF
    ENDIF
#endif

    IF (max_cell_connectivity /= 3) & ! not triangular grid
      CALL message ('read_remaining_patch',&
        & 'nesting incompatible with non-triangular grid')

    !------------------------------------------
    ! nesting/lateral boundary indexes

    ! p_p%cells%phys_id(:,:)
    IF (ig > 1) THEN
      lhave_phys_id = nf_inq_varid(ncid_grf, 'phys_cell_id', varid) == nf_noerr
    ELSE
      lhave_phys_id = .FALSE.
    END IF
    IF (lhave_phys_id) THEN
      DO ip = 0, n_lp
        multivar_2d_data_int(ip)%data => patches(ip)%p%cells%phys_id(:,:)
      END DO
      CALL read_2D_int(stream_id_grf, on_cells, 'phys_cell_id', n_lp+1, &
        &              multivar_2d_data_int(:))
    ELSE
      DO ip = 0, n_lp
        patches(ip)%p%cells%phys_id(:,:) = ig
      END DO
    END IF

    ! p_p%cells%edge_orientation(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_wp(ip)%data => &
        patches(ip)%p%cells%edge_orientation(:,:,1:max_cell_connectivity)
    END DO
    CALL read_2D_extdim(stream_id, on_cells, 'orientation_of_normal', &
      &                 n_lp+1, fill_array=multivar_3d_data_wp(:), &
      &                 start_extdim=1, end_extdim=max_cell_connectivity)

    ! p_p%cells%area(:,:)
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%cells%area(:,:)
    END DO
    CALL read_2D(stream_id, on_cells, 'cell_area_p', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! p_p%edges%phys_id(:,:)
    IF (ig > 1) THEN
      lhave_phys_id = nf_inq_varid(ncid_grf, 'phys_edge_id', varid) == nf_noerr
    ELSE
      lhave_phys_id = .FALSE.
    END IF
    IF (lhave_phys_id) THEN
      DO ip = 0, n_lp
        multivar_2d_data_int(ip)%data => patches(ip)%p%edges%phys_id(:,:)
      END DO
      CALL read_2D_int(stream_id_grf, on_edges, 'phys_edge_id', n_lp+1, &
        &              multivar_2d_data_int(:))
    ELSE
      DO ip = 0, n_lp
        patches(ip)%p%edges%phys_id(:,:) = ig
      ENDDO
    END IF

    ! p_p%edges%cell_idx(:,:,:)
    ! p_p%edges%cell_blk(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_int(ip)%data => patches(ip)%p%edges%cell_idx(:,:,1:2)
    END DO
    CALL read_2D_extdim_int(stream_id, on_edges, 'adjacent_cell_of_edge', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=2)
    DO ip = 0, n_lp
      p_p => patches(ip)%p
      multivar_3d_data_int(ip)%data(:,:,1:2) = &
        (get_valid_local_index(p_p%cells%decomp_info%glb2loc_index, &
         &                     multivar_3d_data_int(ip)%data(:,:,1:2), &
         &                     .TRUE.))
      p_p%edges%cell_blk(:,:,1:2) = &
        blk_no(multivar_3d_data_int(ip)%data(:,:,1:2))
      p_p%edges%cell_idx(:,:,1:2) = &
        idx_no(multivar_3d_data_int(ip)%data(:,:,1:2))
    END DO

    ! p_p%edges%vertex_idx(:,:,:)
    ! p_p%edges%vertex_blk(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_int(ip)%data => patches(ip)%p%edges%vertex_idx(:,:,1:2)
    END DO
    CALL read_2D_extdim_int(stream_id, on_edges, 'edge_vertices', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=2)
    DO ip = 0, n_lp
      p_p => patches(ip)%p
      multivar_3d_data_int(ip)%data(:,:,1:2) = &
        (get_valid_local_index(p_p%verts%decomp_info%glb2loc_index, &
         &                     multivar_3d_data_int(ip)%data(:,:,1:2), &
         &                     .TRUE.))
      p_p%edges%vertex_blk(:,:,1:2) = &
        blk_no(multivar_3d_data_int(ip)%data(:,:,1:2))
      p_p%edges%vertex_idx(:,:,1:2) = &
        idx_no(multivar_3d_data_int(ip)%data(:,:,1:2))
    END DO

    ! p_p%edges%tangent_orientation(:,:)
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%tangent_orientation(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'edge_system_orientation', n_lp+1, &
      &          multivar_2d_data_wp(:))

#ifdef __GNUC__
    DO ip = 0, n_lp
      ALLOCATE(multivar_2d_data_wp(ip)%data(nproma, patches(ip)%p%nblks_e))
      multivar_2d_data_wp(ip)%data(:,:) = 0.0_wp
    END DO
#endif

    ! p_p%edges%center(:,:)%lon
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%center(:,:)%lon
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'lon_edge_centre', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%center(:,:)%lon = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! p_p%edges%center(:,:)%lat
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%center(:,:)%lat
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'lat_edge_centre', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%center(:,:)%lat = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_normal(:,:)%v1
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%primal_normal(:,:)%v1
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'zonal_normal_primal_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%primal_normal(:,:)%v1 = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! p_p%edges%primal_normal(:,:)%v2
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%primal_normal(:,:)%v2
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'meridional_normal_primal_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%primal_normal(:,:)%v2 = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_normal(:,:)%v1
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%dual_normal(:,:)%v1
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'zonal_normal_dual_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%dual_normal(:,:)%v1 = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! p_p%edges%dual_normal(:,:)%v2
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%dual_normal(:,:)%v2
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'meridional_normal_dual_edge', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%dual_normal(:,:)%v2 = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip)%data)
    END DO
#endif

    ! p_p%edges%primal_edge_length(:,:)
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%primal_edge_length(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'edge_length', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! p_p%edges%dual_edge_length(:,:)
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%edges%dual_edge_length(:,:)
    END DO
    CALL read_2D(stream_id, on_edges, 'dual_edge_length', n_lp+1, &
      &          multivar_2d_data_wp(:))

    ! p_p%edges%edge_vert_length(:,:,1:2)
    DO ip = 0, n_lp
      multivar_3d_data_wp(ip)%data => patches(ip)%p%edges%edge_vert_length(:,:,1:2)
    END DO
    CALL read_2D_extdim(stream_id, on_edges, 'edge_vert_distance', n_lp+1, &
      &                 fill_array=multivar_3d_data_wp(:), start_extdim=1, &
      &                 end_extdim=2)

    ! p_p%edges%edge_cell_length(:,:,1:2)
    DO ip = 0, n_lp
      multivar_3d_data_wp(ip)%data => patches(ip)%p%edges%edge_cell_length(:,:,1:2)
    END DO
    CALL read_2D_extdim(stream_id, on_edges, 'edge_cell_distance', n_lp+1, &
      &                 fill_array=multivar_3d_data_wp(:), start_extdim=1, &
      &                 end_extdim=2)

    ! p_p%verts%neighbor_idx(:,:,:)
    ! p_p%verts%neighbor_blk(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_int(ip)%data => &
        patches(ip)%p%verts%neighbor_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'vertices_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      CALL block_connectivity(&
        patches(ip)%p%verts%neighbor_idx(:,:,1:max_verts_connectivity), &
        patches(ip)%p%verts%neighbor_blk(:,:,1:max_verts_connectivity), &
        patches(ip)%p%verts%decomp_info%glb2loc_index, &
        SIZE(patches(ip)%p%verts%neighbor_idx, 2), &
        patches(ip)%p%n_patch_verts, max_verts_connectivity)
    END DO

    ! p_p%verts%cell_idx(:,:,:)
    ! p_p%verts%cell_blk(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_int(ip)%data => &
        patches(ip)%p%verts%cell_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'cells_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      CALL block_connectivity(&
        patches(ip)%p%verts%cell_idx(:,:,1:max_verts_connectivity), &
        patches(ip)%p%verts%cell_blk(:,:,1:max_verts_connectivity), &
        patches(ip)%p%cells%decomp_info%glb2loc_index, &
        SIZE(patches(ip)%p%verts%cell_idx, 2), &
        patches(ip)%p%n_patch_verts, max_verts_connectivity)
    END DO

    ! p_p%verts%edge_idx(:,:,:)
    ! p_p%verts%edge_blk(:,:,:)
    DO ip = 0, n_lp
      multivar_3d_data_int(ip)%data => &
        patches(ip)%p%verts%edge_idx(:,:,1:max_verts_connectivity)
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'edges_of_vertex', &
      &                     n_lp+1, fill_array=multivar_3d_data_int(:), &
      &                     start_extdim=1, end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      CALL block_connectivity(&
        patches(ip)%p%verts%edge_idx(:,:,1:max_verts_connectivity), &
        patches(ip)%p%verts%edge_blk(:,:,1:max_verts_connectivity), &
        patches(ip)%p%edges%decomp_info%glb2loc_index, &
        SIZE(patches(ip)%p%verts%edge_idx, 2), &
        patches(ip)%p%n_patch_verts, max_verts_connectivity)
    END DO

    ! p_p%verts%num_edges
    DO ip = 0, n_lp
      p_p => patches(ip)%p
      IF (use_duplicated_connectivity) THEN
        DO jv = 1, p_p%n_patch_verts
          idx = idx_no(jv)
          blk = blk_no(jv)
          p_p%verts%num_edges(idx,blk) = &
            COUNT((p_p%verts%edge_idx(idx,blk,1:max_verts_connectivity) &
              &    /= p_p%verts%edge_idx(idx,blk,max_verts_connectivity)) .OR.&
              &   (p_p%verts%edge_blk(idx,blk,1:max_verts_connectivity) &
              &    /= p_p%verts%edge_blk(idx,blk,max_verts_connectivity))) + 1
        END DO
      ELSE
        DO jv = 1, p_p%n_patch_verts
          idx = idx_no(jv)
          blk = blk_no(jv)
          p_p%verts%num_edges(idx,blk) = &
            COUNT(p_p%verts%edge_idx(idx,blk,1:max_verts_connectivity) /= 0)
        END DO
      END IF
    END DO

    ! p_p%verts%edge_orientation(:,:,:)
    DO ip = 0, n_lp
      p_p => patches(ip)%p
      ALLOCATE( &
        multivar_3d_data_int(ip)%data(SIZE(p_p%verts%edge_orientation, 1), &
        &                             SIZE(p_p%verts%edge_orientation, 2), &
        &                             max_verts_connectivity))
    END DO
    CALL read_2D_extdim_int(stream_id, on_vertices, 'edge_orientation', n_lp+1, &
      &                     fill_array=multivar_3d_data_int(:), start_extdim=1,&
      &                     end_extdim=max_verts_connectivity)
    DO ip = 0, n_lp
      p_p => patches(ip)%p
      ! move dummy edges to end and set edge orientation to zero
      CALL move_dummies_to_end_idxblk( &
        multivar_3d_data_int(ip)%data(:,:,:), p_p%n_patch_verts, &
        max_verts_connectivity, .FALSE.)
      p_p%verts%edge_orientation(:,:,1:max_verts_connectivity) = &
        REAL(multivar_3d_data_int(ip)%data(:,:,:), wp)
      DEALLOCATE(multivar_3d_data_int(ip)%data)
    END DO

    ! p_p%verts%dual_area(:,:)
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%verts%dual_area(:,:)
    END DO
    CALL read_2D(stream_id, on_vertices, 'dual_area_p', n_lp+1, &
      &          fill_array=multivar_2d_data_wp(:))

    !-------------------------------------------------
    ! read geometry parameters
    patch0 => patches(0)%p
    return_status = parallel_read_geometry_info(ncid, patch0%geometry_info)
    IF (return_status /= 0 ) THEN
      ! the information was missing from the file (ie old grids)
      ! calclulate basic settings
!       CALL finish("","did not read from file")
      CALL set_missing_geometry_info(patch0)
    ENDIF
    CALL set_grid_geometry_derived_info(patch0%geometry_info)

    DO ip = 1, n_lp
      CALL copy_grid_geometry_info(from_geometry_info = patch0%geometry_info, &
        &                     to_geometry_info = patches(ip)%p%geometry_info)
!       write(0,*) "-------------------------------------------------------"
!       write(0,*) "area, char_length=", p_p%geometry_info%mean_cell_area, &
!         & p_p%geometry_info%mean_characteristic_length
!       write(0,*) "-------------------------------------------------------"
    ENDDO
    !---------------------------------------------------
    ! cartesian positions
    IF (gridfile_has_cartesian_info(ncid)) THEN
      CALL read_cartesian_positions(stream_id, patch, n_lp, id_lp, patches)
    ELSE
      CALL calculate_cartesian_positions(patch, n_lp, id_lp, patches)
    END IF
    !-------------------------------------------------


    CALL nf(nf_close(ncid))
    CALL closeFile(stream_id)
    IF (lsep_grfinfo) THEN
      CALL nf(nf_close(ncid_grf))
      CALL closeFile(stream_id_grf)
    END IF
    !-------------------------------------------------
    !Check for plane_torus case
    IF(p_p%geometry_info%geometry_type == planar_torus_geometry .AND. .NOT. is_plane_torus) THEN
      CALL message(routine, &
        & "Grid is plane torus: turning on is_plane_torus automatically")
      is_plane_torus = .TRUE.
    END IF

    IF(p_p%geometry_info%geometry_type /= planar_torus_geometry .AND. is_plane_torus) &
      CALL finish(routine,"Input grid is NOT plane torus, Stopping")
    !-------------------------------------------------

    CALL message (modname//':read_remaining_patch', 'read finished')

  END SUBROUTINE read_remaining_patch
  !-------------------------------------------------------------------------

  SUBROUTINE convert_to_local_index(array, array_size, max_connectivity, &
    &                               glb2loc_index, duplicate)

    INTEGER, INTENT(in) :: array_size, max_connectivity
    INTEGER, INTENT(inout) :: array(:,:,:)
    TYPE(t_glb2loc_index_lookup), INTENT(in) :: glb2loc_index
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: idx, blk, i

    IF (duplicate) THEN
      DO i = 1, array_size
          idx = idx_no(i)
          blk = blk_no(i)
          array(idx,blk,1:max_connectivity) = &
            get_valid_local_index(glb2loc_index, &
            &                     array(idx,blk,1:max_connectivity), .TRUE.)
      END DO
    ELSE
      DO i = 1, array_size
          idx = idx_no(i)
          blk = blk_no(i)
          array(idx,blk,1:max_connectivity) = &
            MAX(get_local_index(glb2loc_index, &
              &                 array(idx,blk,1:max_connectivity)), 0)
      END DO
    END IF
  END SUBROUTINE convert_to_local_index
  !-------------------------------------------------------------------------
  ! Checks for the pentagon case and moves dummy cells to end.
  ! The dummy entry is either set to 0 or duplicated from the last one
  SUBROUTINE move_dummies_to_end(array, array_size, max_connectivity, duplicate)

    INTEGER, INTENT(in) :: array_size, max_connectivity
    INTEGER, INTENT(inout) :: array(1:array_size,1:max_connectivity)
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: i, zeros(1:max_connectivity), num_non_zero
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':move_dummies_to_end'

    IF (duplicate) THEN
      DO i = 1, array_size
        num_non_zero = COUNT(array(i,1:max_connectivity) /= 0)
        IF (num_non_zero /= max_connectivity) THEN
          IF (num_non_zero == 0) THEN
            ! CALL warning(routine, "no connectivity found")
            CYCLE
          END IF
          array(i,1:num_non_zero) = PACK(array(i,1:max_connectivity), &
            &                            array(i,1:max_connectivity) /= 0)
          array(i,num_non_zero+1:max_connectivity) = array(i,num_non_zero)
        END IF
      END DO
    ELSE
      zeros(:) = 0
      DO i = 1,  array_size
        IF (ANY(array(i,1:max_connectivity) /= 0)) THEN

          array(i,1:max_connectivity) = PACK(array(i,1:max_connectivity), &
            &                                array(i,1:max_connectivity) /= 0, &
            &                                zeros(:))
        END IF
      END DO
    END IF

  END SUBROUTINE move_dummies_to_end
  !-------------------------------------------------------------------------

  SUBROUTINE block_connectivity(indices, blocks, glb2loc_index, &
       nconn_blk, nconn_glb, max_connect)
    INTEGER, INTENT(in) :: nconn_glb, nconn_blk, max_connect
    TYPE(t_glb2loc_index_lookup), INTENT(in) :: glb2loc_index
    INTEGER, INTENT(inout) :: indices(nproma,nconn_blk,max_connect), &
         blocks(nproma,nconn_blk,max_connect)
    INTEGER :: jb,jl,jn

    CALL convert_to_local_index(indices, nconn_glb, max_connect, &
         glb2loc_index, use_duplicated_connectivity)
    ! account for dummy cells arising in case of a pentagon
    ! Fill dummy cell with existing index to simplify do loops
    ! Note, however, that related multiplication factors must be zero
    CALL move_dummies_to_end_idxblk(indices, nconn_glb, max_connect, &
         use_duplicated_connectivity)
    DO jn = 1, max_connect
      DO jb = 1, nconn_blk
        DO jl = 1, nproma
          blocks(jl,jb,jn) = blk_no(indices(jl,jb,jn))
          indices(jl,jb,jn) = idx_no(indices(jl,jb,jn))
        END DO
      END DO
    END DO

  END SUBROUTINE block_connectivity
  !-------------------------------------------------------------------------
  ! Checks for the pentagon case and moves dummy cells to end.
  ! The dummy entry is either set to 0 or duplicated from the last one
  SUBROUTINE move_dummies_to_end_idxblk(array, array_size, max_connectivity, &
    &                                   duplicate)

    INTEGER, INTENT(inout) :: array(:,:,:)
    INTEGER, INTENT(in) :: array_size, max_connectivity
    LOGICAL, INTENT(in) :: duplicate

    INTEGER :: i, idx, blk, zeros(1:max_connectivity), num_non_zero
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':move_dummies_to_end_idxblk'

    IF (duplicate) THEN
      DO i = 1, array_size
        idx = idx_no(i)
        blk = blk_no(i)
        num_non_zero = COUNT(array(idx,blk,1:max_connectivity) /= 0)
        IF (num_non_zero /= max_connectivity) THEN
          IF (num_non_zero == 0) THEN
            ! CALL warning(routine, "no connectivity found")
            CYCLE
          END IF
          array(idx,blk,1:num_non_zero) = &
            PACK(array(idx,blk,1:max_connectivity), &
            &    array(idx,blk,1:max_connectivity) /= 0)
          array(idx,blk,num_non_zero+1:max_connectivity) = &
            array(idx,blk,num_non_zero)
        END IF
      END DO
    ELSE
      zeros(:) = 0
      DO i = 1,  array_size
        idx = idx_no(i)
        blk = blk_no(i)
        IF (ANY(array(idx,blk,1:max_connectivity) /= 0)) THEN
          array(idx,blk,1:max_connectivity) = &
            PACK(array(idx,blk,1:max_connectivity), &
            &    array(idx,blk,1:max_connectivity) /= 0, zeros(:))
        END IF
      END DO
    END IF

  END SUBROUTINE move_dummies_to_end_idxblk
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  LOGICAL FUNCTION gridfile_has_cartesian_info(ncid)
    INTEGER, INTENT(in)    :: ncid

    INTEGER :: varid

    gridfile_has_cartesian_info = &
      nf_inq_varid(ncid, 'cell_circumcenter_cartesian_x', varid) == nf_noerr

  END FUNCTION gridfile_has_cartesian_info
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE read_cartesian_positions(stream_id, patch, n_lp, id_lp, patches)

    TYPE(t_stream_id) :: stream_id
    TYPE(t_patch), INTENT(inout) ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level
    TYPE(p_t_patch), TARGET, INTENT(in) :: patches(0:n_lp)

    TYPE(var_data_2d_wp)  :: multivar_2d_data_wp(0:n_lp)
    INTEGER :: ip
#ifdef __GNUC__
    INTEGER :: nblks
#endif

    CHARACTER(LEN=*), PARAMETER :: routine = modname//':read_cartesian_positions'
    !-----------------------------------------------------------------------

#ifdef __GNUC__
    DO ip = 0, n_lp
      nblks = patches(ip)%p%alloc_cell_blocks
      ALLOCATE(multivar_2d_data_wp(ip)%data(nproma, nblks))
      multivar_2d_data_wp(ip)%data(:,:) = 0.0_wp
    END DO
#endif

    ! patches%cells%cartesian_center(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%cells%cartesian_center(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%cells%cartesian_center(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%cells%cartesian_center(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%cells%cartesian_center(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%cells%cartesian_center(:,:)%x(2) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%cells%cartesian_center(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%cells%cartesian_center(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_cells, 'cell_circumcenter_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%cells%cartesian_center(:,:)%x(3) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip)%data)
      ALLOCATE(multivar_2d_data_wp(ip)%data(nproma,patches(ip)%p%nblks_e))
      multivar_2d_data_wp(ip)%data(:,:) = 0.0_wp
    END DO
#endif

    ! patches%edges%cartesian_center(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_center(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_center(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%cartesian_center(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_center(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_center(:,:)%x(2) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%cartesian_center(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_center(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_middle_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_center(:,:)%x(3) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%cartesian_dual_middle(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%cartesian_dual_middle(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(2) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%cartesian_dual_middle(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_middle_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%cartesian_dual_middle(:,:)%x(3) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%primal_cart_normal(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%primal_cart_normal(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%primal_cart_normal(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%primal_cart_normal(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%primal_cart_normal(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%primal_cart_normal(:,:)%x(2) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%primal_cart_normal(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%primal_cart_normal(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_primal_normal_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%primal_cart_normal(:,:)%x(3) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%dual_cart_normal(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%dual_cart_normal(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_x', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%dual_cart_normal(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%dual_cart_normal(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%dual_cart_normal(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_y', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%dual_cart_normal(:,:)%x(2) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%edges%dual_cart_normal(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data &
        => patches(ip)%p%edges%dual_cart_normal(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_edges, 'edge_dual_normal_cartesian_z', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%edges%dual_cart_normal(:,:)%x(3) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip)%data)
      nblks =  patches(ip)%p%nblks_v
      ALLOCATE(multivar_2d_data_wp(ip)%data(nproma,nblks))
      multivar_2d_data_wp(ip)%data(:,:) = 0.0_wp
    END DO
#endif

    ! patches%verts%cartesian(:,:)%x(1)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%verts%cartesian(:,:)%x(1)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_x_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%verts%cartesian(:,:)%x(1) &
        = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%verts%cartesian(:,:)%x(2)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%verts%cartesian(:,:)%x(2)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_y_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%verts%cartesian(:,:)%x(2) = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

    ! patches%verts%cartesian(:,:)%x(3)
#ifndef __GNUC__
    DO ip = 0, n_lp
      multivar_2d_data_wp(ip)%data => patches(ip)%p%verts%cartesian(:,:)%x(3)
    END DO
#endif
    CALL read_2D(stream_id, on_vertices, 'cartesian_z_vertices', n_lp+1, &
      &          multivar_2d_data_wp(:))
#ifdef __GNUC__
    DO ip = 0, n_lp
      patches(ip)%p%verts%cartesian(:,:)%x(3) = multivar_2d_data_wp(ip)%data(:,:)
    END DO
#endif

#ifdef __GNUC__
    DO ip = 0, n_lp
      DEALLOCATE(multivar_2d_data_wp(ip)%data)
    END DO
#endif

  END SUBROUTINE read_cartesian_positions
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE calculate_cartesian_positions(patch, n_lp, id_lp, patches)

    TYPE(t_patch), INTENT(inout), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in)    ::  n_lp     ! Number of local parents on the same level
    INTEGER,       INTENT(in)    ::  id_lp(:) ! IDs of local parents on the same level
    TYPE(p_t_patch), INTENT(inout) :: patches(0:n_lp)

    INTEGER :: ip
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':calculate_cartesian_positions'
    !-----------------------------------------------------------------------
    ! GZ: This routine is always called with grids from the global grid generator, so this warning
    ! is more confusing than useful
    IF (msg_level >= 15 .OR. patch%geometry_info%geometry_type /= sphere_geometry) THEN
      CALL message(routine, " is called")
    ENDIF
    IF (patch%geometry_info%geometry_type /= sphere_geometry) &
      CALL finish(routine, "geometry_type /= sphere_geometry")

    DO ip = 0, n_lp
      ! calculate Cartesian components of primal normal
      ! (these are old grids)
      CALL calculate_patch_cartesian_positions(patches(ip)%p)

    ENDDO

  END SUBROUTINE calculate_cartesian_positions
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  ! get_patch_ptr returns a pointer to patch for idx=0,
  ! a pointer to a local parent patch from the list otherwise
  FUNCTION get_patch_ptr(patch, id_lp, idx) result(patch_ptr)
    TYPE(t_patch), TARGET ::  patch  ! patch data structure
    INTEGER,       INTENT(in) ::  id_lp(:) ! IDs of local parents on the same level
    TYPE(t_patch), POINTER :: patch_ptr
    INTEGER, INTENT(in)    :: idx

    IF (idx == 0) THEN
      patch_ptr => patch
    ELSE
      patch_ptr => p_patch_local_parent(id_lp(idx))
    ENDIF
  END FUNCTION get_patch_ptr
  !-------------------------------------------------------------------------

  !-------------------------------------------------------------------------
  SUBROUTINE nf(STATUS, warnonly, silent)

    INTEGER, INTENT(in)           :: STATUS
    LOGICAL, INTENT(in), OPTIONAL :: warnonly
    LOGICAL, INTENT(in), OPTIONAL :: silent

    LOGICAL :: lwarnonly, lsilent

    lwarnonly = .FALSE.
    lsilent   = .FALSE.
    IF(PRESENT(warnonly)) lwarnonly = .TRUE.
    IF(PRESENT(silent))   lsilent   = silent

    IF (lsilent) RETURN
    IF (STATUS /= nf_noerr) THEN
      IF (lwarnonly) THEN
        CALL message(modname//': netCDF error', nf_strerror(STATUS), &
          & level=em_warn)
      ELSE
        CALL finish(modname//': netCDF error', nf_strerror(STATUS))
      ENDIF
    ENDIF

  END SUBROUTINE nf


  ! -----------------------------------------------------------------
  !
  !  In parent grids: compute "refin_e_ctrl", "refin_v_ctrl" flags for
  !  overlap with nested domain.
  !
  !  input: 
  !   p_p% id
  !        n_patch_cells, n_patch_edges, n_patch_verts, n_patch_edges_g
  !        cells%decomp_info%decomp_domain
  !        edges%decomp_info%glb_index
  !        cells%refin_ctrl
  !
  !        cells%edge_idx/blk  (from "read_remaining_patch")
  !        cells%vertex_idx/blk  (from "read_remaining_patch")
  !
  !  @author F. Prill, DWD (2016-06-16)
  !
  SUBROUTINE set_parent_refin_ev_ctrl(description, p_p)
    CHARACTER(LEN=*),      INTENT(IN)    :: description !< description string (for debugging purposes)
    TYPE(t_patch), TARGET, INTENT(INOUT) :: p_p         !< parent patch
    ! local variables
    INTEGER,          PARAMETER :: UNDEFINED_VALUE = 99
    CHARACTER(LEN=*), PARAMETER :: routine = modname//':set_parent_refin_ev_ctrl'
    INTEGER              :: jc_c, jb_c, jc_v, jb_v, j, i, jc_e, jb_e, jc_n, jb_n,  &
      &                     communicator, refin_e, iidx, j_v
    INTEGER, ALLOCATABLE :: in_data(:), dst_idx(:), out_data(:,:), out_count(:,:), &
      &                     refin_v_ctrl(:,:)

    CALL message(modname, "set negative edge/vertex refin_ctrl flags "//&
      &"for "//TRIM(description)//" (id "//TRIM(int2string(p_p%id))//")")

    ! EDGES ------------------------------------------------------
    !
    ! The "refin_e_ctrl" value is the sum of the "refin_c_ctrl" values
    ! of the two adjacent cells!
    !   
    ALLOCATE(in_data(3*p_p%n_patch_cells), dst_idx(3*p_p%n_patch_cells))

    ! by looping over the cells, we visit each edge twice
    iidx = 0
    DO j = 1,p_p%n_patch_cells
      jc_c = idx_no(j) ;  jb_c = blk_no(j)

      ! loop over inner domain and boundary
      IF (p_p%cells%decomp_info%decomp_domain(jc_c,jb_c) > 1)  CYCLE

      ! (set this entry only, if the left or right side is zero or
      ! negative:)
      IF (p_p%cells%refin_ctrl(jc_c,jb_c) > 0)  CYCLE
      DO i=1,3
        jc_e = p_p%cells%edge_idx(jc_c,jb_c,i)
        jb_e = p_p%cells%edge_blk(jc_c,jb_c,i)

        iidx = iidx + 1
        dst_idx(iidx) = p_p%edges%decomp_info%glb_index(idx_1d(jc_e,jb_e))
        ! (we need a shift of -1 to distinguish between zero values
        ! and unset values:)
        in_data(iidx) = p_p%cells%refin_ctrl(jc_c,jb_c) - 1
      END DO
    END DO

    ALLOCATE(out_data(2,p_p%n_patch_edges),out_count(2,p_p%n_patch_edges))
    out_data = 0
    communicator   = p_comm_work
 
    ! communicate between processors:
    CALL reshuffle("send refin_c_ctrl to edges", dst_idx(1:iidx), in_data(1:iidx),     &
      &            p_p%edges%decomp_info%glb_index, p_p%n_patch_edges_g, communicator, &
      &            out_data, out_count)

    ! now loop over the edges and sum the two adjacent cell refin_ctrl
    ! flags:
    DO j = 1,p_p%n_patch_edges
      jc_e = idx_no(j)  ;  jb_e = blk_no(j) 

      ! nothing was set for this entry:
      IF (out_count(1,j) == 0)  CYCLE
            
      IF (out_count(2,j) == 0) THEN
        ! (the "+2" takes care of the "-1"-shift above)
        refin_e = 2*out_data(1,j) + 2
      ELSE
        refin_e = out_data(1,j) + out_data(2,j) + 2
      END IF

      p_p%edges%refin_ctrl(jc_e,jb_e) = refin_e 
    END DO

    ! Reset edges%refin_ctrl at outer boundary of a limited-area grid
    DO j = 1,p_p%n_patch_cells
      jc_c = idx_no(j) ;  jb_c = blk_no(j)

      IF (p_p%cells%decomp_info%decomp_domain(jc_c,jb_c) > 1)  CYCLE

      IF (p_p%cells%refin_ctrl(jc_c,jb_c) == -1)  THEN
        DO i=1,3
          jc_e = p_p%cells%edge_idx(jc_c,jb_c,i)
          jb_e = p_p%cells%edge_blk(jc_c,jb_c,i)

          jc_n = p_p%cells%neighbor_idx(jc_c,jb_c,i)
          jb_n = p_p%cells%neighbor_blk(jc_c,jb_c,i)

          IF (jc_n <= 0 .OR. jb_n <= 0)  p_p%edges%refin_ctrl(jc_e,jb_e) = -1
        END DO
      ENDIF
    END DO

    DEALLOCATE(in_data, dst_idx, out_data, out_count)

    ! VERTICES ---------------------------------------------------
    !
    ! The "refin_v_ctrl" value is the minimum of the adjacent
    ! "refin_c_ctrl" values!

    ALLOCATE(refin_v_ctrl(SIZE(p_p%verts%refin_ctrl,1),SIZE(p_p%verts%refin_ctrl,2)))
    refin_v_ctrl(:,:) = UNDEFINED_VALUE

    DO j = 1,p_p%n_patch_cells
      jc_c = idx_no(j) ;  jb_c = blk_no(j)

      ! loop over inner domain and boundary
      IF (p_p%cells%decomp_info%decomp_domain(jc_c,jb_c) > 2)  CYCLE
      IF (p_p%cells%refin_ctrl(jc_c,jb_c) > 0)  CYCLE

      DO i=1,3
        jc_v = p_p%cells%vertex_idx(jc_c,jb_c,i)
        jb_v = p_p%cells%vertex_blk(jc_c,jb_c,i)
        j_v = idx_1d(jc_v,jb_v)

        refin_v_ctrl(jc_v,jb_v) = MIN(refin_v_ctrl(jc_v,jb_v), p_p%cells%refin_ctrl(jc_c,jb_c))
      END DO
    END DO
    DO j = 1,p_p%n_patch_verts
      jc_v = idx_no(j)  ;  jb_v = blk_no(j) 
            
      IF (refin_v_ctrl(jc_v,jb_v) < 0) THEN     
        p_p%verts%refin_ctrl(jc_v,jb_v) = refin_v_ctrl(jc_v,jb_v)
      END IF
    END DO
    DEALLOCATE(refin_v_ctrl)

  END SUBROUTINE set_parent_refin_ev_ctrl


END MODULE mo_model_domimp_patches


