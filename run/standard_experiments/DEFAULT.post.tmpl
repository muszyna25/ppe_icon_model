#! /usr/bin/env python #%# -*- mode: python -*- vi: set ft=python :
#SBATCH --account=%{ACCOUNT}
#%  if JOB.qos is defined:
#SBATCH --qos=%{JOB.qos}
#%  endif
#SBATCH --job-name=%{EXP_ID}.%{JOB.id}
#SBATCH --partition=prepost,compute,compute2
# #SBATCH --ntasks=%{JOB.tasks}
#%  if JOB.hardware_threads|default('') is not set:
# #SBATCH --ntasks-per-core=1
#SBATCH --exclusive
#%  endif
#SBATCH --output=%{SCRIPT_DIR}/%x.%j.log
#SBATCH --time=%{JOB.time_limit}

'''\
Post-process ICON experiment data for a given period
'''

import argparse
import logging
import os
import re
import subprocess
import sys

from distutils import sysconfig
lib_dir = sysconfig.get_python_lib(1, 0, '%{MODEL_DIR}')
if lib_dir not in sys.path:
    sys.path.insert(1, lib_dir)
import mtime

# Process command line options

def check_date(arg):
    try:
        value = mtime.DateTime(arg)
    except ValueError as ve:
        raise argparse.ArgumentTypeError(ve.message)
    return value

command_line = argparse.ArgumentParser(description=__doc__.split('\n', 1)[0])
command_line.add_argument('start_date', type=check_date, help=
    'first date of period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('-V', '--version', action='version', version='%{VERSIONS_|join(" ")|trim()}')
command_line.add_argument('-c', '--clean', action='store_true', help=
    'remove output files. '
    'Use ONLY after you made absolutely sure that the raw data still exists!')
args = command_line.parse_args()

# Do time computations using mtime

mtime.setCalendar(mtime.CALENDAR_TYPE.%{calendar_mtime})

initial_date = mtime.DateTime('%{INITIAL_DATE}')
start_date = args.start_date

if start_date < initial_date:
    sys.stderr.write("Oops: start_date is before initial_date\n")
    sys.exit(1)
    
interval = mtime.TimeDelta('%{INTERVAL}')
next_date = start_date + interval
end_date = next_date + mtime.TimeDelta('-%{ATMO_TIME_STEP}')

atmo_time_step = mtime.TimeDelta('%{ATMO_TIME_STEP}')
ocean_time_step = mtime.TimeDelta('%{OCEAN_TIME_STEP}')

# Format cdo shifttime commands for Makefile generation

atmo_shift_list = ['-shifttime,-{0}{1}'.format(value, name)
    for (name, value) in atmo_time_step.items() if name != 'sign' and value]
ocean_shift_list = ['-shifttime,-{0}{1}'.format(value, name)
    for (name, value) in ocean_time_step.items() if name != 'sign' and value]

# Define required output

tags = []
#%  if output is defined:
#%      for tag in output.scalars|sort if output[tag] and output[tag]|lower != 'false':
tags.append('%{tag}')
#%      endfor
#%      if output.fixed is defined and output.fixed.scalars:
#%          set fixed = output.fixed
if start_date == initial_date:
#%          for tag in fixed.scalars|sort if fixed[tag] and fixed[tag]|lower != 'false':
    tags.append('%{tag}')
#%          endfor
#%      endif
#%  endif
#%  if output is defined and output.pressure_level is defined:
tags.extend(%{output.pressure_level.sections|d([])})
#%  endif

restart_tags = ['atm', 'oce']

#%#
# Set-up template variables

weed = re.compile(r'[-:]|\.\d+$')

template_dict = {}
template_dict['start_date'] = re.sub(weed, '', str(start_date.date))
template_dict['start_stamp'] = re.sub(weed, '', str(start_date))
template_dict['next_date'] = re.sub(weed, '', str(next_date.date))
template_dict['next_stamp'] = re.sub(weed, '', str(next_date))
template_dict['end_date'] = re.sub(weed, '', str(end_date.date))
template_dict['end_stamp'] = re.sub(weed, '', str(end_date))
template_dict['atmo_shift_spec'] = ' '.join(reversed(atmo_shift_list))
template_dict['ocean_shift_spec'] = ' '.join(reversed(ocean_shift_list))
template_dict['tags'] = ' '.join(tags)
template_dict['restart_tags'] = ' '.join(restart_tags)

# Prolog

logging.basicConfig(format='%(asctime)s: %(levelname)s%(message)s',
                    level=logging.INFO)
logging.addLevelName(logging.DEBUG, 'Debug: ')
logging.addLevelName(logging.INFO, '')
logging.addLevelName(logging.WARNING, 'Hey: ')
logging.addLevelName(logging.ERROR, 'Oops: ')
logging.addLevelName(logging.CRITICAL, 'Sorry: ')
#%  if JOB.debug_level|d(0)|int > 0:
logging.getLogger().setLevel(logging.DEBUG)
#%  endif
#%#
logging.info('post processing started '
             'for {start_stamp}-{end_stamp}'.format(**template_dict))

# Set-up directory structure

post_dir = os.path.join('%{WORK_DIR}',
                        'post_{start_stamp}-{end_stamp}'.format(**template_dict))
run_dir = os.path.join('%{WORK_DIR}',
                       '%{jobs.run.subdir}'.format(**template_dict))
restart_dir = '%{RESTART_DIR}'
data_dir = '%{DATA_DIR}'

if not os.path.isdir(post_dir): os.makedirs(post_dir)
if not os.path.isdir(restart_dir): os.makedirs(restart_dir)
if not os.path.isdir(data_dir): os.makedirs(data_dir)

# Add dirs as template variables
template_dict['run_dir'] = run_dir
template_dict['restart_dir'] = restart_dir
template_dict['data_dir'] = data_dir

os.chdir(post_dir)

logging.info('working directory is {0}'.format(post_dir))

# Generate Makefile

makefile_template = '''\
CDO = cdo
CDOFLAGS = -r -O -z zip

NC_COMPRESS = nccopy -d 1 -s

EXP_ID = %{EXP_ID}
RESTART_DIR = {restart_dir}
DATA_DIR = {data_dir}
RUN_DIR = {run_dir}

TAGS = {tags}
RESTART_TAGS = {restart_tags}

TARGETS = $(TAGS:%=$(DATA_DIR)/$(EXP_ID)_%_{start_date}.nc) $(RESTART_TAGS:%=$(RESTART_DIR)/$(EXP_ID)_restart_%_{next_date}.nc)

all: $(TARGETS)

clean:
	$(RM) $(TARGETS)

$(RESTART_DIR)/$(EXP_ID)_restart_%_{next_date}.nc: $(RUN_DIR)/$(EXP_ID)_restart_%_{next_stamp}Z.nc
	$(NC_COMPRESS) -s $< $@

$(DATA_DIR)/$(EXP_ID)_atm_%_{start_date}.nc: $(RUN_DIR)/$(EXP_ID)_atm_%_{start_stamp}Z.nc
	$(CDO) $(CDOFLAGS) {atmo_shift_spec} $< $@

$(DATA_DIR)/$(EXP_ID)_lnd_%_{start_date}.nc: $(RUN_DIR)/$(EXP_ID)_lnd_%_{start_stamp}Z.nc
	$(CDO) $(CDOFLAGS) {atmo_shift_spec} $< $@

$(DATA_DIR)/$(EXP_ID)_oce_%_{start_date}.nc: $(RUN_DIR)/$(EXP_ID)_oce_%_{start_stamp}Z.nc
	$(CDO) $(CDOFLAGS) {ocean_shift_spec} $< $@

$(DATA_DIR)/$(EXP_ID)_hamoc%_{start_date}.nc: $(RUN_DIR)/$(EXP_ID)_hamoc%_{start_stamp}Z.nc
	$(CDO) $(CDOFLAGS) {ocean_shift_spec} $< $@

#%  if output is defined and output.pressure_level is defined:
#%    for target in output.pressure_level.sections|d([]):
#%        set info = output.pressure_level[target]
$(DATA_DIR)/$(EXP_ID)_%{target}_{start_date}.nc: $(DATA_DIR)/$(EXP_ID)_%{info.input_tag}_{start_date}.nc
	$(CDO) $(CDOFLAGS) selvar,%{info.variable_names|join(',')} -ap2pl,%{info.pressure_levels|join(',')} $< $@
#%    endfor
#%  endif
'''

makefile_name = 'Makefile'
makefile = open(makefile_name, mode='w')
makefile.write(makefile_template.format(**template_dict))
makefile.close()

# Run the actual make process

make_args = ['make', '-k', '-j', '%{JOB.tasks|d(1)}']
if args.clean: make_args.append('clean')
make = subprocess.Popen(make_args, universal_newlines=True,
                        stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
for line in make.stdout:
    logging.info(line.rstrip('\n'))

make_result = make.wait()
if make_result:
    logging.error("'{0}' returned {1}".format(' '.join(make_args), make_result))
    sys.exit(1)

os.chdir('%{SCRIPT_DIR}')

#%  if JOB.debug_level|d(0)|int < 1:
for entry in os.listdir(post_dir):
    os.remove(os.path.join(post_dir, entry))
os.removedirs(post_dir)
#%    if jobs.run.subdir|d(''):
logging.info("removing run dir '{run_dir}'".format(**template_dict))
for entry in os.listdir(run_dir):
    os.remove(os.path.join(run_dir, entry))
os.removedirs(run_dir)
#%    endif

#%  endif

# Epilog

logging.info('post processing finished '
             'for {start_stamp}-{end_stamp}'.format(**template_dict))

#% for job in JOB['.trigger']|list:
subprocess.check_call(['%{JOB.batch_command}', '%{EXP_ID}.%{job}', str(start_date)])
#% endfor
