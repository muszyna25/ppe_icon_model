#! %{JOB.python3} #%# -*- mode: python -*- vi: set ft=python :

'''\
Create intake directory entries for a given period
'''

#%#
#%#-# Generate list of all active file definitions
#%#-
#%- set extension_map = {1: 'grb', 2: 'grb', 3: 'nc', 4: 'nc', 5: 'nc', 6: 'nc'}
#%- set file_infos = []
#%  for namelist in namelists:
#%    set namelist_data = namelists[namelist]
#%    for group in namelist_data.sections if group|match('output_nml'):
#%      set group_data = namelist_data[group]
#%      set hidden = group_data['.hide'] is set
#%      if not hidden:
#%        set level_types = []
#%        for name in group_data.scalars if name|match('^(.+)_varlist$')
#%          do level_types.append(name|match('^(.+)_varlist$'))
#%        endfor
#%        set filename_format = group_data.filename_format
#%        set output_filename = group_data.output_filename
#%        set levtype_l = level_types[-1]
#%        set extension = extension_map[group_data.filetype|d(4)|int]
#%        set output_start = group_data.output_start
#%        set output_start = output_start|replace('{initial_date}', INITIAL_DATE)
#%        set output_start = output_start|replace('{final_date}', FINAL_DATE)
#%        set output_end = group_data.output_end
#%        set output_end = output_end|replace('{initial_date}', INITIAL_DATE)
#%        set output_end = output_end|replace('{final_date}', FINAL_DATE)
#%#       # Revert file life-time hack
#%-       set file_interval = group_data.file_interval
#%        if file_interval.endswith('T1S'):
#%          set file_interval = file_interval[0:-3]
#%        endif
#%        set info = {}
#%        do info.update(filename_tmpl = filename_format|replace('<output_filename>', output_filename)|replace('<levtype_l>', levtype_l)+'.'+extension)
#%        do info.update(experiment = EXP_ID)
#%        do info.update(frequency = group_data.output_interval)
#%        do info.update(level_type = levtype_l)
#%        do info.update(variable = group_data[levtype_l+'_varlist']|list)
#%        do info.update(output_start = output_start)
#%        do info.update(output_end = output_end)
#%        do info.update(file_interval = file_interval)
#%        do info.update(operation = group_data.operation or 'inst')
#%        do file_infos.append(info)
#%      endif
#%    endfor
#%  endfor

import argparse
import errno
import json
import os
import re
import sys
import xarray

#% include 'standard_experiments/mtime.tmpl'
#% include 'standard_experiments/logging.tmpl'

# Process command line options

def check_date(arg):
    try:
        value = mtime.DateTime(arg)
    except ValueError as ve:
        raise argparse.ArgumentTypeError(ve.message)
    return value

command_line = argparse.ArgumentParser(description=__doc__.split('\n', 1)[0])
command_line.add_argument('start_date', type=check_date, help=
    'first date of period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('next_date', nargs='?', type=check_date, help=
    'date of next period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('-w', '--work-stamp', action='store_true', help=
    'expect time stamps in model format instead of post-processed')
command_line.add_argument('-W', '--work-dir', action='store_true', help=
    'take files from work directory instead of data directory')
command_line.add_argument('-I', '--input-dir', help=
    'take files from a non-standard directory')
command_line.add_argument('-V', '--version', action='version', version="""
$Id: DEFAULT.intake.tmpl 0 2021-08-31 12:37:44Z m221078 $
%{VERSIONS_|join('\n')}
""")
args = command_line.parse_args()

os.chdir('%{SCRIPT_DIR}')

# Do time computations using mtime

initial_date = mtime.DateTime('%{INITIAL_DATE}')
start_date = args.start_date
next_date = args.next_date

# if start_date < initial_date:
#     logging.error("start_date is before initial_date")
#     sys.exit(1)

if not next_date:
    interval = mtime.TimeDelta('%{INTERVAL}')
    next_date = start_date + interval

offset = mtime.TimeDelta('-%{namelists.NAMELIST_atm.run_nml.modeltimestep}')
end_date = next_date + offset

# Determine time stamp format conversion

weed = re.compile(r'[-:]|\.\d+$') # for re-formatting of date/time stamps
def data_stamp(date):
    return re.sub(weed, '', str(date.date))
def work_stamp(date):
    return re.sub(weed, '', str(date)) + 'Z'

start_stamp = data_stamp(start_date)
end_stamp = data_stamp(end_date)

stamp = work_stamp if args.work_stamp else data_stamp

# Determine directory to read files from

input_dir = '%{DATA_DIR}'
if args.work_dir:
    input_dir = os.path.join('%{WORK_DIR}', '%{jobs.run.subdir}').format(
        start_stamp=start_stamp, end_stamp=end_stamp)

if args.input_dir:
    input_dir = args.input_dir

# Start processing

logging.info('%{JOB.id} process started for %s-%s', start_stamp, end_stamp)

# Define file metadata

file_infos = %{file_infos}

# Read intake-esm catalog

intake_name = '%{JOB.url|d((DATA_ROOT, MODEL_SUBDIR, JOB.id, "index.json")|join("/"))}'

attributes = ("variable", "experiment", "frequency", "level_type", "operation", "time_range")
assets = "filename"
columns = attributes + (assets,)

try:

    with open(intake_name) as intake_file:
        intake_data = json.load(intake_file)

        intake_attributes = tuple(att['column_name'] for att in intake_data['attributes'])
        intake_assets = intake_data['assets']['column_name']

        if intake_attributes != attributes or intake_assets != assets:
            logging.error("incompatible catalog '{0}'".format(intake_file))
            sys.exit(1)

except (IOError, ValueError) as e:

    logging.warning("cannot load '{0}': {1}".format(intake_name, str(e)))
    logging.warning("creating new catalog")

    intake_data = {
        "esmcat_version": "0.1.0",
        "id": "ICON-ESM",
        "description": "Default catalog for ICON-ESM experiments by MPIMet",
        "attributes": [{"column_name": att} for att in attributes],
        "assets": {
            "column_name": assets,
            "format": "netcdf"
        },
        "aggregation_control": {
            "variable_column_name": "variable",
            "groupby_attrs": [att for att in attributes
                if att not in ["time_range", "variable"]],
            "aggregations": [
                {
                    "type": "union",
                    "attribute_name": "variable"
                },
                {
                    "type": "join_existing",
                    "attribute_name": "time_range",
                    "options": {
                         "dim": "time",
                         "coords": "minimal",
                         "compat": "override"
                    }
                }
            ]
        },
        "catalog_dict": []
    }

# Create catalog data for this year

catalog_dict = {}
for entry in intake_data['catalog_dict']:
    # Skip files that no longer exist
    if os.path.isfile(entry[assets]):
        key = tuple(str(entry[att]) for att in attributes)
        catalog_dict[key] = entry
    else:
        logging.warning("unsubscribing non-existing file '%s'", entry[assets])

for info in file_infos:

    output_start = mtime.DateTime(info['output_start'])
    output_end = mtime.DateTime(info['output_end'])
    file_interval = mtime.TimeDelta(info['file_interval'])

    # Check if current interval is overlapping with file life-time
    if output_start < next_date and output_end > start_date:

        # Set lower and upper limits for file life-time wrtt current interval
        file_start_date = max(start_date, output_start)
        file_next_date = min(next_date, output_end)

        # Generate output per file date
        date = file_start_date
        while date < file_next_date:
            # Set current file name
            info[assets] = os.path.join(input_dir,
                info['filename_tmpl'].replace('<datetime2>', stamp(date)))
            logging.info("current file '%s'", info[assets])

            # Determine time range
            info['time_range'] = str(date) + '-' + str(date+file_interval+offset)

            # Get variables from file
            try:
                with xarray.open_dataset(info[assets], use_cftime=True) as ds:
                    # Try to remove spurious entries (eg bounds)
                    info['variable'] = [var for var in ds.data_vars
                        if not var.endswith('_bnds')]

                key = tuple(str(info[att]) for att in attributes)
                if (key in catalog_dict and
                    info[assets] != catalog_dict[key][assets]):
                    logging.warning("key '%s' is already taken by assets '%s'", 
                        str(key), catalog_dict[key][assets])
                else:
                    catalog_dict[key] = {col: info[col] for col in columns}

            except IOError as x:
                if x.errno != errno.ENOENT: raise
                logging.warning("skipped: "+x.strerror)
                

            date += file_interval

intake_data["catalog_dict"] = list(catalog_dict.values())

# Write intake-esm catalog

with open(intake_name, 'w') as intake_file:
    json.dump(intake_data, intake_file,
              indent=4, separators=(',',': ')) ### , sort_keys=True)
    intake_file.write('\n')

# Finish processing

logging.info('%{JOB.id} process finished for %s-%s', start_stamp, end_stamp)

