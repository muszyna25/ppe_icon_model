#! %{JOB.python3} #%# -*- mode: python -*- vi: set ft=python :

'''\
Create intake directory entries for a given period
'''

#%#
#%#-# Generate list of all active file definitions
#%#-
#%- set extension_map = {1: 'grb', 2: 'grb', 3: 'nc', 4: 'nc', 5: 'nc', 6: 'nc'}
#%- set format_map = {'grb': 'grib', 'nc': 'netcdf'}
#%- set file_infos = []
#%  for namelist in namelists:
#%    set namelist_data = namelists[namelist]
#%    set realm = namelist|split('_')|last
#%    set remove_list = namelist_data['.remove']|d([])|list
#%    for group in namelist_data.sections if group|match('^output_nml'):
#%      set group_data = namelist_data[group]
#%      set hidden = (group_data['.hide'] is set) or (group in remove_list)
#%      if not hidden:
#%        set level_types = []
#%        for name in group_data.scalars if name|match('^(.+)_varlist$')
#%          do level_types.append(name|match('^(.+)_varlist$'))
#%        endfor
#%        set filename_format = group_data.filename_format
#%        set output_filename = group_data.output_filename
#%        set levtype_l = level_types[-1]
#%        set extension = extension_map[group_data.filetype|d(4)|int]
#%        set output_start = group_data.output_start
#%        set output_start = output_start|replace('{initial_date}', INITIAL_DATE)
#%        set output_start = output_start|replace('{final_date}', FINAL_DATE)
#%        set output_end = group_data.output_end
#%        set output_end = output_end|replace('{initial_date}', INITIAL_DATE)
#%        set output_end = output_end|replace('{final_date}', FINAL_DATE)
#%#       # Revert file life-time hack
#%-       set file_interval = group_data.file_interval
#%        if file_interval.endswith('T1S'):
#%          set file_interval = file_interval[0:-3]
#%        endif
#%        set info = {}
#%        do info.update(project = PROJECT|d('private'))
#%        do info.update(institution_id = INSTITUTION_ID|d('MPI-M'))
#%        do info.update(source_id = SOURCE_ID|d('ICON-ESM'))
#%        do info.update(experiment_id = EXP_KIND)
#%        do info.update(simulation_id = EXP_ID)
#%        do info.update(realm = realm)
#%        do info.update(output_interval = group_data.output_interval)
#%        do info.update(time_reduction = group_data.operation or 'inst')
#%        do info.update(grid_label = 'gn')
#%        do info.update(level_type = levtype_l)
#%        do info.update(grid_id = 'not implemented')
#%        do info.update(format = format_map[extension])
#%        do info.update(output_start = output_start)
#%        do info.update(output_end = output_end)
#%        do info.update(file_interval = file_interval)
#%        do info.update(filename_tmpl = filename_format|replace('<output_filename>', output_filename)|replace('<levtype_l>', levtype_l)+'.'+extension)
#%        do file_infos.append(info)
#%      endif
#%    endfor
#%  endfor

import argparse
import errno
import json
import os
import re
import sys
import xarray

#% include 'standard_experiments/mtime.tmpl'
#% include 'standard_experiments/logging.tmpl'

# Process command line options

def check_date(arg):
    try:
        value = mtime.DateTime(arg)
    except ValueError as ve:
        raise argparse.ArgumentTypeError(ve.message)
    return value

command_line = argparse.ArgumentParser(description=__doc__.split('\n', 1)[0])
command_line.add_argument('start_date', type=check_date, help=
    'first date of period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('next_date', nargs='?', type=check_date, help=
    'date of next period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('-w', '--work-stamp', action='store_true', help=
    'expect time stamps in model format instead of post-processed')
command_line.add_argument('-W', '--work-dir', action='store_true', help=
    'take files from work directory instead of data directory')
command_line.add_argument('-N', '--include-next-date', action='store_true', help=
    'also include files with time stamps for next start date')
command_line.add_argument('-I', '--input-dir', help=
    'take files from a non-standard directory')
command_line.add_argument('-f', '--force', action='store_true', help=
    'only warn about duplicated data instead of exiting with an error')
command_line.add_argument('-V', '--version', action='version', version="""
$Id: DEFAULT.intake.tmpl 0 2021-08-31 12:37:44Z m221078 $
%{VERSIONS_|join('\n')}
""")
args = command_line.parse_args()

os.chdir('%{SCRIPT_DIR}')

# Do time computations using mtime

initial_date = mtime.DateTime('%{INITIAL_DATE}')
start_date = args.start_date
next_date = args.next_date

# if start_date < initial_date:
#     logging.error("start_date is before initial_date")
#     sys.exit(1)

if not next_date:
    interval = mtime.TimeDelta('%{JOB.interval|d(INTERVAL)}')
    next_date = start_date + interval

offset = mtime.TimeDelta('-%{namelists.NAMELIST_atm.run_nml.modeltimestep}')
end_date = next_date + offset

# Determine time stamp format conversion

weed = re.compile(r'[-:]|\.\d+$') # for re-formatting of date/time stamps
def data_stamp(date):
    return re.sub(weed, '', str(date.date))
def run_stamp(date):
    return re.sub(weed, '', str(date))
def work_stamp(date):
    return run_stamp(date) + 'Z'

start_stamp = data_stamp(start_date)
end_stamp = data_stamp(end_date)

stamp = work_stamp if args.work_stamp else data_stamp

# Determine directory to read files from

input_dir = '%{DATA_DIR}'
if args.work_dir:
    input_dir = os.path.join('%{WORK_DIR}', '%{jobs.run.subdir}').format(
        start_stamp=run_stamp(start_date), end_stamp=run_stamp(end_date))

if args.input_dir:
    input_dir = args.input_dir

# Start processing

logging.info('%{JOB.id} process started for %s-%s', start_stamp, end_stamp)

# Define file metadata

file_infos = [
#%  for info in file_infos:
    %{info},
#%  endfor
]

# Read intake-esm catalog

intake_name = '%{JOB.url|d((DATA_ROOT, MODEL_SUBDIR, JOB.id, "index.json")|join("/"))}'

groupby_attributes = (
    "project",
    "institution_id",
    "source_id",
    "experiment_id",
    "simulation_id",
    "realm",
    "frequency",
    "time_reduction",
    "grid_label",
    "level_type",
)
additional_attributes = (
    "time_max",
    "grid_id"
)

assets = "uri"
format_column_name = "format"
variable_column_name = "variable_id"
time_column_name = "time_min"

non_variable_attributes = (groupby_attributes + (time_column_name,) +
    additional_attributes)
attributes = (variable_column_name,) + non_variable_attributes
columns = attributes + (format_column_name, assets)

try:

    with open(intake_name) as intake_file:
        intake_data = json.load(intake_file)

        intake_attributes = tuple(att['column_name'] for att in intake_data['attributes'])
        intake_assets = intake_data['assets']['column_name']

        if sorted(intake_attributes) != sorted(attributes) or intake_assets != assets:
            logging.error("incompatible catalog '{0}'".format(intake_name))
            sys.exit(1)

except (IOError, ValueError) as e:

    logging.warning("cannot load '{0}': {1}".format(intake_name, str(e)))
    logging.warning("creating new catalog")

    intake_data = {
        "esmcat_version": "0.1.0",
        "id": "ICON-ESM",
        "description": "Default catalog for ICON-ESM experiments by MPIMet",
        "attributes": [{"column_name": att} for att in attributes],
        "assets": {
            "column_name": assets,
            "format_column_name": "format"
        },
        "aggregation_control": {
            "variable_column_name": variable_column_name,
            "groupby_attrs": groupby_attributes,
            "aggregations": [
                {
                    "type": "union",
                    "attribute_name": variable_column_name
                },
                {
                    "type": "join_existing",
                    "attribute_name": time_column_name,
                    "options": {
                         "dim": "time",
                         "coords": "minimal",
                         "compat": "override"
                    }
                }
            ]
        },
        "catalog_dict": []
    }

# Create catalog data for this year

def generate_keys(entry):
    for var in entry[variable_column_name]:
        yield ( (var,) +
            tuple(str(entry[att]) for att in non_variable_attributes) )

catalog_dict = {}
catalog_index = {} # Contains an entry for each var name in var list

for entry in intake_data['catalog_dict']:
    # Skip files that no longer exist
    if os.path.isfile(entry[assets]):
        key = tuple(str(entry[att]) for att in attributes)
        catalog_dict[key] = entry
        for key in generate_keys(entry):
            catalog_index[key] = entry
    else:
        logging.warning("unsubscribing non-existing file '%s'", entry[assets])

for info in file_infos:

    output_start = mtime.DateTime(info['output_start'])
    output_end = mtime.DateTime(info['output_end'])
    file_interval = mtime.TimeDelta(info['file_interval'])
    output_interval = mtime.TimeDelta(info['output_interval'])
    neg_output_interval = mtime.TimeDelta('-'+info['output_interval'])

    info['frequency'] = ' '.join('{0}{1}'.format(value, name)
        for (name, value) in output_interval.items() if name != 'sign' and value)


    # Check if current interval is overlapping with file life-time
    if output_start < next_date and output_end >= start_date:

        # Set lower and upper limits for file life-time wrtt current interval
        file_start_date = max(start_date, output_start)
        file_next_date = min(next_date, output_end)

        # Generate output per file date
        date = file_start_date
        while ((args.include_next_date and date <= file_next_date)
               or date < file_next_date):
            # Set current file name
            info[assets] = os.path.join(input_dir,
                info['filename_tmpl'].replace('<datetime2>', stamp(date)))
            logging.info("current file '%s'", info[assets])

            # Determine time range
            if args.include_next_date and date != initial_date:
                if date == file_next_date:
                     info[time_column_name] = str(date)
                     info['time_max'] = str(date)
                else:
                    info[time_column_name] = str(date+output_interval)
                    info['time_max'] = str(date+file_interval+neg_output_interval)
            else:
                info[time_column_name] = str(date)
                info['time_max'] = str(date+file_interval+neg_output_interval)

            # Get variables from file
            try:
                with xarray.open_dataset(info[assets], use_cftime=True,
                    chunks={}) as ds:

                    # Try to remove spurious entries (eg bounds)
                    info[variable_column_name] = [var for var in ds.data_vars
                        if not var.endswith('_bnds')]

                duplicates = tuple(key for key in generate_keys(info)
                    if key in catalog_index
                    and info[assets] != catalog_index[key][assets])
                if duplicates:
                    if args.force:
                        for key in duplicates:
                            logging.warning("key '%s' is already taken by assets"
                            " '%s'", str(key), catalog_index[key][assets])
                    else:
                        for key in duplicates:
                            logging.error("key '%s' is already taken by assets"
                            " '%s'", str(key), catalog_index[key][assets])
                        sys.exit(1)
                else:
                    entry = {col: info[col] for col in columns}
                    key = tuple(str(entry[att]) for att in attributes)
                    catalog_dict[key] = entry
                    for key in generate_keys(entry):
                        catalog_index[key] = entry

            except IOError as x:
                if x.errno != errno.ENOENT: raise
                logging.warning("skipped: "+x.strerror)
                

            date += file_interval

intake_data["catalog_dict"] = list(catalog_dict.values())

# Write intake-esm catalog

with open(intake_name, 'w') as intake_file:
    json.dump(intake_data, intake_file,
              indent=4, separators=(',',': ')) ### , sort_keys=True)
    intake_file.write('\n')

# Finish processing

logging.info('%{JOB.id} process finished for %s-%s', start_stamp, end_stamp)

