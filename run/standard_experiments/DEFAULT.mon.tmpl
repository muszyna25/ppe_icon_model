#! /usr/bin/env python #%# -*- mode: python -*- vi: set ft=python :
#SBATCH --account=%{ACCOUNT}
#%  if JOB.qos is defined:
#SBATCH --qos=%{JOB.qos}
#%  endif
#SBATCH --job-name=%{EXP_ID}.%{JOB.id}
#SBATCH --partition=prepost,shared
#%  if JOB.tasks is defined:
#SBATCH --ntasks=%{JOB.tasks}
#%  endif
#%  if JOB.hardware_threads|default('') is not set:
#SBATCH --ntasks-per-core=1
#%  endif
#SBATCH --output=%{SCRIPT_DIR}/%x.%j.log
#SBATCH --time=%{JOB.time_limit}

'''\
Create monitoring from ICON experiment data for a given period
'''

import argparse
import logging
import os
import subprocess
import sys

sys.path.append(os.path.join('%{MODEL_DIR}', 'lib', 'python'))
import mtime

# Process command line options

def check_date(arg):
    try:
        value = mtime.DateTime(arg)
    except ValueError as ve:
        raise argparse.ArgumentTypeError(ve.message)
    return str(value.date)

command_line = argparse.ArgumentParser(description=__doc__.split('\n', 1)[0])
command_line.add_argument('start_date', type=check_date, help=
    'first date of period (YYYY-MM-DD... or YYYYMMDD...)')
command_line.add_argument('-V', '--version', action='version', version='%{VERSIONS_|join(" ")|trim()}')
command_line.add_argument('-c', '--clean', action='store_true', help=
    'remove output files. '
    'Use ONLY after you made absolutely sure that the raw data still exists!')
args = command_line.parse_args()

# Do time computations using mtime

mtime.setCalendar(mtime.CALENDAR_TYPE.%{calendar_mtime})

initial_date = mtime.DateTime('%{INITIAL_DATE}')
start_date = mtime.DateTime(args.start_date)

if start_date < initial_date:
    sys.stderr.write("Oops: start_date is before initial_date\n")
    sys.exit(1)
    
interval = mtime.TimeDelta('%{INTERVAL}')
next_date = start_date + interval
end_date = next_date + mtime.TimeDelta('-P1D')

# Define required output

tags = ['atm_mon', 'hamocc_monitor', 'oce_mon']

# Set-up template variables

template_dict = {}
template_dict['start_date'] = args.start_date.translate(None, '-')
template_dict['end_date'] = str(end_date.date).translate(None, '-')
template_dict['tags'] = ' '.join(tags)

# Prolog

logging.basicConfig(format='%(asctime)s: %(levelname)s%(message)s',
                    level=logging.INFO)
logging.addLevelName(logging.DEBUG, 'Debug: ')
logging.addLevelName(logging.INFO, '')
logging.addLevelName(logging.WARNING, 'Hey: ')
logging.addLevelName(logging.ERROR, 'Oops: ')
logging.addLevelName(logging.CRITICAL, 'Sorry: ')

logging.info('monitoring started '
             'for {start_date}-{end_date}'.format(**template_dict))

# Set-up directory structure

targets_dir = os.path.join('%{MON_DIR}', 'targets')
if not os.path.isdir(targets_dir): os.makedirs(targets_dir)
template_dict['targets_dir'] = targets_dir

work_dir = os.path.join('%{WORK_DIR}',
                        '%{JOB.id}_{start_date}-{end_date}'.format(**template_dict))
if not os.path.isdir(work_dir): os.makedirs(work_dir)
os.chdir(work_dir)

logging.info('working directory is {0}'.format(work_dir))

# Generate Makefile

makefile_template = '''\
CDO = cdo
CDOFLAGS = -O
PLOT_TIMESER = plot_timeser

EXP_ID = %{EXP_ID}
DATA_DIR = %{DATA_DIR}
MON_DIR = %{MON_DIR}

START_DATE = {start_date}
TAGS = {tags}
TARGETS_DIR = {targets_dir}

TARGETS = $(TAGS:%=$(TARGETS_DIR)/$(EXP_ID)_%_$(START_DATE).pdf)

all: $(TARGETS)

clean:
	$(RM) $(TARGETS) $(TARGETS:.pdf=.nc)

.PRECIOUS: $(TARGETS_DIR)/$(EXP_ID)_%_$(START_DATE).nc

$(TARGETS_DIR)/$(EXP_ID)_%_$(START_DATE).nc: $(DATA_DIR)/$(EXP_ID)_%_$(START_DATE).nc
	[ -f $(MON_DIR)/$(EXP_ID)_$*.nc ] || cp $< $(MON_DIR)/$(EXP_ID)_$*.nc
	$(CDO) $(CDOFLAGS) mergetime $< $(MON_DIR)/$(EXP_ID)_$*.nc $(EXP_ID)_$*.nc
	mv $(EXP_ID)_$*.nc $(MON_DIR)/$(EXP_ID)_$*.nc
	touch $@

$(TARGETS_DIR)/$(EXP_ID)_%_$(START_DATE).pdf: $(TARGETS_DIR)/$(EXP_ID)_%_$(START_DATE).nc
	$(PLOT_TIMESER) --manifest=$(EXP_ID)_$*.lst --mode=monitoring --with1=$(MON_DIR)/$(EXP_ID)_$*.nc --output=$(MON_DIR)/$(EXP_ID)_$*
	touch $@

'''

makefile_name = 'Makefile'
makefile = open(makefile_name, mode='w')
makefile.write(makefile_template.format(**template_dict))
makefile.close()

# Run the actual make process

# Make sure that mergetime does not create duplicate data when run again
os.putenv('SKIP_SAME_TIME', '1')
os.putenv('SKIPSAMETIME', '1') # workaround for bug in cdo-1.9.6

make_args = ['make', '-k', '-j', '%{JOB.tasks|d(1)}']
if args.clean: make_args.append('clean')
make = subprocess.Popen(make_args,
                        stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
for line in make.stdout:
    logging.info(line.rstrip('\n\r'))

make_result = make.wait()
if make_result:
    logging.error("'{0}' returned {1}".format(' '.join(make_args), make_result))
    sys.exit(1)

os.remove(makefile_name)
os.chdir('%{SCRIPT_DIR}')
os.removedirs(work_dir)

# Epilog

logging.info('monitoring finished '
             'for {start_date}-{end_date}'.format(**template_dict))

#% for job in JOB['.trigger']|list:
subprocess.check_call(['%{JOB.batch_command}', '%{EXP_ID}.%{job}', args.start_date])
#% endfor
