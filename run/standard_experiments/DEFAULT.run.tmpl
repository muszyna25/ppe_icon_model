#! %{JOB.ksh|default('/bin/ksh')} #%# -*- mode: sh -*- vi: set ft=sh :
#
# %{EXP_ID}.%{JOB.id}
#
# %{mkexp_input}
#
# $Id: DEFAULT.run.tmpl 11 2021-06-25 06:53:19Z m221078 $
#
# %{VERSIONS_|join('\n# ')}
#
#%from 'standard_experiments/format.tmpl' import format_files
#=============================================================================

# mistral cpu batch job parameters
# --------------------------------
#SBATCH --account=%{ACCOUNT}
#% if JOB.qos is defined:
#SBATCH --qos=%{JOB.qos}
#% endif
#SBATCH --job-name=%{EXP_ID}.run
#SBATCH --partition=%{JOB.partition|d('compute2,compute')|join(',')}
#SBATCH --nodes=%{JOB.nodes}
#SBATCH --threads-per-core=2
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=%{SCRIPT_DIR}/%{EXP_ID}.run.%j.log
#SBATCH --error=%{SCRIPT_DIR}/%{EXP_ID}.run.%j.log
#SBATCH --exclusive
#SBATCH --time=%{JOB.time_limit}

#=============================================================================
set -e
#%  if JOB.debug_level|d(0)|int > 1:
set -x
#%  endif

# Utilities
PATH=%{MODEL_DIR}/utils/slurm:%{MODEL_DIR}/utils:$PATH

# Support log style output
pipe=%{EXP_ID}_%{JOB.id}_$$.pipe
mkfifo $pipe
trap "cd $PWD && rm -f $pipe" EXIT
awk '{print strftime("%FT%T:"), $0; fflush()}' $pipe &
exec > $pipe 2>&1

#% do VARIABLES_.add('MODEL_DIR')
#% do VARIABLES_.add('SCRIPT_DIR')
#% do VARIABLES_.add('BUILD_DIR')
#% for var in VARIABLES_|sort:
#%   if context(var):
%{var}=%{context(var)}
#%   endif
#% endfor
#%#
# Make sure that 'add_run_routines' picks up the right base directory
cd $BUILD_DIR/run
. ./add_run_routines

#=============================================================================
#
# OpenMP environment variables
# ----------------------------
export OMP_NUM_THREADS=%{JOB.threads_per_task|d(1)}
export ICON_THREADS=$OMP_NUM_THREADS
#
# MPI variables
# -------------
no_of_nodes=${SLURM_JOB_NUM_NODES:=%{JOB.nodes|d(1)}}
mpi_procs_pernode=$((${SLURM_JOB_CPUS_PER_NODE%%\(*}%{'' if JOB.hardware_threads|d() else '/ 2'} / OMP_NUM_THREADS))
((mpi_total_procs=no_of_nodes * mpi_procs_pernode))
#
#=============================================================================

# load local setting, if existing
# -------------------------------
if [ -a ../setting ]
then
  echo "Load Setting"
  . ../setting
fi

# environment variables for the experiment and the target system
# --------------------------------------------------------------
export EXPNAME=%{EXP_ID}

#%  for var in JOB.env:
#%    set val = JOB.env[var]
#%    if val:
export %{var}=%{'"'+val+'"' if ' ' in val else val}
#%    else:
unset %{var}
#%    endif
#%  endfor

# how to start the icon model
# ---------------------------
#%#
#%  set mpi_command = MPI.command
#%#
#%  if JOB.use_cpu_mask is set:
#%#   TODO: add regexp replace to mkexp and use here
#%-   if mpi_command|match('\s+--cpu-bind=\S*'):
#%      set mpi_command_1 = mpi_command|match('^(.*?)\s+--cpu-bind=\S*.*$')
#%      set mpi_command_2 = mpi_command|match('^.*?\s+--cpu-bind=\S*(.*)$')
#%      set mpi_command = mpi_command_1 + mpi_command_2
#%    endif
#%    set mpi_command = mpi_command + ' --cpu-bind=v,mask_cpu=$(ccpumask -t $OMP_NUM_THREADS)'
#%  endif
#%#
#%  if JOB.use_hostfile is set:
#%#   TODO: add regexp replace to mkexp and use here
#%    if mpi_command|match('\s+--distribution=\S*'):
#%      set mpi_command_1 = mpi_command|match('^(.*?)\s+--distribution=\S*.*$')
#%      set mpi_command_2 = mpi_command|match('^.*?\s+--distribution=\S*(.*)$')
#%      set mpi_command = mpi_command_1 + mpi_command_2
#%    endif
#%    set mpi_command = mpi_command + ' --distribution=arbitrary'
#%  endif

export START="%{mpi_command} --nodes=$no_of_nodes --ntasks=$((no_of_nodes * mpi_procs_pernode)) --ntasks-per-node=${mpi_procs_pernode} --cpus-per-task=$((2 * OMP_NUM_THREADS))"
export MODEL="%{BIN_DIR}/%{MODEL_EXE}"

# how to submit the next job
# --------------------------
job_name="%{EXP_ID}.run"

# cdo for post-processing
# -----------------------
cdo="cdo"
cdo_diff="cdo diffn"

#=============================================================================

ulimit -s %{JOB.stack_size}
ulimit -c 0

# ----------------------------------------------------------------------------
# %{EXP_DESCRIPTION|split("\n")|join("\n# ")|replace("# \n", "#\n")}
# ----------------------------------------------------------------------------

# (0) Basic model configuration
# -----------------------------

#%  if JOB.share_nodes|d():
oce_per_node=%{JOB.ocean_tasks_per_node}
atm_per_node=$((mpi_procs_pernode - oce_per_node))
oce_procs=$((no_of_nodes * oce_per_node))
mpi_oce_procs=$oce_procs
atm_procs=$((no_of_nodes * atm_per_node))
#%  else:
mpi_oce_nodes=%{JOB.ocean_nodes}
((mpi_oce_procs=mpi_oce_nodes * mpi_procs_pernode))
#%  endif
#
#--------------------------------------------------------------------------------------------------

# (4) Set variables to configure the experiment:
# ----------------------------------------------

# start and end date+time of experiment
# -------------------------------------
initial_date="%{INITIAL_DATE}"
final_date="%{FINAL_DATE}"

# restart/checkpoint/output intervals
# -----------------
restart_interval="%{namelists['icon_master.namelist'].master_time_control_nml.restarttimeintval}"

# Read and compute time control information

start_date_file=$SCRIPT_DIR/$EXPNAME.date
exp_log_file=$SCRIPT_DIR/$EXPNAME.log

#%  if JOB.entry_point is set:
if [[ -f $start_date_file ]]
then
    exec >&2
    echo "Oops: you have tried to begin an experiment that had already been started."
    echo "      Remove '$start_date_file' if you want to continue"
    exit 1
fi
start_date=$initial_date
rm -f $exp_log_file
#%  else:
read start_date < $start_date_file
#%  endif
#%#
code=$(python -c "
#%  if PREFIX:
import sys
from distutils.sysconfig import get_python_lib
sys.path.insert(1, get_python_lib(prefix='%{PREFIX}'))
#%  else
import os
os.chdir('$BUILD_DIR/externals/mtime/src')
#%  endif
import mtime
mtime.setCalendar(mtime.CALENDAR_TYPE.%{calendar_mtime})
initial_date = mtime.DateTime('$initial_date')
final_date = mtime.DateTime('$final_date')
start_date = mtime.DateTime('$start_date')
reference_date = initial_date + mtime.TimeDelta('-$restart_interval')
next_date = start_date + mtime.TimeDelta('$restart_interval')
if next_date > final_date:
    logging.warn('next date after final date; setting next date to final date')
    next_date = final_date
end_date = next_date + mtime.TimeDelta('-%{ATMO_TIME_STEP}')
atmo_reference_jstep = (initial_date, reference_date)//mtime.TimeDelta('%{ATMO_TIME_STEP}')
ocean_reference_jstep = (initial_date, reference_date)//mtime.TimeDelta('%{OCEAN_TIME_STEP}')

print('initial_date=' + str(initial_date))
print('final_date=' + str(final_date))
print('start_date=' + str(start_date))
print('reference_date=' + str(reference_date))
print('end_date=' + str(end_date))
print('next_date=' + str(next_date))
print('atmo_reference_jstep=' + str(atmo_reference_jstep))
print('ocean_reference_jstep=' + str(ocean_reference_jstep))
")
eval "$code"

# Dates used as timestamps
start_stamp=${start_date%.*}
start_stamp=${start_stamp//[-:]/}
end_stamp=${end_date%.*}
end_stamp=${end_stamp//[-:]/}
next_stamp=${next_date%.*}
next_stamp=${next_stamp//[-:]/}
y0=${start_date%%-*}
yN=${end_date%%-*}

# Mark current run as started in log
echo $(date -u +'%Y-%m-%dT%H:%M:%SZ') ${start_date%:*} ${end_date%:*} ${%{JOB.id_environ}} start >> $exp_log_file

#------------------------------------------------------------------------------

# (5) Define the model configuration
#-----------------------------------

# Write namelist files directly to working directory, create this if missing

EXPDIR=%{WORK_DIR}/%{JOB.subdir}

#%  if JOB.subdir|d(''):
if [[ -d $EXPDIR ]]
then
    echo "$(date +'%Y-%m-%dT%H:%M:%S'): removing run dir '$EXPDIR'"
    rm -fvr $EXPDIR
fi

#%    endif
mkdir -vp $EXPDIR
cd $EXPDIR

#------------------------------------------------------------------------------
# I. coupling section
#------------------------------------------------------------------------------

if [ $mpi_total_procs -lt 2 ] ; then
  check_error 0 "This setup requires at least 2 mpi processes. Exit"
fi

# I.1 Split the number of total procs and assign to each component
# ----------------------------------------------------------------
oce_min_rank=`expr ${mpi_total_procs} - ${mpi_oce_procs}`
oce_max_rank=`expr ${oce_min_rank} + ${mpi_oce_procs} - 1`
oce_inc_rank=1
atm_min_rank=0
atm_max_rank=`expr ${oce_min_rank} - 1`
atm_inc_rank=1

#
# create ICON master, coupling and model namelists
# ------------------------------------------------
# For a complete list see Namelist_overview and Namelist_overview.pdf
#

cat > icon_master.namelist << EOF
%{format_namelist(namelists['icon_master.namelist'])}
EOF

#%  if WITH_ATMO is set and WITH_OCEAN is set:
# I.3 YAC coupling library configuration
#-----------------------------------------------------------------------------
# component names in coupling.xml must (!) match with modelname_list[*]
cat > coupling.xml << EOF
%{COUPLING_XML}
EOF

#%  endif
#%  if WITH_ATMO is set or WITH_LAND is set:
#-----------------------------------------------------------------------------
# II. ATMOSPHERE and LAND
#-----------------------------------------------------------------------------
#
#%  endif
#%  if WITH_ATMO is set:
# atmosphere namelist
# -------------------
cat > NAMELIST_atm << EOF
%{format_namelist(namelists.NAMELIST_atm)}
EOF

#%  endif
#%  if WITH_LAND is set:
# jsbach namelist
# ---------------

cat > NAMELIST_lnd << EOF
%{format_namelist(namelists.NAMELIST_lnd)}
EOF

#%  endif
#-----------------------------------------------------------------------------
# III. OCEAN and SEA-ICE (and HAMOCC) 
#-----------------------------------------------------------------------------
#
# ocean namelist
# --------------

cat > NAMELIST_oce << EOF
%{format_namelist(namelists.NAMELIST_oce)}
EOF

# Selectively add disturbance to overcome model failure
# ------------------------------------------------

#% set (file, group, setting, delta) = DISTURBANCE_SETTINGS|split(' ')
#% set value = namelists[file][group][setting]|float + delta|float
if [[ " %{DISTURBANCE_YEARS|join(" ")} " == *" ${y0} "* ]]
then   
    print "disturbing model in year ${y0}, setting rayleigh_coeff to %{value}"
    patch_namelist %{group} %{setting} %{value} %{file}
fi

#-----------------------------------------------------------------------------

if [ $mpi_total_procs -lt `expr $mpi_oce_procs + 1` ] ; then
   echo "Too few mpi_total_procs for requested mpi_oce_procs."
   echo "-> check mpi_total_procs and mpi_oce_procs. Exiting."
   check_error 0
   exit
fi

#=============================================================================
#
# This section of the run script prepares and starts the model integration. 
#
#-----------------------------------------------------------------------------
# Reset files and file names for check_error and check_final_status
final_status_file=${SCRIPT_DIR}/${job_name}.final_status
current_status_file=${SCRIPT_DIR}/${job_name}.status
rm -f ${final_status_file} ${current_status_file}

#-----------------------------------------------------------------------------
# Provide input files
%{format_files(files, 'yr', 'y0', 'yN')}

#-----------------------------------------------------------------------------
#  get model
#
ls -l ${MODEL}
check_error $? "${MODEL} does not exist?"
#
ldd ${MODEL}
#
#-----------------------------------------------------------------------------
#
# start experiment
#
#%  if JOB.use_hostfile is set or JOB.use_mpmd is set:
mpi_atm_io_procs=%{namelists.NAMELIST_atm.parallel_nml.num_io_procs}
mpi_oce_io_procs=%{namelists.NAMELIST_oce.parallel_nml.num_io_procs}

#%  endif
#%  if JOB.use_hostfile is set:
export SLURM_HOSTFILE=$EXPDIR/hst_file

function fill_hostfile(){
   total=$1
   nio=$2
   direction=$3

   block=8

   if [ $direction -eq 0 ] ; then
     frst_block=0
     last_block=$(( ($total/$no_of_nodes) % $block ))
   else
     last_block=0
     frst_block=$(( ($total/$no_of_nodes) % $block ))
   fi

   iohsts=""
   left=$total
   while [ $left -gt $nio ] ; do
     for hst in $(nodeset -e $SLURM_NODELIST); do
       myblock=$block
       if [ $left -le $(($last_block*$no_of_nodes)) ] ; then
         myblock=$last_block
       else
         if [ $left -gt $(($total - $frst_block*$no_of_nodes)) ] ; then
           myblock=$frst_block
         fi
       fi
       if [ $left -le $(($myblock*$nio)) ] ; then
         myblock=$(($myblock - 1))
         iohsts="$iohsts $hst"
       fi
       for ib in $(seq 1 $myblock); do
         echo "$hst"
         left=$(($left - 1))
       done
     done
   done
   for hst in $iohsts; do
     echo "$hst"
   done
}

fill_hostfile $atm_procs $mpi_atm_io_procs 0 > $SLURM_HOSTFILE
fill_hostfile $oce_procs $mpi_oce_io_procs 1 >> $SLURM_HOSTFILE

#%  endif
cp ${MODEL} .
export MODEL=./icon

rm -f finish.status

#%  if JOB.use_mpmd is set:
if [ $mpi_atm_io_procs -eq 0 ] ; then
  atm_io_line="0 numactl --interleave=0-3 -- ${MODEL}"
else
  if [ $((atm_per_node*2)) -lt  $mpi_procs_pernode ] ; then
    atm_io_line="$((atm_procs -1 - mpi_atm_io_procs))-$((atm_procs -1)) numactl --interleave=0-3 -- ${MODEL}"
  else
    atm_io_line="$((atm_procs -1 - mpi_atm_io_procs))-$((atm_procs -1)) numactl --interleave=4-7 -- ${MODEL}"
  fi
fi
if [ $mpi_oce_io_procs -eq 0 ] ; then
  if [ $((atm_per_node*2)) -lt  $mpi_procs_pernode ] ; then
    oce_io_line="$atm_procs numactl --interleave=0-3 -- ${MODEL}"
  else
    oce_io_line="$atm_procs numactl --interleave=4-7 -- ${MODEL}"
  fi
else
  oce_io_line="$((atm_procs + oce_procs -1 - mpi_oce_io_procs))-$((atm_procs + oce_procs -1)) numactl --interleave=4-7 -- ${MODEL}"
fi
cat > icon_mpmd.conf <<EOF
$atm_io_line
$oce_io_line
* numactl --localalloc -- ${MODEL}
EOF

#%  endif
#%  if JOB.debug_level|d(0)|int > 0:
thisscript=$(realpath $0)
echo "#################################################"
echo "###################  script  ####################"
echo "#################################################"
cat $thisscript
cp -p $thisscript .
echo "#################################################"
echo "#################################################"
echo "####################  env  ######################"
echo "#################################################"
rm -f env.txt
env 2>&1 | tee env.txt
echo "#################################################"
echo "#################################################"
echo "####################  ldd  ######################"
echo "#################################################"
rm -f ldd.txt
echo $(realpath ${MODEL}) 2>&1 | tee ldd.txt
srun -N1 -n1 -- ldd -v $(realpath ${MODEL}) 2>&1 | tee ldd.txt
echo "#################################################"

#%  endif
date
#%  if JOB.use_mpmd is set:
${START} --multi-prog icon_mpmd.conf
#%  else:
${START} ${MODEL}
#%  endif
date

if [ -r finish.status ] ; then
  check_final_status 0 "${START} ${MODEL}"
else
  check_final_status -1 "${START} ${MODEL}"
fi

#% set atmo_restart = namelists.NAMELIST_atm.run_nml.restart_filename
#% set ocean_restart = namelists.NAMELIST_oce.run_nml.restart_filename

#%  if JOB.subdir|d(''):
# Handle restart hand-over

#%    if WITH_ATMO is set:
cp -lrfv %{atmo_restart|replace('<rsttime>', '${next_stamp}Z')} %{WORK_DIR}
#%    endif
#%    if WITH_OCEAN is set:
cp -lrfv %{ocean_restart|replace('<rsttime>', '${next_stamp}Z')} %{WORK_DIR}
#%    endif

#%  endif

# Update date info for next run

echo $next_date > $start_date_file

#%  if JOB.subdir|d(''):
# Clean up our restart

#%    if WITH_ATMO is set:
rm -rfv %{atmo_restart|replace('<rsttime>', '${start_stamp}Z')}
#%    endif
#%    if WITH_OCEAN is set:
rm -rfv %{ocean_restart|replace('<rsttime>', '${start_stamp}Z')}
#%    endif

#%  endif

#
#-----------------------------------------------------------------------------
#
finish_status=`cat finish.status`
echo $finish_status
echo "============================"
echo "Script run successfully: $finish_status"
echo "============================"

#%  if WITH_OBGC is set:
#-----------------------------------------------------------------------------
# store HAMOCC log file
mv bgcout bgcout_${start_date}
#%  endif

#-----------------------------------------------------------------------------

# Mark current run as successful in log
echo $(date -u +'%Y-%m-%dT%H:%M:%SZ') ${start_date%:*} ${end_date%:*} ${%{JOB.id_environ}}  end >> $exp_log_file

cd $SCRIPT_DIR
set +e
status=0

#% for trigger in JOB['.trigger']|list:
#%   set job = (trigger|split|list)[0]
#%   set type = jobs[job]['.type']
#%   if type == 'restart':
if [ $finish_status = "RESTART" ]
then
  (
#%     if JOB.use_hostfile is set:
    # Clean SLURM_HOSTFILE
    unset SLURM_HOSTFILE
#%     endif
    echo "restart: submitting '%{EXP_ID}.%{trigger}'"
    %{JOB.batch_command} %{EXP_ID}.%{trigger}
  ) || status=$?
fi
#%   elif type == 'serial':
echo "running '%{EXP_ID}.%{trigger}'"
./%{EXP_ID}.%{trigger} $start_date || status=$?
#%   else:
echo "submitting '%{EXP_ID}.%{trigger}'"
(
  # Clean SLURM environment
  eval $(python -c 'import os
for key in os.environ:
    if key.startswith("SLURM_"): print("unset "+key)')
  %{JOB.batch_command} %{EXP_ID}.%{trigger} $start_date
) || status=$?
#%   endif
#% endfor

exit $status

#-----------------------------------------------------------------------------
# vim:ft=sh
#-----------------------------------------------------------------------------
