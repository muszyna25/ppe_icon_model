#! %{JOB.ksh|default('/bin/ksh')} #%# -*- mode: sh -*- vi: set ft=sh :
#
# %{EXP_ID}.%{JOB.id}
#
# %{mkexp_input}
#
# $Id: DEFAULT.run.tmpl 3 2020-10-05 08:51:25Z m221078 $
#
# %{VERSIONS_|join('\n# ')}
#
#%from 'standard_experiments/format.tmpl' import format_files
#=============================================================================

# mistral cpu batch job parameters
# --------------------------------
#SBATCH --account=%{ACCOUNT}
#% if JOB.qos is defined:
#SBATCH --qos=%{JOB.qos}
#% endif
#SBATCH --job-name=%{EXP_ID}.run
#SBATCH --partition=%{JOB.partition|d('compute2,compute')|join(',')}
#SBATCH --nodes=%{JOB.nodes}
#SBATCH --threads-per-core=2
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=%{SCRIPT_DIR}/%{EXP_ID}.run.%j.log
#SBATCH --error=%{SCRIPT_DIR}/%{EXP_ID}.run.%j.log
#SBATCH --exclusive
#SBATCH --time=%{JOB.time_limit}

#=============================================================================
set -ex
#=============================================================================
#
# ICON run script:
# !ATTENTION! Do not change the format of the following lines.
#             They are evaluated by checksuite scripts.
# created by ./run/make_target_runscript
# target machine is %{use_target}
# target use_compiler is %{use_compiler}
# with_mpi=%{use_mpi}
# with_openmp=%{use_openmp}
# memory_model=large
# submit with %{use_submit}
#
#=============================================================================

#% do VARIABLES_.add('MODEL_DIR')
#% do VARIABLES_.add('SCRIPT_DIR')
#% do VARIABLES_.add('BUILD_DIR')
#% for var in VARIABLES_|sort:
#%   if context(var):
%{var}=%{context(var)}
#%   endif
#% endfor
#%#
# Make sure that 'add_run_routines' picks up the right base directory
cd $BUILD_DIR/run
. ./add_run_routines

#=============================================================================
#
# OpenMP environment variables
# ----------------------------
export OMP_NUM_THREADS=1
export ICON_THREADS=1
export OMP_SCHEDULE=dynamic,1
export OMP_DYNAMIC="false"
export OMP_STACKSIZE=200M
#
# MPI variables
# -------------
no_of_nodes=${SLURM_JOB_NUM_NODES:=%{JOB.nodes}}
mpi_procs_pernode=$((${SLURM_JOB_CPUS_PER_NODE%%\(*} / 2))
((mpi_total_procs=no_of_nodes * mpi_procs_pernode))
#
#=============================================================================

# load local setting, if existing
# -------------------------------
if [ -a ../setting ]
then
  echo "Load Setting"
  . ../setting
fi

# environment variables for the experiment and the target system
# --------------------------------------------------------------
export EXPNAME="%{EXP_ID}"
export KMP_AFFINITY="verbose,granularity=core,compact,1,1"
export KMP_LIBRARY="turnaround"
export KMP_KMP_SETTINGS="1"
export OMP_WAIT_POLICY="active"
export OMPI_MCA_pml="cm"
export OMPI_MCA_mtl="mxm"
export OMPI_MCA_coll="^fca"
export MXM_RDMA_PORTS="mlx5_0:1"
export HCOLL_MAIN_IB="mlx5_0:1"
export HCOLL_ML_DISABLE_BARRIER="1"
export HCOLL_ML_DISABLE_IBARRIER="1"
export HCOLL_ML_DISABLE_BCAST="1"
export HCOLL_ENABLE_MCAST_ALL="1"
export HCOLL_ENABLE_MCAST="1"
export OMPI_MCA_coll_sync_barrier_after_alltoallv="1"
export OMPI_MCA_coll_sync_barrier_after_alltoallw="1"
export MXM_HANDLE_ERRORS="bt"
export UCX_HANDLE_ERRORS="bt"
export MALLOC_TRIM_THRESHOLD_="-1"

# directories with absolute paths
# -------------------------------
icon_data_rootFolder="/pool/data/ICON"

# how to start the icon model
# ---------------------------
export START="srun --cpu-freq=HighM1 --kill-on-bad-exit=1 --nodes=${SLURM_JOB_NUM_NODES:-1} --cpu_bind=verbose,cores --distribution=block:block --ntasks=$((no_of_nodes * mpi_procs_pernode)) --ntasks-per-node=${mpi_procs_pernode} --cpus-per-task=$((2 * OMP_NUM_THREADS)) --propagate=STACK,CORE"
export MODEL="%{BIN_DIR}/%{MODEL_EXE}"

# how to submit the next job
# --------------------------
submit="%{use_submit}"
job_name="%{EXP_ID}.run"

# cdo for post-processing
# -----------------------
cdo="cdo"
cdo_diff="cdo diffn"

#=============================================================================

ulimit -s 2097152
ulimit -c 0

# ----------------------------------------------------------------------------
# %{EXP_DESCRIPTION|split("\n")|join("\n# ")|replace("# \n", "#\n")}
# ----------------------------------------------------------------------------

# (0) Basic model configuration
# -----------------------------

read_restart_namelists=".false."

# restart=.true.    # deactivate semaphore mechanism
#
# 1 ocean node is sufficient for low resolution, 9 nodes for coupled is suitable.
# faster: 2 nodes for ocean, 16 nodes in total
#         running with Hamocc: 8 nodes for ocean, 22 in total for similar performance
mpi_oce_nodes=%{JOB.ocean_nodes}
#mpi_oce_nodes=${mpi_oce_nodes:=((no_of_nodes/2))}   # default: half of requested nodes
((mpi_oce_procs=mpi_oce_nodes * mpi_procs_pernode))
#
#--------------------------------------------------------------------------------------------------

# (2) unset some setting of create_target_header for mistral

unset OMPI_MCA_coll_fca_enable
unset OMPI_MCA_coll_fca_priority

#--------------------------------------------------------------------------------------------------

# (4) Set variables to configure the experiment:
# ----------------------------------------------

# start and end date+time of experiment
# -------------------------------------
initial_date=${initial_date:="%{INITIAL_DATE}T00:00:00.000"}
final_date=${final_date:="%{FINAL_DATE}T00:00:00.000"}

# restart/checkpoint/output intervals
# -----------------
restart_interval="%{INTERVAL}"
checkpoint_interval="%{INTERVAL}"

# file interval is restart-interval
atm_file_interval="%{INTERVAL}T1S"
oce_file_interval="%{INTERVAL}T1S"
lnd_file_interval="%{INTERVAL}T1S"
#%  if WITH_OBGC is set:
hamocc_file_interval="%{INTERVAL}T1S"
#%  endif
#%#
atm_output_interval="%{OUTPUT_INTERVAL}"
oce_output_interval="%{OUTPUT_INTERVAL}"
lnd_output_interval="%{OUTPUT_INTERVAL}"
#%  if WITH_OBGC is set:
hamocc_output_interval="%{OUTPUT_INTERVAL}"
#%  endif
#%#
# Read and compute time control information

start_date_file=$SCRIPT_DIR/$EXPNAME.date
exp_log_file=$SCRIPT_DIR/$EXPNAME.log

#%  if JOB.entry_point is set:
if [[ -f $start_date_file ]]
then
    exec >&2
    echo "Oops: you have tried to begin an experiment that had already been started."
    echo "      Remove '$start_date_file' and all files (not directories!) in"
    echo "      '%{WORK_DIR}' if you want to continue"
    exit 1
fi
start_date=$initial_date
rm -f $exp_log_file
#%  else:
read start_date < $start_date_file
#%  endif

export PYTHONPATH=$MODEL_DIR/lib/python${PYTHONPATH:+:}$PYTHONPATH
eval $(python -c "
import mtime
mtime.setCalendar(mtime.CALENDAR_TYPE.%{calendar_mtime})
reference_date = mtime.DateTime('$initial_date') + mtime.TimeDelta('-$restart_interval')
next_date = mtime.DateTime('$start_date') + mtime.TimeDelta('$restart_interval')
end_date = next_date + mtime.TimeDelta('-%{ATMO_TIME_STEP}')

print('reference_date=' + str(reference_date))
print('end_date=' + str(end_date))
print('next_date=' + str(next_date))
")

# Mark current run as started in log
echo $(date -u +'%Y-%m-%dT%H:%M:%SZ') ${start_date%:*} ${end_date%:*} ${%{JOB.id_environ}} start >> $exp_log_file

# asynchronous diagnostic output processes
# ----------------------------------------

# Note that "mpi_atm_io_procs" must match the number of output files
mpi_atm_io_procs=0      # >0 for atmosphere plus land (not working for monitoring)
mpi_oce_io_procs=0      # >0 for ocean is not working yet

# output file selection
# ---------------------
#
# output_<xyz>=yes : yes --> output files for <xyz>, any other value --> no files for <xyz>

# output interval for large atm/oce files, normally set to general atmos interval:
atm_output_interval_3d="$atm_output_interval"
atm_output_interval_2d="$atm_output_interval"
oce_output_interval_def="%{SLOW_OUTPUT_INTERVAL}"  # annual mean output for deep ocean

#--------------------------------------------------------------------------------------------------

# (5) Define the model configuration
#-----------------------------------

# JSBACH settings
run_jsbach=yes

# Some further processing for land configuration
# ----------------------------------------------

ljsbach=$([ "${run_jsbach:=no}" == yes ]          && echo .TRUE. || echo .FALSE. )

#-------------------------------------------------------------------------------

# Write namelist files directly to working directory, create this if missing

EXPDIR=%{WORK_DIR}

mkdir -vp $EXPDIR
cd $EXPDIR

# namelist files
# --------------
atm_namelist=NAMELIST_atm
lnd_namelist=NAMELIST_lnd
oce_namelist=NAMELIST_oce

#--------------------------------------------------------------------------------------------------
# I. coupling section
#--------------------------------------------------------------------------------------------------

if [ $mpi_total_procs -lt 2 ] ; then
  check_error 0 "This setup requires at least 2 mpi processes. Exit"
fi

# I.1 Split the number of total procs and assign to each component
# ----------------------------------------------------------------
oce_min_rank=`expr ${mpi_total_procs} - ${mpi_oce_procs}`
oce_max_rank=`expr ${oce_min_rank} + ${mpi_oce_procs} - 1`
oce_inc_rank=1
atm_min_rank=0
atm_max_rank=`expr ${oce_min_rank} - 1`
atm_inc_rank=1
#
# I.2 Fill model list
# -------------------
#
namelist_list[0]="$atm_namelist"
modelname_list[0]="atmo"
modeltype_list[0]=1
minrank_list[0]=$atm_min_rank
maxrank_list[0]=$atm_max_rank
incrank_list[0]=$atm_inc_rank
#
namelist_list[1]="$oce_namelist"
modelname_list[1]="ocean"
modeltype_list[1]=2
minrank_list[1]=$oce_min_rank
maxrank_list[1]=$oce_max_rank
incrank_list[1]=$oce_inc_rank


# I.3 YAC coupling library configuration
#-----------------------------------------------------------------------------
# component names in coupling.xml must (!) match with modelname_list[*]
cat > coupling.xml << EOF
%{COUPLING_XML}
EOF

#-----------------------------------------------------------------------------
# II. ATMOSPHERE and LAND
#-----------------------------------------------------------------------------
#
# atmosphere namelist
# -------------------
cat > ${atm_namelist} << EOF
%{format_namelist(namelists.NAMELIST_atm)}
EOF

# jsbach namelist
# ---------------

cat > ${lnd_namelist} << EOF
%{format_namelist(namelists.NAMELIST_lnd)}
EOF

#-----------------------------------------------------------------------------
# III. OCEAN and SEA-ICE (and HAMOCC) 
#-----------------------------------------------------------------------------

# ----------------------------------------------------------------------------
#
# ocean namelist
# --------------

cat > ${oce_namelist} << EOF
%{format_namelist(namelists.NAMELIST_oce)}
EOF

#-----------------------------------------------------------------------------

if [ $mpi_total_procs -lt `expr $mpi_oce_procs + 1` ] ; then
   echo "Too few mpi_total_procs for requested mpi_oce_procs."
   echo "-> check mpi_total_procs and mpi_oce_procs. Exiting."
   check_error 0
   exit
fi

#-----------------------------------------------------------------------------


#!/bin/ksh
#=============================================================================
#
# This section of the run script prepares and starts the model integration. 
#
# MODEL and START must be defined as environment variables or
# they must be substituted with appropriate values.
#
# Marco Giorgetta, MPI-M, 2010-04-21
#
#-----------------------------------------------------------------------------
#
# directories definition
#
if [ x$grids_folder = x ] ; then
   HGRIDDIR=${MODEL_DIR}/grids
else
   HGRIDDIR=$grids_folder
fi

#-----------------------------------------------------------------------------
# Reset files and file names for check_error and check_final_status
final_status_file=${SCRIPT_DIR}/${job_name}.final_status
current_status_file=${SCRIPT_DIR}/${job_name}.status
rm -f ${final_status_file} ${current_status_file}

#-----------------------------------------------------------------------------
# set up the model lists if they do not exist
# this works for single model runs
# for coupled runs the lists should be declared explicilty
if [ x$namelist_list = x ]; then
#  minrank_list=(        0           )
#  maxrank_list=(     65535          )
#  incrank_list=(        1           )
  minrank_list[0]=0
  maxrank_list[0]=65535
  incrank_list[0]=1
  if [ x$atmo_namelist != x ]; then
    # this is the atmo model
    namelist_list[0]="$atmo_namelist"
    modelname_list[0]="atmo"
    modeltype_list[0]=1
    run_atmo="true"
  elif [ x$ocean_namelist != x ]; then
    # this is the ocean model
    namelist_list[0]="$ocean_namelist"
    modelname_list[0]="oce"
    modeltype_list[0]=2
  elif [ x$psrad_namelist != x ]; then
    # this is the psrad model
    namelist_list[0]="$psrad_namelist"
    modelname_list[0]="psrad"
    modeltype_list[0]=3
  elif [ x$hamocc_namelist != x ]; then
    # this is the hamocc model
    namelist_list[0]="$hamocc_namelist"
    modelname_list[0]="hamocc"
    modeltype_list[0]=4
  elif [ x$testbed_namelist != x ]; then
    # this is the testbed model
    namelist_list[0]="$testbed_namelist"
    modelname_list[0]="testbed"
    modeltype_list[0]=99
  else
    check_error 1 "No namelist is defined"
  fi 
fi

#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# set some default values and derive some run parameteres
restart=${restart:=".false."}
restartSemaphoreFilename='isRestartRun.sem'
#AUTOMATIC_RESTART_SETUP:
if [ -f ${restartSemaphoreFilename} ]; then
  restart=.true.
  #  do not delete switch-file, to enable restart after unintended abort
  #[[ -f ${restartSemaphoreFilename} ]] && rm ${restartSemaphoreFilename}
fi
#END AUTOMATIC_RESTART_SETUP
#
# wait to let GPFS finish the write operations
if [ "x$restart" != 'x.false.' -a "x$submit" != 'x' ]; then
  if [ x$(stat -fc %T $EXPDIR) = xgpfs ]; then
    sleep 10;
  fi
fi
# fill some checks

run_atmo=${run_atmo="false"}
if [ x$atmo_namelist != x ]; then
  run_atmo="true"
fi
run_jsbach=${run_jsbach="false"}
run_ocean=${run_ocean="false"}
if [ x$ocean_namelist != x ]; then
  run_ocean="true"
fi
run_psrad=${run_psrad="false"}
if [ x$psrad_namelist != x ]; then
  run_psrad="true"
fi
run_hamocc=${run_hamocc="false"}
if [ x$hamocc_namelist != x ]; then
  run_hamocc="true"
fi

#-----------------------------------------------------------------------------
# add grids to required files
all_grids="${atmo_dyn_grids} ${atmo_rad_grids} ${ocean_grids}"
for gridfile in ${all_grids}; do
  #
  gridfile=${gridfile//\'/} # strip all ' in case ' is used to delimit the grid names
  gridfile=${gridfile//\"/} # strip all " in case " is used to delimit the grid names
  gridfile=${gridfile//\,/} # strip all , in case , is used to separate the grid names
  #
  grfinfofile=${gridfile%.nc}-grfinfo.nc
  #
  ls -l ${HGRIDDIR}/$gridfile
  check_error $? "${HGRIDDIR}/$gridfile does not exist."
  add_link_file ${HGRIDDIR}/${gridfile} ./
  if [ -f ${HGRIDDIR}/${grfinfofile} ]; then    
    add_link_file ${HGRIDDIR}/${grfinfofile} ./
  fi
done
#-----------------------------------------------------------------------------

%{format_files(files, 'yr', 'y0', 'yN')}

# print_required_files
copy_required_files
link_required_files


#-----------------------------------------------------------------------------
# get restart files

if  [ x$restart_atmo_from != "x" ] ; then
  rm -f restart_atm_DOM01.nc
#  ln -s ${MODEL_DIR}/experiments/${restart_from_folder}/${restart_atmo_from} ${EXPDIR}/restart_atm_DOM01.nc
  cp ${MODEL_DIR}/experiments/${restart_from_folder}/${restart_atmo_from} cp_restart_atm.nc
  ln -s cp_restart_atm.nc restart_atm_DOM01.nc
  restart=".true."
fi
if  [ x$restart_ocean_from != "x" ] ; then
  rm -f restart_oce.nc
#  ln -s ${MODEL_DIR}/experiments/${restart_from_folder}/${restart_ocean_from} ${EXPDIR}/restart_oce.nc
  cp ${MODEL_DIR}/experiments/${restart_from_folder}/${restart_ocean_from} cp_restart_oce_DOM01.nc
  ln -s cp_restart_oce_DOM01.nc restart_oce_DOM01.nc
  restart=".true."
fi
#-----------------------------------------------------------------------------


read_restart_namelists=${read_restart_namelists:=".true."}

#-----------------------------------------------------------------------------
#
# create ICON master namelist
# ------------------------
# For a complete list see Namelist_overview and Namelist_overview.pdf

#-----------------------------------------------------------------------------
# create master_namelist
master_namelist=icon_master.namelist
if [ x$final_date = x ]; then
cat > $master_namelist << EOF
&master_nml
 lrestart            = .true.
/
&master_time_control_nml
 experimentStartDate  = "$initial_date" 
 experimentReferenceDate = "$reference_date"
 restartTimeIntval    = "$restart_interval" 
 checkpointTimeIntval = "$checkpoint_interval" 
/
&time_nml
 is_relative_time = .false.
/
EOF
else
cat > $master_namelist << EOF
&master_nml
 lrestart            = .true.
 read_restart_namelists = $read_restart_namelists
/
&master_time_control_nml
 calendar             = "%{CALENDAR}"
 checkpointTimeIntval = "$checkpoint_interval" 
 restartTimeIntval    = "$restart_interval" 
 experimentStartDate  = "$initial_date" 
 experimentReferenceDate = "$reference_date"
 experimentStopDate   = "$final_date" 
/
&time_nml
 is_relative_time = .false.
/
EOF
fi
#-----------------------------------------------------------------------------


#-----------------------------------------------------------------------------
# add model component to master_namelist
add_component_to_master_namelist()
{
    
  model_namelist_filename="$1"
  model_name=$2
  model_type=$3
  model_min_rank=$4
  model_max_rank=$5
  model_inc_rank=$6
  
cat >> $master_namelist << EOF
&master_model_nml
  model_name="$model_name"
  model_namelist_filename="$model_namelist_filename"
  model_type=$model_type
  model_min_rank=$model_min_rank
  model_max_rank=$model_max_rank
  model_inc_rank=$model_inc_rank
/
EOF

#-----------
#check namelist file
  if [[ ! -f $model_namelist_filename ]] ; then
    check_error 1 "$model_namelist_filename does not exist"
  fi  

}
#-----------------------------------------------------------------------------


no_of_models=${#namelist_list[*]}
echo "no_of_models=$no_of_models"

j=0
while [ $j -lt ${no_of_models} ]
do
  add_component_to_master_namelist "${namelist_list[$j]}" "${modelname_list[$j]}" ${modeltype_list[$j]} ${minrank_list[$j]} ${maxrank_list[$j]} ${incrank_list[$j]}
  j=`expr ${j} + 1`
done

#-----------------------------------------------------------------------------
# Add JSBACH part to master_namelist

if [[ $run_jsbach == @(yes|true) ]]; then
  cat >> $master_namelist << EOF
&jsb_control_nml
 is_standalone      = .false.
 restart_jsbach     = .false.
 debug_level        = 0
 timer_level        = 0
/
EOF
#
if [[ -n ${atmo_dyn_grids} ]]; then
  set -A gridfiles $atmo_dyn_grids
  no_of_domains=${#gridfiles[*]}
else
  no_of_domains=1
fi
echo "no_of_domains=$no_of_domains"
domain=""
domain_suffix=""
j=1
while [ $j -le ${no_of_domains} ]
do
  if [[ $no_of_domains -gt 1 ]]; then
    # no_of_domains < 10 !
    domain=" DOM0${j}"
    domain_suffix="_d${j}"
  fi
  cat >> $master_namelist << EOF
&jsb_model_nml
 model_id = $j
 model_name = "JSBACH${domain}"
 model_shortname = "jsb${domain_suffix}"
 model_description = 'JSBACH land surface model'
 model_namelist_filename = "${lnd_namelist}${domain_suffix}"
/
EOF
  if [[ ! -f ${lnd_namelist}${domain_suffix} ]] ; then
    check_error 1 "${lnd_namelist}${domain_suffix} does not exist"
  fi
  j=`expr ${j} + 1`
done
fi
#
#  get model
#
ls -l ${MODEL}
check_error $? "${MODEL} does not exist?"
#
ldd -v ${MODEL}
#
#-----------------------------------------------------------------------------
#
# start experiment
#

rm -f finish.status
#
date
${START} ${MODEL} # > out.txt 2>&1
date
#
if [ -r finish.status ] ; then
  check_final_status 0 "${START} ${MODEL}"
else
  check_final_status -1 "${START} ${MODEL}"
fi

# Update date info for next run

echo $next_date > $start_date_file

#
#-----------------------------------------------------------------------------
#
finish_status=`cat finish.status`
echo $finish_status
echo "============================"
echo "Script run successfully: $finish_status"
echo "============================"

#%  if WITH_OBGC is set:
#-----------------------------------------------------------------------------
# store HAMOCC log file
strg="$(ls -rt ${EXPNAME}_hamocc_EU*.nc* | tail -1 )"
prefx="${EXPNAME}_hamocc_EU_tendencies"
foo=${strg##${prefx}}
foo=${foo%%.*}
bgcout_file="bgcout_${foo}"
mv bgcout $bgcout_file
#%  endif
#-----------------------------------------------------------------------------
namelist_list=""
#-----------------------------------------------------------------------------
# check if we have to restart, ie resubmit
#   Note: this is a different mechanism from checking the restart
if [ $finish_status = "RESTART" ] ; then
  echo "restart next experiment..."
  this_script="${SCRIPT_DIR}/${job_name}"
  echo 'this_script: ' "$this_script"
  touch ${restartSemaphoreFilename}
  cd ${SCRIPT_DIR}
  ${submit} $this_script
else
  [[ -f ${restartSemaphoreFilename} ]] && rm ${restartSemaphoreFilename}
fi

#-----------------------------------------------------------------------------

# Mark current run as successful in log
echo $(date -u +'%Y-%m-%dT%H:%M:%SZ') ${start_date%:*} ${end_date%:*} ${%{JOB.id_environ}}  end >> $exp_log_file

cd $SCRIPT_DIR

#% for job in JOB['.trigger']|list:
%{JOB.batch_command} %{EXP_ID}.%{job} $start_date
#% endfor

#-----------------------------------------------------------------------------
# vim:ft=sh
#-----------------------------------------------------------------------------
