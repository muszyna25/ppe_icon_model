#! /usr/bash
#______________________________________________________________________________
#
# Sets the parameters for various machine use_compiler configurations
# and creates the run script headers for these configurations
#
# Original version by Marco Giorgetta and Hermann Asensio
# Updated by Leonidas Linardakis, MPI-M, 2010-11-24
#______________________________________________________________________________
# Basic parameters in this script
#
# use_nodes  =  no of nodes to run
# use_mpi_procs_pernode = number of mpi procs per node
#______________________________________________________________________________

set -eu

BB_SYSTEM=${BB_SYSTEM:=""}

add_free_var()
{
    varName=$1
    varValue=$2

    (( no_of_free_variables++ ))
    free_variable[$no_of_free_variables]=$varName
    free_variable_value[$no_of_free_variables]=$varValue
}

set_default()
{
    if [[ "x$(eval echo \$$1)" == "x" ]]
    then
	eval "$1=$2"
    fi
}

start_header()
{
    if [[ "$use_mpi" == "no" ]]
    then
	start=${start:=""}
    else
	start=${start:="$use_mpi_startrun"}
    fi

    if [[ ! -e $output_script ]]
    then
	echo "[ERROR] $output_script does not exist!"
 	exit 1
    fi
    cat >> $output_script << EOF
#=============================================================================
set +x
$(test x"${use_ulimit}" = xno || printf 'ulimit -s unlimited')
#=============================================================================
#
# ICON run script:
# !ATTENTION! Do not change the format of the following lines.
#             They are evaluated by checksuite scripts.
# created by $0
# target machine is $use_target
# target use_compiler is $use_compiler
# with_mpi=$use_mpi
# with_openmp=$use_openmp
# memory_model=$use_memory_model
# submit with $use_submit
#
#=============================================================================
#
# OpenMP environment variables
# ----------------------------
export OMP_NUM_THREADS=${use_openmp_threads}
export ICON_THREADS=${use_openmp_threads}
export OMP_SCHEDULE=${use_OMP_SCHEDULE}
export OMP_DYNAMIC="false"
export OMP_STACKSIZE=${use_omp_stacksize}
#
# MPI variables
# -------------
no_of_nodes=${use_nodes}
mpi_procs_pernode=$use_mpi_procs_pernode
((mpi_total_procs=no_of_nodes * mpi_procs_pernode))
#
# blocking length
# ---------------
nproma=$use_nproma
nblocks_c=$use_nblocks_c
#
#=============================================================================

# load local setting, if existing
# -------------------------------
if [ -a ../setting ]
then
  echo "Load Setting"
  . ../setting
fi

# environment variables for the experiment and the target system
# --------------------------------------------------------------
EOF

# environment variables for the experiment and the target system

    i=1
    while [ $i -le ${no_of_free_variables} ]
    do
	echo "export ${free_variable[$i]}=\"${free_variable_value[$i]}\"" >> $output_script
	i=$((i+1));
    done

    if [[ "x$use_load_profile" != "x" ]]
    then
	profile_filename=`echo $use_load_profile | cut -d ' ' -f2`
	cat >> $output_script << EOF
# load profile
# ------------
if [[ -a  $profile_filename ]]
then
	$use_load_profile
fi
EOF
    fi

if [[ "x$use_load_profile" != "x" && "x$use_load_modules" != "x" ]]; then
    profile_filename=`echo $use_load_profile | cut -d ' ' -f2`
    cat >> $output_script << EOF
#=============================================================================
# load modules
loadmodule="$use_load_modules"
module load \$loadmodule
module list
#=============================================================================
EOF
fi


    cat >> $output_script << EOF

#=============================================================================

# directories with absolute paths
# -------------------------------
thisdir=\$(pwd)
basedir=\${thisdir%/*}
export basedir
icon_data_rootFolder="${icon_data_rootFolder}"

# how to start the icon model
# ---------------------------
export START="$start"
export MODEL="\${basedir}/bin/icon"

# how to submit the next job
# --------------------------
submit="$use_submit"
job_name="$job_name"

# cdo for post-processing
# -----------------------
cdo="${cdo}"
cdo_diff="${cdo} ${cdo_diff}"

# define script functions used in the experiment run script
# ---------------------------------------------------------
. ./add_run_routines

#=============================================================================

EOF

}

#=============================================================================
set_run_target_default()
{
    # Default configuration for unknown systems

    icon_data_rootFolder=${icon_data_rootFolder:=~/pool/data/ICON}

    set_default use_ulimit no
    set_default use_nproma 48
    set_default use_nblocks_c 0
    set_default use_nodes 1

    if [[ "$use_mpi" == "yes" ]]
    then
	set_default use_mpi_procs_pernode 2
    fi
    if [[ "$use_openmp" == "yes" ]]
    then
	set_default use_openmp_threads 2
    else
	set_default use_openmp_threads 1
    fi

    start_header

}

#=============================================================================
set_run_target_()
{
  echo "!! THIS IS A FALLBACK SETUP !!"
  echo "!! YOUR HOST/SITE IS unknown TO THE SCRIPTING ENGINE !!"
  set_run_target_default
}

#=============================================================================
set_run_target_bullx_cpu()
{
    # mistral.dkrz.de
    icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    set_default use_nodes 1
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	use_account_no="mh0156"
    fi
    #   use_submit="$use_submit -N \${SLURM_JOB_NUM_NODES:-1}"
    use_OMP_SCHEDULE="dynamic,1"
    use_omp_stacksize=200M

    # set to 1 to use smt feature
    smt_adjust=2
    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / 6 / '"$smt_adjust"'))"'
	    set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '$smt_adjust' / OMP_NUM_THREADS))"'
	else
	    set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
	fi
    else
	# on mistral, srun is still needed for running processes at
	# full frequency
	start="srun --cpu-freq=HighM1 -n 1"
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
	    #      set_default use_mpi_procs_pernode 1
	    start+=" --cpus-per-task=\$(($smt_adjust * OMP_NUM_THREADS))"
	else
	    is_serial_run="yes"
	fi
    fi

    [[ "x$use_queue" != 'x' ]] && queue=$use_queue

    use_mpi_startrun="srun --cpu-freq=HighM1 --kill-on-bad-exit=1 --nodes=\${SLURM_JOB_NUM_NODES:-1} --cpu_bind=quiet,cores --distribution=block:block --ntasks=\$((no_of_nodes * mpi_procs_pernode)) --ntasks-per-node=\${mpi_procs_pernode} --cpus-per-task=\$((${smt_adjust} * OMP_NUM_THREADS)) --propagate=STACK,CORE"
    # TODO: disabled because other core files are generated
    # case $use_flags_group in
    #   debug)
    #     use_mpi_startrun="${use_mpi_startrun},CORE"
    #     ;;
    # esac

    case _$use_compiler in
	"_intel" )
            add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
            add_free_var KMP_LIBRARY "turnaround"
            add_free_var KMP_KMP_SETTINGS "1"
            add_free_var OMP_WAIT_POLICY "active"
            add_free_var KMP_WARNINGS "0"
	    ;;
    esac
    case $use_mpi_root in
	*bullxmpi*)
	    # this does currently not work on mistral but is most probably needed
	    # for other SLURM installations
	    #use_mpi_startrun+=" --mpi=openmpi"
	    add_free_var OMPI_MCA_pml cm
	    add_free_var OMPI_MCA_mtl mxm
	    add_free_var OMPI_MCA_coll '^ghc'
	    add_free_var MXM_RDMA_PORTS 'mlx5_0:1'
	    ;;
	*openmpi*)
	    add_free_var OMPI_MCA_pml cm
	    add_free_var OMPI_MCA_mtl mxm
	    add_free_var OMPI_MCA_coll ^fca
	    add_free_var MXM_RDMA_PORTS 'mlx5_0:1'
	    add_free_var HCOLL_MAIN_IB 'mlx5_0:1'
	    add_free_var HCOLL_ML_DISABLE_BARRIER 1
	    add_free_var HCOLL_ML_DISABLE_IBARRIER 1
	    add_free_var HCOLL_ML_DISABLE_BCAST 1
	    add_free_var HCOLL_ENABLE_MCAST_ALL 1
	    add_free_var HCOLL_ENABLE_MCAST 1
	    add_free_var OMPI_MCA_coll_sync_barrier_after_alltoallv 1
	    add_free_var OMPI_MCA_coll_sync_barrier_after_alltoallw 1
	    add_free_var MXM_HANDLE_ERRORS bt
	    add_free_var UCX_HANDLE_ERRORS bt
	    ;;
	*intel*mpi*)
	    add_free_var I_MPI_FABRICS shm:dapl
	    add_free_var I_MPI_DAPL_UD enable
	    add_free_var I_MPI_DAPL_UD_PROVIDER ofa-v2-mlx5_0-1u
	    add_free_var DAPL_UCM_REP_TIME 8000
	    add_free_var DAPL_UCM_RTU_TIME 4000
	    add_free_var DAPL_UCM_CQ_SIZE 1000
	    add_free_var DAPL_UCM_QP_SIZE 1000
	    add_free_var DAPL_UCM_RETRY 10
	    add_free_var DAPL_ACK_RETRY 10
	    add_free_var DAPL_ACK_TIMER 20
	    add_free_var DAPL_UCM_TX_BURST 100
	    add_free_var DAPL_WR_MAX 500
	    ;;
    esac

    add_free_var MALLOC_TRIM_THRESHOLD_ -1
    if [[ "$is_serial_run" == "yes"  ]]
    then
	queue=${queue:="compute2,compute"}
	use_nodes=1
	tasks_per_node=1
	SBATCH_ntasks_per_node=""
    else
	queue=${queue:="compute2,compute"}
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi

    # ------------------------------------
    s_cpu_time="00:30:00"
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time=$use_cpu_time
    fi
    # ------------------------------------

    cat >> $output_script << EOF
#=============================================================================

# mistral cpu batch job parameters
# --------------------------------
#SBATCH --account=$use_account_no
$(if [[ "x$BB_SYSTEM" != 'x' ]]
 then
  printf '#SBATCH --qos=buildbot
';fi)
#SBATCH --job-name=$job_name
#SBATCH --partition=$queue
#SBATCH --chdir=$output_folder
#SBATCH --nodes=$use_nodes
#SBATCH --threads-per-core=2
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --exclusive
#SBATCH --time=$s_cpu_time
$(if [[ "$use_mpi_root" == *intel*mpi* ]]
 then
  printf '#========================================
# the following line is only needed for srun to work with Intel MPI
# but should be commented when using BullX MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
#========================================' ; fi)
EOF

    # for the rest use the SLURM_JOB_NUM_NODES by default
    use_nodes="\${SLURM_JOB_NUM_NODES:=${use_nodes}}"

    start_header

    cat >> $output_script <<EOF
ulimit -s 2097152
ulimit -c 0
$(case $use_flags_group in
    (debug)
      printf 'ulimit -c unlimited'
      ;;
  esac)
EOF
}

#=============================================================================
set_run_target_bullx_gpu()
{
    # mistral.dkrz.de
    icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    set_default use_nodes 1
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	use_account_no="mh0156"
    fi
    use_submit="$use_submit -N \${SLURM_JOB_NUM_NODES:-1}"
    use_OMP_SCHEDULE="dynamic,1"
    use_omp_stacksize=200M

    # set to 1 to use smt feature
    smt_adjust=2
    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	set_default use_mpi_procs_pernode '"\$((\${SLURM_JOB_CPUS_PER_NODE%%\(*} / '"$smt_adjust"'))"'
    else
	# on mistral, srun is still needed for running processes at
	# full frequency
	start="srun --cpu-freq=HighM1 -n 1"
	is_serial_run="yes"
    fi

    [[ "x$use_queue" != 'x' ]] && queue=$use_queue

    use_mpi_startrun="srun --cpu-freq=HighM1"

    add_free_var MALLOC_TRIM_THRESHOLD_ -1
    if [[ "$is_serial_run" == "yes"  ]]
    then
	queue=${queue:="gpu"}
	use_nodes=1
	tasks_per_node=1
	SBATCH_ntasks_per_node=""
    else
	queue=${queue:="gpu"}
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi

    # ------------------------------------
    s_cpu_time="00:30:00"
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time=$use_cpu_time
    fi
    # ------------------------------------

    cat >> $output_script << EOF
#=============================================================================

# mistral gpu batch job parameters
# --------------------------------
#SBATCH --account=$use_account_no
$(if [[ "x$BB_SYSTEM" != 'x' ]]
 then
  printf '#SBATCH --qos=buildbot
';fi)
#SBATCH --job-name=$job_name
#SBATCH --partition=$queue
#SBATCH --chdir=$output_folder
#SBATCH --nodes=$use_nodes
#SBATCH --threads-per-core=1
# the following is needed to work around a bug that otherwise leads to
# a too low number of ranks when using compute,compute2 as queue
#SBATCH --mem=0
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --exclusive
#SBATCH --time=$s_cpu_time
#SBATCH --constraint=k80
$(if [[ "$use_mpi_root" == *intel*mpi* ]]
 then
  printf '#========================================
# the following line is only needed for srun to work with Intel MPI
# but should be commented when using BullX MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so
#========================================' ; fi)
EOF

    # for the rest use the SLURM_JOB_NUM_NODES by default
    use_nodes="\${SLURM_JOB_NUM_NODES:=${use_nodes}}"

    start_header

    cat >>$output_script << EOF
ulimit -s 2097152
ulimit -c 0
$(case $use_flags_group in
    (debug)
      printf 'ulimit -c unlimited'
      ;;
  esac)
EOF
}

#=============================================================================
set_run_target_mpipc()
{
    # *.mpimet.mpg.de with Linux (workstations and breeze)
    icon_data_rootFolder=${icon_data_rootFolder:=/pool/data/ICON}

    set_default use_nproma 64
    set_default use_nblocks_c 0
    use_nodes=1

    if [[ $use_mpi == "yes" ]]
    then
	set_default use_mpi_procs_pernode 2
    fi
    if [[ $use_openmp == "yes" ]]
    then
	set_default use_openmp_threads 2
    else
	set_default use_openmp_threads 1
    fi

    start_header
}

#=============================================================================
set_run_target_docker_cpu()
{
    # Docker container
    icon_data_rootFolder=${icon_data_rootFolder:=/icon-data-pool}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    use_nodes=1

    set_default use_mpi_procs_pernode 1
    set_default use_openmp_threads 1

    use_OMP_SCHEDULE="static,12"

    start_header
}

#=============================================================================
set_run_target_docker_gpu()
{
    # Docker container with GPU
    icon_data_rootFolder=${icon_data_rootFolder:=/icon-data-pool}

    set_default use_nproma 20480
    set_default use_nblocks_c 0
    use_nodes=1

    set_default use_mpi_procs_pernode 1
    set_default use_openmp_threads 1

    use_OMP_SCHEDULE="static,12"

    start_header
}

#=============================================================================
set_run_target_oflws()
{
  # *.dwd.de with Linux (workstations)
    icon_data_rootFolder=${icon_data_rootFolder:=""}
    
    set_default use_nproma 64
    set_default use_nblocks_c 0
    use_nodes=1

    if [[ "$use_mpi" == "yes" ]]
    then
	set_default use_mpi_procs_pernode 4
    fi
    if [[ "$use_openmp" == "yes" ]]
    then
	set_default use_openmp_threads 2
    fi

    start_header
}

#=============================================================================
set_run_target_hpc()
{
    # xce.dwd.de
    icon_data_rootFolder=${icon_data_rootFolder:=""}
    
    set_default use_nproma 64
    set_default use_nblocks_c 0
    set_default use_nodes 1
    set_default use_mpi_procs_pernode 1
    set_default use_openmp_threads 1

    cdo="/e/uhome/hanlauf/X86_64/bin/cdo"
    cdo_diff="diff"

    
    cat >> $output_script << EOF
#### BATCH_SYSTEM=PBS ####
#-----------------------------------------------------------------------------
#PBS -q lang
#PBS -j oe
#PBS -o LOG.$job_name.o
#PBS -l select=$use_nodes:ncpus=$use_mpi_procs_pernode
#PBS -m n
# ===========================
#-----------------------------------------------------------------------------
# for PBS change to directory where job was submitted
# (without this job is started in HOME)
if [[ -n \${PBS_O_WORKDIR} ]]
 then
  cd \${PBS_O_WORKDIR}
fi
export F_PROGINF=DETAIL
#-----------------------------------------------------------------------------
EOF
  
    start_header
}

#=============================================================================
set_run_target_tsa_cpu()
{
    rootFolder_prefix=/scratch/jenkins/icon
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    set_default use_nodes 1
    # use_OMP_SCHEDULE="static,1"
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buildbot, do not use job id
	use_account_no="d56"
    fi
    use_OMP_SCHEDULE="static,12"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	    set_default use_openmp_threads 1
	else
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	fi
    else
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 1
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	else
	    is_serial_run="yes"
	fi
    fi

    case _$use_compiler in
	"_intel" )
            use_mpi_startrun="$use_mpi_startrun --verbose"
            add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
            add_free_var KMP_LIBRARY "turnaround"
            add_free_var KMP_KMP_SETTINGS "1"
            add_free_var OMP_WAIT_POLICY "active"
	    ;;
    esac

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_nodes="SBATCH --nodes=1"
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_nodes="SBATCH --nodes=${use_nodes}"
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi
    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 15 minutes on Tsa (SLURM delays)
	s_cpu_time="SBATCH --time=00:15:00"
    fi
    # ------------------------------------

    use_nodes=${use_nodes}
    use_mpi_procs_pernode=${use_mpi_procs_pernode}

    cat >> $output_script << EOF
#=============================================================================

# tsa cpu batch job parameters
# ------------------------------
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
EOF

  start_header
  cat >>$output_script <<EOF
export UCX_MEMTYPE_CACHE=n
export UCX_TLS=tcp,sm
export ECCODES_DEFINITION_PATH=${icon_data_rootFolder}/mch/eccodes_definitions
EOF
}

set_run_target_tsa_gpu()
{

    rootFolder_prefix=/scratch/jenkins/icon
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 8000
    set_default use_nblocks_c 0
    set_default use_prefetch_procs 0
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	use_account_no="d56"
    fi
    use_OMP_SCHEDULE="static,1"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
     set_default use_nodes 1
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	    set_default use_openmp_threads 1
	else
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	fi
    else
    	set_default use_nodes 1
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 1
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	else
	    is_serial_run="yes"
	fi
    fi

    # Currently ICON on Tsa only works correctly with 1 gpu per node
    use_mpi_procs_pernode=1

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_nodes="SBATCH --nodes=1"
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_nodes="SBATCH --nodes=$use_nodes"
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi

    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 45 minutes
	s_cpu_time="SBATCH --time=00:45:00"
    fi

    # ------------------------------------

    use_nodes=${use_nodes}
    use_mpi_procs_pernode=${use_mpi_procs_pernode}

    cat >> $output_script << EOF
#=============================================================================

# tsa gpu batch job parameters
# ------------------------------
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#SBATCH --gres=gpu:${tasks_per_node}
#SBATCH --partition=dev
#$s_cpu_time
EOF

    start_header
    cat >>$output_script <<EOF
export CRAY_CUDA_MPS=1
## WORKAROUNDS FOR TSA
export UCX_MEMTYPE_CACHE=n
export UCX_TLS=rc_x,ud_x,mm,shm,cuda_copy,cuda_ipc,cma
if [ "\$mpi_total_procs" -ne 2 ] && [ "\$mpi_procs_pernode" -ne 1 ]; then
export PGI_ACC_NOTIFY=1
export PGI_ACC_SYNCHRONOUS=1
export UCX_TLS=tcp,sm
export OMPI_MCA_osc="^rdma"
export OMPI_MCA_pml="ucx"
export OMPI_MCA_btl="^uct"
export ECCODES_DEFINITION_PATH=${icon_data_rootFolder}/mch/eccodes_definitions
fi
EOF
}

#=============================================================================
set_run_target_daint_cpu()
{
    rootFolder_prefix=/users/icontest
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    set_default use_nodes 1
    # use_OMP_SCHEDULE="static,1"
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buildbot, do not use job id
	use_account_no="d56"
    fi
    use_OMP_SCHEDULE="static,12"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode 4
	    set_default use_openmp_threads 6
	else
	    set_default use_mpi_procs_pernode 12
	fi
    else
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 12
	    set_default use_mpi_procs_pernode 1
	else
	    is_serial_run="yes"
	fi
    fi

    case _$use_compiler in
	"_intel" )
            use_mpi_startrun="$use_mpi_startrun --verbose"
            add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
            add_free_var KMP_LIBRARY "turnaround"
            add_free_var KMP_KMP_SETTINGS "1"
            add_free_var OMP_WAIT_POLICY "active"
	    ;;
    esac

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_nodes="SBATCH --nodes=1"
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_nodes="SBATCH --nodes=${use_nodes}"
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi
    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 15 minutes on Daint (SLURM delays)
	s_cpu_time="SBATCH --time=00:15:00"
    fi
    # ------------------------------------

    use_nodes=${use_nodes}
    use_mpi_procs_pernode=${use_mpi_procs_pernode}

    cat >> $output_script << EOF
#=============================================================================

# daint cpu batch job parameters
# ------------------------------
#SBATCH --constraint=gpu
#SBATCH --account=$use_account_no
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
EOF

  start_header

  cat >> $output_script << EOF
export ECCODES_DEFINITION_PATH=${icon_data_rootFolder}/mch/eccodes_definitions
EOF
}

#=============================================================================
#=============================================================================
set_run_target_daint_gpu()
{
    rootFolder_prefix=/users/icontest
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 8000
    set_default use_nblocks_c 1
    set_default use_prefetch_procs 0
    set_default use_account_no "$(id -gn)"
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	use_account_no="d56"
    fi
    use_OMP_SCHEDULE="static,1"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
    	set_default use_nodes 2
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	    set_default use_openmp_threads 1
	else
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	fi
    else
    	set_default use_nodes 1
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 1
	    set_default use_mpi_procs_pernode $((1+use_prefetch_procs))
	else
	    is_serial_run="yes"
	fi
    fi

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_nodes="SBATCH --nodes=1"
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_nodes="SBATCH --nodes=$use_nodes"
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=${tasks_per_node}"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi

    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 45 minutes
	s_cpu_time="SBATCH --time=00:45:00"
    fi

    # ------------------------------------

    use_nodes=${use_nodes}
    use_mpi_procs_pernode=${use_mpi_procs_pernode}

    cat >> $output_script << EOF
#=============================================================================

# daint gpu batch job parameters
# ------------------------------
#SBATCH --constraint=gpu
#SBATCH --account=$use_account_no
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#$SBATCH_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
EOF

    start_header
    cat >>$output_script <<EOF
export CRAY_CUDA_MPS=1
export MPICH_RDMA_ENABLED_CUDA=1
#export PGI_ACC_SYNCHRONOUS=1
export ECCODES_DEFINITION_PATH=${icon_data_rootFolder}/mch/eccodes_definitions
EOF
}

#=============================================================================
set_run_target_dom_gpu()
{
    rootFolder_prefix=/users/icontest
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 256
    set_default use_nblocks_c 0
    set_default use_nodes 1
    use_OMP_SCHEDULE="static,1"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode 1
	    set_default use_openmp_threads 1
	else
	    set_default use_mpi_procs_pernode 1
	fi
    else
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 1
	    set_default use_mpi_procs_pernode 1
	else
	    is_serial_run="yes"
	fi
    fi

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=1"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi

    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 45 minutes
	s_cpu_time="SBATCH --time=00:45:00"
    fi

    # ------------------------------------

    cat >> $output_script << EOF
#=============================================================================

# dom gpu batch job parameters
# ----------------------------
#SBATCH --account=csstaff
#SBATCH --job-name=$job_name
#SBATCH --output=$job_log_name.o
#SBATCH --error=$job_log_name.o
#SBATCH --nodes=$use_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
EOF

    start_header
}

#=============================================================================
set_run_target_tave_knl()
{
    rootFolder_prefix=/users/icontest
    icon_data_rootFolder=${icon_data_rootFolder:=${rootFolder_prefix}/pool/data/ICON}

    set_default use_nproma 16
    set_default use_nblocks_c 0
    set_default use_nodes 1
    use_OMP_SCHEDULE="static,2"

    is_serial_run="no"
    if [[ $use_mpi == "yes" ]]
    then
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_mpi_procs_pernode 60
	    set_default use_openmp_threads 2
	else
	    set_default use_mpi_procs_pernode 60
	fi
    else
	if [[ $use_openmp == "yes" ]]
	then
	    set_default use_openmp_threads 60
	    set_default use_mpi_procs_pernode 1
	else
	    is_serial_run="yes"
	fi
    fi

    case _$use_compiler in
	"_intel" )
            use_mpi_startrun="$use_mpi_startrun -npernode \$mpi_procs_pernode -cpus-per-proc \$OMP_NUM_THREADS --bind-to-core --verbose --report-bindings"
            add_free_var KMP_AFFINITY "verbose,granularity=core,compact,1,1"
            add_free_var KMP_LIBRARY "turnaround"
            add_free_var KMP_KMP_SETTINGS "1"
            add_free_var OMP_WAIT_POLICY "active"
	    ;;
    esac

    if [[ "$is_serial_run" == "yes"  ]]
    then
	use_nodes=1
	tasks_per_node=1
	SBATCH_ntasks_per_node=""
    else
	job_type=parallel
	tasks_per_node=$use_mpi_procs_pernode
	SBATCH_ntasks_per_node="SBATCH --ntasks-per-node=32"
    fi

    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this buildbot, do not use job id
	job_log_name="LOG.$job_name"
    else
	job_log_name="LOG.$job_name.%j"
    fi
    # ------------------------------------
    s_cpu_time=""
    if [[ "x$use_cpu_time" != "x" ]]
    then
	s_cpu_time="SBATCH --time=$use_cpu_time"
    fi
    if [[ "x$BB_SYSTEM" != "x" ]]
    then
	# this is buldbot, do not run more than 45 minutes
	s_cpu_time="SBATCH --time=00:45:00"
    fi
    # ------------------------------------

    use_nodes=${use_nodes}
    use_mpi_procs_pernode=${use_mpi_procs_pernode}

    cat >> $output_script << EOF
#=============================================================================
# tave batch job parameters
#--------------------------
#SBATCH --account=csstaff
#SBATCH --job-name=$job_name
#SBATCH --output=$output_folder/$job_log_name.o
#SBATCH --error=$output_folder/$job_log_name.o
#SBATCH --nodes=$use_nodes
#$SBATCH_ntasks_per_node
#$s_cpu_time
EOF
    start_header
}

#=============================================================================
#=============================================================================
set_run_target_euler()
{
  # euler.ethz.ch
  typeset poolFolder_prefix=/cluster/work/climate/colombsi
  icon_data_rootFolder=${icon_data_rootFolder:=${poolFolder_prefix}/icon_input}
  icon_data_poolFolder=${icon_data_poolFolder:=${poolFolder_prefix}/icon_input}

  set_default use_nproma 64
  set_default use_nodes 1

  #JENKINS_NO_OF_CORES can is unbound when not used with jenkins icon
  set +eu

  if [[ -z $JENKINS_NO_OF_CORES ]]; then 
      set_default use_mpi_procs_pernode 1
  else
      set_default use_mpi_procs_pernode $JENKINS_NO_OF_CORES
  fi
  set_default use_openmp_threads 1

  set -eu

  cdo="/cluster/apps/climate/2.0/bin/cdo"
  cdo_diff="diff"

  job_log_name="LOG.$job_name.%J"

  cat >> $output_script << EOF
# ===========================
#### BATCH_SYSTEM=BSUB ####
#-----------------------------------------------------------------------------
#BSUB -J $job_name
#BSUB -oo $output_folder/$job_log_name.o
#BSUB -eo $output_folder/$job_log_name.o
# ===========================
EOF

  # create header
  start_header

}

#=============================================================================

#=============================================================================
set_run_target_pacluster()
{  
    set_default use_nproma 32
    set_default use_nblocks_c 0
    set_default use_nodes 1
    set_default use_cpu_time 04:00:00
   
    if [[ $use_openmp == "yes" ]]
    then
	use_mpi="no"
	set_default use_openmp_threads 24
	queue=""
	resources=""
    elif [[ $use_mpi == "yes" ]]
    then
	set_default use_mpi_procs_pernode 12
	if [[ "$use_mpi_procs_pernode" == "1" ]]
	then
	    queue=""
	    resources=""
	else
	    queue=""
	    resources=""
	fi
    else
	queue="-q s8"
	resources=""
    fi

    cat >> $output_script << EOF
#############################################################################
# DLR Linux Cluster batch job parameters
# EMBEDDED FLAGS FOR PBS Pro
#############################################################################
################# shell to use
#PBS -S /bin/ksh
################# export all  environment  variables to job-script
#PBS -V
################# name of the log file
#PBS -o ./${job_name}.\${PBS_JOBID}.log
################# join standard and error stream (oe, eo) ?
#PBS -j oe
################# do not rerun job if system failure occurs
#PBS -r n    
################# send e-mail when [(a)borting|(b)eginning|(e)nding] job
### #PBS -m ae
### #PBS -M my_userid@my_institute.my_toplevel_domain
################# always ppn=12 tasks per node!
#PBS -l nodes=$use_nodes:ppn=$use_mpi_procs_pernode
#PBS -l walltime=$use_cpu_time
#############################################################################
#-----------------------------------------------------------------------------
# for PBS change to directory where job was submitted
# (without this job is started in HOME)
if [[ -n \${PBS_O_WORKDIR} ]]
 then
  cd \${PBS_O_WORKDIR}
fi
#-----------------------------------------------------------------------------
EOF

    start_header
}

#=============================================================================
set_run_target_fh2()
{
    icon_data_rootFolder=${icon_data_rootFolder:=/pfs/work6/workspace/scratch/ln1297-boundary_mistral2-0/pool/data/ICON}

    set_default use_nproma 8
    set_default use_nblocks_c 0
    use_nodes=1
  
    if [[ $use_mpi == "yes" ]]
    then
	set_default use_mpi_procs_pernode 2
    fi
    if [[ $use_openmp == "yes" ]]
    then
	set_default use_openmp_threads 2
    else
	set_default use_openmp_threads 1
    fi
    output_script_name=$(basename "$output_script")
    #| sed -r "s/(.+)\/.+/\1/"
    stripped_output="LOG.$output_script_name.err"
    cat >> $output_script << EOF

#SBATCH --nodes=4
#SBATCH --time=1:00:00
#SBATCH --ntasks-per-node=20
#SBATCH --partition=develop

EOF

    start_header
}

#=============================================================================
set_run_target_hk()
{
    icon_data_rootFolder=${icon_data_rootFolder:=/pfs/work6/workspace/scratch/ln1297-boundary_mistral2-0/pool/data/ICON}

    set_default use_nproma 8
    set_default use_nblocks_c 0
    use_nodes=1
  
    if [[ $use_mpi == "yes" ]]
    then
	set_default use_mpi_procs_pernode 2
    fi
    if [[ $use_openmp == "yes" ]]
    then
	set_default use_openmp_threads 2
    else
	set_default use_openmp_threads 1
    fi
    output_script_name=$(basename "$output_script")
    #| sed -r "s/(.+)\/.+/\1/"
    stripped_output="LOG.$output_script_name.err"
    cat >> $output_script << EOF

#SBATCH --nodes=4
#SBATCH --time=1:00:00
#SBATCH --ntasks-per-node=76
#SBATCH --partition=develop

EOF

    start_header
}

#=============================================================================
create_target_header()
{
    cdo="cdo"
    cdo_diff="diffn"
    use_OMP_SCHEDULE="static"

    if [[ "$use_mpi" == "no" ]]
    then
	use_nodes=1
	use_mpi_procs_pernode=1
    fi

    if [[ "$use_openmp" == "no" ]]
    then
	use_openmp_threads=1
    fi

    use_prefetch_procs=0
 	case $job_name in
        *mch_opr_r04b07.run)
            use_nodes=1
            use_mpi_procs_pernode=2
            use_nproma=4250
            ;;
        *mch_opr_r04b07_nest.run)
            use_nodes=3
            use_mpi_procs_pernode=1
            use_nproma=1600
            ;;
	    *mch_*)
	        use_nodes=4
	        ;;
	esac

    # set more default values
    set_default use_shell "/bin/ksh"
    set_default use_memory_model "default"
    set_default use_omp_stacksize 32M
      
    cat > $output_script << EOF
#! $use_shell
EOF
    set_run_target_${use_target}
}

