# Set use_site:
use_host='@host_fqdn@'
case "$use_host" in
  unknown) ;;
  wizard*|mlogin*|btlogin*|btc*) use_site=dkrz.de ;;
  daint*|tave*|santis*|kesch*) use_site=cscs.ch ;;
  ic2*|uc1*|fh2*) use_site=kit.edu ;;
  melian*) use_site=local.net ;;
  *mpimet.mpg.de) use_site=mpg.de ;;
esac
test -z "$use_site" && test x"$ICON_DOCKER" = x1 && use_site=docker
test -n "$use_site" || use_site=`echo "$use_host" | sed 's/^[^.]*\\.//'`
test -n "$use_site" || use_site='local.net'

# Set use_compiler:
use_compiler='@FC_VENDOR@'
case "$use_compiler" in
  gnu) use_compiler='gcc' ;;
  portland) use_compiler='pgi' ;;
  unknown) use_compiler= ;;
esac

# Set use_compiler_version:
use_compiler_version='@FC_VERSION@'
case "$use_compiler_version" in
  unknown) use_compiler_version= ;;
esac

# Set use_mpi:
use_mpi='no'
@MPI_ENABLED@use_mpi='yes'

# Set use_openmp:
use_openmp='no'
@OPENMP_ENABLED@use_openmp='yes'

# Set use_gpu:
use_gpu='no'
@GPU_ENABLED@use_gpu='yes'

# Init other variables:
use_mpi_startrun=
use_flags_group=
use_target=
use_mpi_procs_pernode=
use_load_modules=
use_load_profile=
use_submit=
use_sync_submit=
use_shell=

# Set default values:
test -n '@MPI_LAUNCH@' && use_mpi_startrun='@MPI_LAUNCH@ -n $mpi_total_procs'

# Set site-specific values:
case "$use_site" in
  cscs.ch)
    case "$HOST" in
      *daint*|*dom*)
        use_load_profile='. /opt/modules/default/etc/modules.sh'
        use_submit='sbatch'
        use_sync_submit='sbatch --wait'
        use_mpi_startrun='srun -n $mpi_total_procs --ntasks-per-node $mpi_procs_pernode --threads-per-core=1 --cpus-per-task $OMP_NUM_THREADS'
        if test xyes = x"${use_gpu}"; then use_target='daint_gpu'; else use_target='daint_cpu'; fi
      ;;

      *tave*)
        use_load_profile='. /opt/modules/default/etc/modules.sh'
        use_submit='sbatch'
        use_sync_submit='sbatch --wait'
        use_mpi_startrun='srun -n $mpi_total_procs --ntasks-per-node $mpi_procs_pernode --threads-per-core=1 --cpus-per-task $OMP_NUM_THREADS'
        use_target='tave_knl'
      ;;

      *kesch*)
        use_load_profile='. /etc/profile.d/modules.sh'
        use_submit='sbatch'
        use_sync_submit='sbatch --wait'
        use_mpi_startrun='srun -n $mpi_total_procs --ntasks-per-node $mpi_procs_pernode --threads-per-core=1 --cpus-per-task $OMP_NUM_THREADS'
        if test xyes = x"${use_gpu}"; then use_target='kesch_gpu'; else use_target='kesch_cpu'; fi
      ;;
    esac
  ;;

  dwd.de)
    case "$use_host" in
      xc*)
        use_submit='qsub'
        use_sync_submit='qsub -Wblock=true'
      ;;
      oflws*)
        use_target='oflws'
      ;;
      *) #hpc
        use_submit='qsubw'
        use_sync_submit='qsub -W block=true'
        use_target='hpc'
      ;;
    esac
  ;;

  dkrz.de)
    case "$use_host" in
      btlogin*|btc*|mlogin*|m*)
        use_load_profile='. /etc/profile'
        use_load_modules='ncl/6.2.1-gccsys cdo/1.9.5-gcc64 git python'
        use_submit='sbatch'
        use_sync_submit='sbatch --wait'
        use_mpi_startrun='srun'
        if test xyes = x"${use_gpu}"; then use_target='bullx_gpu'; else use_target='bullx_cpu'; fi
      ;;
    esac
  ;;

  kfa-juelich.de|fz-juelich.de|*jureca*|*juwels*)
    case "$use_host" in
      jrl*|juwels*)
        use_submit='sbatch'
        use_mpi_startrun='srun'
        use_target='jureca'
      ;;
      juqueen*)
        use_submit='llsubmit'
        use_mpi_startrun='runjob --ranks-per-node \$mpi_procs_pernode --envs OMP_NUM_THREADS=\$OMP_NUM_THREADS --exe'
        use_target='juqueen'
      ;;
    esac
  ;;

  kit.edu)
    case "$use_host" in
      uc1*)
        use_submit='sbatch'
        use_mpi_startrun='runjob --ranks-per-node \$mpi_procs_pernode --envs OMP_NUM_THREADS=\$OMP_NUM_THREADS --exe'
        use_target='ic2'
      ;;
      fh2*)
        use_load_profile='. /opt/lmod/lmod/init/sh'
        use_target='fh2'
        case "$use_compiler" in
          intel)
            use_submit='sbatch'
            case "$use_compiler_version" in
              17.*) use_mpi_startrun='/software/all/mpi/openmpi/3.0_intel_17.0/bin/mpirun' ;;
              19.*) use_mpi_startrun='software/all/mpi/openmpi/3.0_intel_19.0/bin/mpirun' ;;
            esac
          ;;
          gcc)
            use_submit='msub'
            use_mpi_startrun='mpirun'
          ;;
        esac
        use_mpi_startrun='/software/all/mpi/openmpi/3.0_intel_17.0/bin/mpirun'
      ;;
    esac
  ;;

  mpg.de)
    case "$use_host" in
      mpipc*|breeze*)
        use_target='mpipc'
        case "`lsb_release -c | awk '{print $2}'`" in
          stretch)
            use_load_profile='. /etc/profile.d/mpim.sh'
            use_load_modules='cdo/1.9.6-gccsys python/2.7.14-stable'
          ;;
        esac
      ;;
      *)
        case '@host_os@' in
          darwin*) use_target='mpimac' ;;
        esac
      ;;
    esac
  ;;

  ecmwf.int)
  ;;

  local.net)
    case "$use_host" in
      melian)
        use_target='archlinux'
        use_mpi_startrun='mpiexec'
        use_mpi_procs_pernode=2
        use_shell='/bin/bash'
      ;;
    esac
  ;;

  pa.cluster)
    use_target='pacluster'
    use_submit='qsub'
  ;;

  docker)
    if test xyes = x"${use_gpu}"; then use_target='docker_gpu'; else use_target='docker_cpu'; fi
  ;;

esac

# vim:ft=sh
