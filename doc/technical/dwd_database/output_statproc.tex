\section{Statistically processed output fields}
\label{sec_statproc_fields}

\subsection{Time-averaged fields}

The quantities
\begin{note}
\begin{tabbing}
  \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \kill
  ALHFL\_S \>
  ASHFL\_S \>
  AUMFL\_S \>
  AVMFL\_S \\
  APAB\_S  \>
  ASOB\_S  \>
  ASOB\_T  \>
  ATHB\_S  \\
  ATHB\_T  \>
  ASWDIR\_S  \>
  ASWDIFS\_S  \>
  ASWDIFU\_S
\end{tabbing}
\end{note}

constitute time averages over the respective forecast time. The averaging process is performed from forecast start  
($t_{0}=0\,\mathrm{s}$) till forecast end. Thus, time averaged fields which are written to the database at $t=t_{i}$ 
contain averages for the elapsed time interval $[t_{0}, t_{i}]$.

Let $\Psi$ denote the instantaneous value of one of the above fields. The time average $\overline{\Psi}$ at time $t$ stored in the 
database is given as
\begin{align*}
 \overline{\Psi}(t) = \frac{1}{t}\int_{0}^{t} \Psi\, \mathrm{d}t  \quad \text{, for } t>0.
\end{align*}
For $t=0$, the average $\overline{\Psi}$ is equal to 0. 
If time averages are required for other time intervals $[t_{1},t_{2}]$, with $t_{1}>0$, these can be computed as follows:
\begin{align*}
 \overline{\Psi}(t_{2}-t_{1}) &= \frac{1}{t_{2}-t_{1}}\int_{t_{1}}^{t_{2}} \Psi\,\mathrm{d}t \\
                              &= \frac{1}{t_{2}-t_{1}} \left[\int_{0}^{t_{2}} \Psi\,\mathrm{d}t  - \int_{0}^{t_{1}} \Psi\,\mathrm{d}t \right] \\
                              &= \frac{1}{t_{2}-t_{1}} \left[ t_{2}\overline{\Psi}(t_{2}) - t_{1}\overline{\Psi}(t_{1}) \right]
\end{align*}
For this equation to work, it is of course necessary that the fields $\overline{\Psi}(t_{1})$ and $\overline{\Psi}(t_{2})$ are available from the database.



The averaging process is fully reflected by the field's GRIB2 metainfo. In order to check whether a field contains the desired time average, it is 
advisable to check the content of the GRIB2 keys listed in Table~\ref{tab_GRIB2avg}.
I.e.\ \texttt{productDefinitionTemplateNumber=8} indicates that the field in question is statistically processed. The statistical process itself 
is specified by the key \texttt{typeOfStatisticalProcessing}. The averaging interval (relative to the start of the forecast) is given by 
\begin{center}
$[\texttt{forecastTime}, \texttt{forecastTime+lengthOfTimeRange}]$. 
\end{center}
Since the averaging process starts at $t=0$, the key \texttt{forecastTime} 
is set to 0. 

\begin{longtable}{p{1.2cm}p{5.5cm}p{1.0cm}p{6.0cm}}
\caption[]{List of GRIB2 keys which provide information about the \emph{averaging} process}\label{tab_GRIB2avg}\\
  \toprule
\multicolumn{1}{c}{\textbf{Octet(s)}}  &  \multicolumn{1}{c}{\textbf{Key}}  & \multicolumn{1}{c}{\textbf{Value}} & \multicolumn{1}{c}{\textbf{Meaning}} \\
\midrule
%%\endfirsthead
%%\caption[]{\emph{continued}}\\
%%\midrule
\endhead
\hline \multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot
\endlastfoot
 8-9        &  \texttt{productDefinitionTemplateNumber}             &     8               &    Average, accumulation, extreme values or other statistically processed values at a horizontal level or in a horizontal layer in a continuous or non-continuous time interval   \\
 19-22      &  \texttt{forecastTime}                                &     0               &    Starting time of the averaging process relative to the reference time. \\
 47         &  \texttt{typeOfStatisticalProcessing}                 &     0               &    Average                                  \\
 50-53      &  \texttt{lengthOfTimeRange}                           &    \emph{variable}  &    Time range over which statistical processing is done \\
\bottomrule
\end{longtable}


\subsection{Accumulated fields}

The quantities
\begin{note}
\begin{tabbing}
  \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \kill
  RAIN\_GSP \>
  SNOW\_GSP \>
  RAIN\_CON \>
  SNOW\_CON \\
  TOT\_PREC  \>
  RUNOFF\_S  \>
  RUNOFF\_G
\end{tabbing}
\end{note}

are accumulated over the respective forecast time. The accumulation process is performed from forecast start  
($t_{0}=0\,\mathrm{s}$) till forecast end. Thus, fields which are written to the database at $t=t_{i}$ 
are accumulated for the elapsed time interval $[t_{0}, t_{i}]$.

Let $\Psi$ denote the instantaneous value of one of the above fields. The accumulation $\hat{\Psi}$ at time $t$ stored in the 
database is given as
\begin{align*}
 \hat{\Psi}(t) = \int_{0}^{t} \Psi\, \mathrm{d}t  \quad \text{, for } t>0.
\end{align*}
For $t=0$, the accumulation $\hat{\Psi}$ is equal to 0. 
If accumulations are required for other time intervals $[t_{1},t_{2}]$, with $t_{1}>0$, these can be computed as follows:
\begin{align*}
 \hat{\Psi}(t_{2}-t_{1}) &= \int_{t_{1}}^{t_{2}} \Psi\,\mathrm{d}t \\
                         &= \int_{0}^{t_{2}} \Psi\,\mathrm{d}t  - \int_{0}^{t_{1}} \Psi\,\mathrm{d}t \\
                         &= \hat{\Psi}(t_{2}) - \hat{\Psi}(t_{1})
\end{align*}
For this equation to work, it is of course necessary that the fields $\hat{\Psi}(t_{1})$ and $\hat{\Psi}(t_{2})$ are available from the database.

The accumulation process is fully reflected by the field's GRIB2 metainfo. In order to check whether a field contains the desired accumulation, 
it is advisable to check the content of the GRIB2 keys listed in Table~\ref{tab_GRIB2acc}.
I.e.\ \texttt{productDefinitionTemplateNumber=8} indicates that the field in question is statistically processed. The statistical process itself 
is specified by the key \texttt{typeOfStatisticalProcessing}. The accumulation interval (relative to the start of the forecast) is given by 
\begin{center}
$[\texttt{forecastTime},\texttt{forecastTime+lengthOfTimeRange}]$. 
\end{center}
Since the accumulation process starts at $t=0$, the key \texttt{forecastTime} 
is set to 0. 
\begin{longtable}{p{1.2cm}p{5.5cm}p{1.0cm}p{6.0cm}}
\caption[]{List of GRIB2 keys which provide information about the \emph{accumulation} process}\label{tab_GRIB2acc}\\
  \toprule
\multicolumn{1}{c}{\textbf{Octet(s)}}  &  \multicolumn{1}{c}{\textbf{Key}}  & \multicolumn{1}{c}{\textbf{Value}} & \multicolumn{1}{c}{\textbf{Meaning}} \\
\midrule
%%\endfirsthead
%%\caption[]{\emph{continued}}\\
%%\midrule
\endhead
\hline \multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot
\endlastfoot
 8-9        &  \texttt{productDefinitionTemplateNumber}             &     8               &    Average, accumulation, extreme values or other statistically processed values at a horizontal level or in a horizontal layer in a continuous or non-continuous time interval   \\
 19-22      &  \texttt{forecastTime}                                &     0               &    Starting time of the accumulation process relative to the reference time. \\
 47         &  \texttt{typeOfStatisticalProcessing}                 &     1               &    Accumulation                                  \\
 50-53      &  \texttt{lengthOfTimeRange}                           &    variable         &    Time range over which statistical processing is done \\
\bottomrule
\end{longtable}



\subsection{Extreme value fields}

The quantities
\begin{note}
\begin{tabbing}
  \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \kill
  VMAX\_10M \>
  TMAX\_2M \>
  TMIN\_2M 
\end{tabbing}
\end{note}
represent extreme values, which are collected over certain time intervals $\chi$, starting from the beginning of the forecast.
The interval $\chi$ differs temperatures and gusts:
\begin{itemize}
  \item $\chi=6\,\mathrm{h}$ for temperatures, TMAX\_2M and TMIN\_2M,
  \item $\chi=1\,\mathrm{h}$ for gusts, VMAX\_10M.
\end{itemize}
After $\chi$ hours of forecast the fields are re-initialized with $0$ for the first time and the next $\chi$-hourly collection 
phase is started. This procedure is repeated till the end of the forecast.

Let $\Psi$ denote the instantaneous value of one of the above fields. The maximumu value $\Psi_{max}$ at time $t$ stored in the 
database is given as
\begin{align*}
 \Psi_{max}(t) = \max(\Psi(t),\Psi_{max}(t))  \quad \text{, for } t_{i}<t<t_{i}+\chi
\end{align*}
Here, $t_{i}$ indicates the time when $\Psi_{max}$ was (re)-initialized the last time. For $t=0$, the extreme value $\Psi_{max}$ 
is equal to the instantaneous value $\Psi$. 

\paragraph{Please note:} Even though a 6 hour time window is used for temperatures, the database contains hourly, 2-hourly, etc.        
extreme temperatures. This is because the extreme temperatures are written to the database hourly, irrespective of the 
start/end of the 6-hourly time windows. Example: Extreme temperatures which are written into the database after a 
forecast time of $8$ hours, contain extreme values collected over the last $2$ hours. On the other hand, extreme temperatures
written into the database after $12$ hours contain values collected over the last $6$ hours.
Thus, when dealing with those fields it is very important to check the GRIB2 keys listed in Table \ref{tab_GRIB2extreme}.

\texttt{productDefinitionTemplateNumber=8} indicates that the field in question is statistically processed. The statistical process itself 
is specified by the key \texttt{typeOfStatisticalProcessing}. The time interval (relative to the start of the forecast) over which the 
extreme value collection was performed is given by $[\texttt{forecastTime},\texttt{forecastTime+lengthOfTimeRange}]$. 
Since the collection process is restarted every $\chi$ hours, the key \texttt{forecastTime} can differ from 0. 

\begin{longtable}{p{1.2cm}p{5.5cm}p{1.0cm}p{6.0cm}}
\caption[]{List of GRIB2 keys which provide information about the \emph{extreme value} process}\label{tab_GRIB2extreme}\\
  \toprule
\multicolumn{1}{c}{\textbf{Octet(s)}}  &  \multicolumn{1}{c}{\textbf{Key}}  & \multicolumn{1}{c}{\textbf{Value}} & \multicolumn{1}{c}{\textbf{Meaning}} \\
\midrule
%%\endfirsthead
%%\caption[]{\emph{continued}}\\
%%\midrule
\endhead
\hline \multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot
\endlastfoot
 8-9        &  \texttt{productDefinitionTemplateNumber}             &     8               &    Average, accumulation, extreme values or other statistically processed values at a horizontal level or in a horizontal layer in a continuous or non-continuous time interval   \\
 19-22      &  \texttt{forecastTime}                                &    \emph{variable}  &    Starting time of the statistical process relative to the reference time. \\
 47         &  \texttt{typeOfStatisticalProcessing}                 &     2,3             &    Maximum/Minimum                                  \\
 50-53      &  \texttt{lengthOfTimeRange}                           &    \emph{variable}  &    Time range over which statistical processing is done \\
\bottomrule
\end{longtable}



% --------------------------------------------------------------------------------
\section{Technical Details of the Horizontal Interpolation}
\label{section:technical_details_of_the_horizontal_interpolation}
% --------------------------------------------------------------------------------
ICON currently supports three different methods for interpolating data horizontally  
from the native triangular grid onto a regular lat-lon grid:
\begin{description}[leftmargin=3.0cm,style=sameline]
 \item [RBF] Radial basis functions
 \item [BCT] Barycentric interpolation
 \item [NNB] Nearest-neighbor interpolation
\end{description}
The interpolation selected for a particular field is indicated in the previous tables 
which list all available output fields.

Most of the output data on regular grids is processed using an \emph{RBF-based interpolation method}.
The algorithm  approximates the input field with a linear combination of 
radial basis functions~(RBF) located at the data sites, see, for example, \cite{Ruppert2007}.
RBF interpolation typically produces over- and undershoots at position where the input field
exhibits steep gradients.
Therefore, the internal interpolation algorithm performs a cut-off by default.
Note that RBF-based interpolation is \emph{not conservative}.

\emph{Barycentric interpolation} is a two-dimensional generalization of
linear interpolation.
This method uses just three near-neighbors to interpolate and
avoids over- and undershoots, since extremal values are taken
only in the data points.
This interpolation makes sense for fields where the values change  
in a roughly piecewise linear way.

A small number of output fields is treated differently, with a \emph{nearest-neighbor interpolation} 
(NNB). The nearest neighbor algorithm selects the value of the nearest point and does not consider 
the values of neighboring points at all, yielding a piecewise-constant interpolant. 
% \begin{tabbing}
%   \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \= \hspace{0.2\textwidth} \kill
%   CAPE\_CON  \>
%   CAPE\_ML   \>
%   CIN\_ML    \>
%   HBAS\_CON  \\
%   HTOP\_CON  \>
%   HTOP\_DC   \>  
%   HZEROCL    \>
%   RAIN\_CON  \\
%   RAIN\_GSP  \>   
%   SNOW\_CON  \>
%   SNOW\_GSP  \>
%   SNOWLMT    \\
%   SOILTYP    \>
%   TOT\_PREC  \>
%   W\_SO\_ICE \>
%   WW   
% \end{tabbing}


