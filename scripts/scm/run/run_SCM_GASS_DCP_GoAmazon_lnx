#!/bin/ksh
#-----------------------------------------------------------------------------
# CRAY@DWD
#
#PBS -q xc_norm_b
#PBS -o /lustre2/uwork/mkoehler/wq
#PBS -e /lustre2/uwork/mkoehler/wq
#PBS -j oe                       
#PBS -l select=1:ompthreads=4
#PBS -l place=scatter            
#PBS -l walltime=06:00:00
#PBS -N SCM_GoAmazon
#PBS -m ae
#-----------------------------------------------------------------------------

set -x

# ----------------------------------------------------------------------------
# specifiy experiment (idealized simulation)
# ----------------------------------------------------------------------------
EXPNAME=SCM_GoAmazon_OP1_t2
#EXPNAME=SCM_GoAmazon

# ----------------------------------------------------------------------------
# path definitions
# ----------------------------------------------------------------------------

# base directory for ICON sources and binary:
ICONDIR="$PWD/../../.."

# SCM data directory (grids, init data, extpar)
SCMDATA=/hpc/uwork/mkoehler/scm/data       # at DWD on NEC

# directory with input grids:
GRIDDIR=${SCMDATA}/grid

# directory with init files:
INITDIR=${SCMDATA}/init_data

# absolute path to output directory for results:
EXPDIR=${SCMDATA}/${EXPNAME}

# path to model binary, including the executable:
#MODEL=${ICONDIR}/build/x86_64-unknown-linux-gnu/bin/icon
MODEL=${ICONDIR}/bin/icon


# ----------------------------------------------------------------------------
# copy input data: grids, external parameters, model
# ----------------------------------------------------------------------------

# the directory for the experiment will be created, if not already there
if [ ! -d $EXPDIR ]; then
    mkdir -p $EXPDIR
fi
cd ${EXPDIR}

# files needed for radiation
ln -sf ${ICONDIR}/data/ECHAM6_CldOptProps.nc .
ln -sf ${ICONDIR}/data/rrtmg_lw.nc .
ln -sf ${ICONDIR}/data/rrtmg_sw.nc .


# link binary
ln -sf $MODEL icon


#-----------------------------------------------------------------------------
# model timing
#-----------------------------------------------------------------------------

# 0.4 s for 70 m res., 0.5 for 100 m; 30 s for 2.5 km?
dtime=60
ndyn_substeps=3
dt_checkpoint=`expr 100 \*86400`  # write restart file every hours (when lrestart = TRUE)
nhours=72
#nhours=1
nsteps=`expr ${nhours} \* 3600 / ${dtime}`

#start_date="2014-01-01T00:00:00Z"
#end_date="2014-01-04T00:00:00Z"
start_date="2014-02-15T00:00:00Z"
end_date="2014-03-23T21:00:00Z"

#-----------------------------------------------------------------------------
# output
#-----------------------------------------------------------------------------
DT_DATA=`expr 1 \* 3600`      # output each n hours
DT_DATA=${dtime}              # output every time step
#n_in_ofile=60                # number of time steps per output file 
n_in_ofile=10000              # number of time steps per output file 

# ----------------------------------------------------------------------------
# grid namelist settings
# ----------------------------------------------------------------------------

# the grid parameters
atmo_dyn_grids="Torus_Triangles_4x4_2500m.nc"
atmo_rad_grids=""

# reconstruct the grid parameters in namelist form
dynamics_grid_filename=""
for gridfile in ${atmo_dyn_grids}; do
  dynamics_grid_filename="${dynamics_grid_filename} '${gridfile}',"
done
radiation_grid_filename=""
for gridfile in ${atmo_rad_grids}; do
  radiation_grid_filename="${radiation_grid_filename} '${gridfile}',"
done

ln -sf ${GRIDDIR}/${atmo_dyn_grids} .

#forcings
#ln -sf ${INITDIR}/init_SCM_GASS_DCP_GoAmazon.nc init_SCM.nc
ln -sf ${INITDIR}/init_SCM_GASS_DCP_GoAmazon_IOP1.nc init_SCM.nc

# ----------------------------------------------------------------------------
# create ICON master namelist
# ----------------------------------------------------------------------------

cat > icon_master.namelist << EOF

! master_nml: ----------------------------------------------------------------
&master_nml
 lrestart                    =                     .FALSE.        ! .TRUE.=current experiment is resumed
/

! master_model_nml: repeated for each model ----------------------------------
&master_model_nml
 model_type                  =                          1         ! identifies which component to run (atmosphere,ocean,...)
 model_name                  =                      "ATMO"        ! character string for naming this component.
 model_namelist_filename     =       "NAMELIST_${EXPNAME}"        ! file name containing the model namelists
 model_min_rank              =                          1         ! start MPI rank for this model
 model_max_rank              =                      65536         ! end MPI rank for this model
 model_inc_rank              =                          1         ! stride of MPI ranks
/

! time_nml: specification of date and time------------------------------------
&time_nml
 ini_datetime_string         =               "$start_date"        ! initial date and time of the simulation
 end_datetime_string         =                 "$end_date"        ! initial date and time of the simulation
/

EOF


# ----------------------------------------------------------------------------
# model namelists
# ----------------------------------------------------------------------------
# For a complete list see doc/Namelist_overview.pdf

cat > NAMELIST_${EXPNAME} << EOF

&parallel_nml
 nproma         =  8
 p_test_run     = .false.
 num_io_procs   =  1         ! number of I/O processors
 l_test_openmp  = .false.
 l_log_checks   = .false.
/

&grid_nml
 dynamics_grid_filename = "${atmo_dyn_grids}",
 corio_lat              = -3.11         ! Manaus 3.11S, 60.02W
 is_plane_torus         = .TRUE.
 l_scm_mode = .TRUE.       ! main logical to turn on SCM mode
/


&SCM_nml
 i_scm_netcdf = 1            ! read initial profiles and forcings from netcdf
 lscm_read_tke = .FALSE.    ! read initial tke from netcdf
 lscm_read_z0  = .FALSE.    ! read initial z0 from netcdf
 scm_sfc_mom   = 2          ! 2: prescribed u*
 scm_sfc_temp  = 2          ! 2: prescribed sensible heat flux at surface
 scm_sfc_qv    = 2          ! 2: prescribed latent heat flux at surface
/

&io_nml
 dt_checkpoint  = ${dt_checkpoint}
 lkeep_in_sync  = .true.
/

&run_nml
 num_lev        = 90           ! number of full levels of vertical grid
 dtime          = ${dtime}     ! timestep in seconds
 nsteps         = ${nsteps}
 ldynamics      = .FALSE.      ! compute adiabatic dynamic tendencies
 ltransport     = .FALSE.
 ntracer        = 5            ! default: 0
 iforcing       = 3            ! 3: NWP forcing; 6:inhecham forcing
 ltestcase      = .TRUE.       ! run testcase
 ltimer         = .FALSE.      ! 
 msg_level      = 12           ! detailed report during integration
 output         = 'nml','totint'
/

&nwp_phy_nml
 inwp_gscp       = 1
 inwp_convection = 1 ! 1:Tiedtke/Bechtold
 inwp_radiation  = 1 ! 1:RRTM radiation
 inwp_cldcover   = 1 ! 3: clouds from COSMO SGS cloud scheme
 inwp_turb       = 1 ! 1: TKE diffusion and transfer
 inwp_satad      = 1
 inwp_sso        = 0
 inwp_gwd        = 0
 inwp_surface    = 0 ! 0: none; 1: TERRA   (0: simple ocean, sea-ice albedo!)
 icapdcycl       = 3 ! apply CAPE modification to improve diurnalcycle over tropical land (optimizes NWP scores)
 latm_above_top  = .TRUE.  ! needed for radiation routine
 itype_z0        = 1
 dt_rad	         = 1800.        ! Default: 1800   ! M. Koehler: 1440
 dt_conv         = 600.         ! Default: 600    ! M. Koehler: 360
 dt_sso	         = 600.         ! Default: 1200   ! M. Koehler: 720
 dt_gwd	         = 600.         ! Default: 1200   ! M. Koehler: 720
/

&lnd_nml
 ntiles	         = 1       ! Default: 3
 lmulti_snow     = .false.
 itype_heatcond	 = 2
 idiag_snowfrac	 = 20      ! Default: 1 ! M. Koehler: 2
 lsnowtile       = .true.  ! later on .true. if GRIB encoding issues are solved
 lseaice         = .true.
 llake	         = .true.
 itype_lndtbl    = 4       ! minimizes moist/cold bias in lower tropical troposphere
 itype_root      = 2
 sstice_mode     = 2       ! requires extpar >= 20170202
/

&radiation_nml
 irad_o3         =  9      ! ozone climatology (ecrad: 0, 7 GEMS, 9 MACC, 79, 97)
 ecRad_data_path = '${ICONDIR}/externals/ecrad/data'
/

&ls_forcing_nml
 is_subsidence_moment = .TRUE.
 is_subsidence_heat   = .TRUE.
 is_advection         = .TRUE.
 is_advection_uv      = .FALSE.
 is_advection_tq      = .TRUE.
 is_geowind           = .FALSE.
 is_rad_forcing       = .FALSE.
 is_nudging           = .TRUE.
 is_nudging_uv        = .TRUE.
 is_nudging_tq        = .FALSE.
/

! &les_nml
!  isrfc_type       = 6      !2=Fixed flux, 5=fixed SST, 3=fixed bflux -> does not work for nwp mode!
!  expname          = '${EXPNAME}'
!  avg_interval_sec = 30.    ! averaging interval in sec for statistical output (1D) <- does that work??? test!!!
!  sampl_freq_sec   = 30.    ! sampling frequ in sec for statistical output (1D and 0D)
!  ldiag_les_out    = .TRUE. ! statistical output in LES mode
!  km_min           = 0.0    ! minimum turbulent viscosity
! /

&turbdiff_nml
 tkhmin        = 0.75   ! new default since rev. 16527
 tkmmin        = 0.75   !           " 
 pat_len       = 750.
 c_diff        = 0.2
 rat_sea       = 8.5    ! new value since rev. 25646 (reduced saturation vapor pressure over salt water)
 ltkesso       = .false.
!frcsmot       = 0.2    ! these 2 switches together apply vertical smoothing of the TKE source terms
!imode_frcsmot = 2      ! in the tropics (only), which reduces the moist bias in the tropical lower troposphere
 ! use horizontal shear production terms with 1/SQRT(Ri) scaling to prevent unwanted side effects:
 itype_sher    = 0 
 ltkeshs       = .false.
 a_hshr        = 2.0
 !lconst_z0    = .TRUE.
 !const_z0     = 0.035
 icldm_tran    = 2
 icldm_turb    = 2
/

&diffusion_nml
 lhdiff_temp  = .TRUE.
 lhdiff_vn    = .TRUE.
 lhdiff_w     = .TRUE.
 hdiff_order  = 4
/

&nonhydrostatic_nml
 ivctype        = 2          ! sleve vertical coordinate
 damp_height    = 25000.     ! top_height-damp_height should be about 15km
 ndyn_substeps  = ${ndyn_substeps}
 l_open_ubc     = .false.   ! top open upper boundary condition. might help to go higher
 rayleigh_coeff = 0.0
 htop_moist_proc= 22500.
 hbot_qvsubstep	= 22500.    ! Default: 22500  ! M. Koehler: 19000; at least as large as htop_moist_proc
/

&sleve_nml
 min_lay_thckn	= 20.       ! Default: 50     ! M. Koehler: 20
 max_lay_thckn	= 400.      ! maximum layer thickness below htop_thcknlimit
 htop_thcknlimit= 14000.    ! 
 top_height     = 75000.
 stretch_fac	= 0.9       ! Default: 1      ! M. Koehler: 0.9
 decay_scale_1	= 4000.
 decay_scale_2	= 2500.
 decay_exp	= 1.2
 flat_height	= 16000.
/

&extpar_nml
 itopo          = 0 ! 0: analytical topo; 1: topography/ext. data read from file
/

&dynamics_nml
 iequations     = 3       ! equation system
 idiv_method    = 1
 divavg_cntrwgt = 0.50
 lcoriolis      = .TRUE.
/

&output_nml
 output_start     = "${start_date}"
 output_end       = "${end_date}"
 output_interval  = "PT01H"
 !output_interval  = "PT60S"
 !file_interval    = "P01H"
 steps_per_file   = ${n_in_ofile}
 include_last     = .TRUE.
 output_filename  = 'scm_out'
 filename_format  = "<output_filename>_<levtype>_<datetime2>"
 ml_varlist       = 'z_ifc','z_mc','u','v','w','temp','pres','rho','theta_v','pres_sfc','div',
                    'qv','qc','qi','qs','qr','rh',
                    'ashfl_s', 'alhfl_s', 'athb_s', 'athb_t', 'asob_s', 'asob_t', 
                    'ddt_temp_radsw', 'ddt_temp_radlw', 'ddt_temp_turb', 'ddt_temp_drag', 'ddt_temp_pconv',
                    'ddt_qv_turb','ddt_qc_turb','ddt_qv_conv','ddt_qc_conv','u_10m', 'v_10m', 't_2m', 't_g',
                    'qv_s','z_mc','lhfl_s','shfl_s','umfl_s','vmfl_s','tcm','tch','clc','tke','rcld','qhfl_s',
                    'sob_s', 'thb_s','sob_t','ddt_u_turb','ddt_v_turb','sod_t'
 output_grid      = .TRUE.
/

EOF


# ----------------------------------------------------------------------------
# run the model!
# ----------------------------------------------------------------------------

./icon > out.txt 2>out2.txt

# select=4 : Number of nodes - see header (n/N)
# -n 48    : Number of MPI Tasks (N*select)
# -N 12    : Number of MPI Tasks/Node            (12: haswell, 18: broadwell)
# -d 4     : Number of Threads/MPI Task
# -j 2     : Hyperthreading enabled: 24 physical cores -> 48 "virtual" cores
# -m 3g    : 3GB memory, 64GB available shared on one node (m x N < 64)

#aprun -n 18 -N 18 -j 2 -d 4 -m 3g ./icon
